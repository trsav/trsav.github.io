---
title: "Bayesian Optimisation with Noisy Measurements"
subtitle: "A review"
author: 
 - name: "Tom Savage"
categories: [Optimisation, Machine Learning]
date: "12/05/2023"
callout-appearance: minimal
format:
  html:
    css: styles.css
bibliography: ref.bib
---

I have reworked some notes from [@garnett_bayesoptbook_2023] regarding noisy-expected improvement. Please refer to the original text for a more detailed explanation.

:::{.cleanbox}
The assumptions that hold for the expected improvement utility function do not hold when measurements have noise. 

We really want to find the point where the _signal_ is optimised [@Jones1998].

How do you determine if a measurement is signal or noise?
:::

We begin by specifying an underlying objective function, which we consider unknown, shown in @fig-underlying. We have access to observations which we assume contain normally distributed noise. 

```{python}
#| echo: false
import numpy as np 
from jax import jit
import time
from scipy.optimize import minimize
import torch
import gpytorch
import matplotlib.pyplot as plt 
import matplotlib as mpl
import warnings

warnings.filterwarnings("ignore")
# system font = Arial for text
mpl.rcParams['font.sans-serif'] = "Arial"

```
```{python}
#| code-fold: true
#| code-summary: "Definition of underlying function..."
def f(x):
  return 2.4*np.sin(2.8*x) - ((x-3.5*np.pi) ** 2)/4 +  3.8*np.cos(1.7*x) - (x**2)/16 + np.random.normal(0,2)
```
```{python}
#| echo: false
#| label: fig-underlying
#| fig-align: center
#| fig-width: 50% 
#| fig-cap: "The underlying noisy funtion to be maximised alongside example observations." 
figsize = (8,2)
fig,ax = plt.subplots(1,1,figsize=figsize)
x = np.linspace(0,3*np.pi,100)
y_mean  = 2.4*np.sin(2.8*x) - ((x-3.5*np.pi) ** 2)/4 +  3.8*np.cos(1.7*x) - (x**2)/16 
y_upper = 2.4*np.sin(2.8*x) - ((x-3.5*np.pi) ** 2)/4 +  3.8*np.cos(1.7*x) - (x**2)/16 + 4
y_lower = 2.4*np.sin(2.8*x) - ((x-3.5*np.pi) ** 2)/4 +  3.8*np.cos(1.7*x) - (x**2)/16 - 4
samples = [f(x_i) for x_i in np.linspace(0,3*np.pi,70)]

ax.plot(x,y_mean,c='k',lw=1,label='$\mu$')
ax.fill_between(x,y_upper,y_lower,lw=0,alpha=0.1,color='k',label='$2\sigma$')
ax.scatter(np.linspace(0,3*np.pi,70),samples,c='k',marker='+',s=70,lw=0.75,label='Samples')
# right and top spines 
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
ax.set_xticks([],[])
ax.set_yticks([],[])
ax.legend(frameon=False)
ax.set_xlabel('$x$')
ax.set_ylabel('$f(x)$')
plt.show()
```

We first begin by motivating the usual expected-improvement criteria. 
We are looking to maximise the <span style="color:red;">expected</span> increase between the of the <span style="color:green;">maximum of the mean of the resulting Gaussian process after making an observation at $x$</span> and the <span style="color:blue;">maximum of the current Gaussian process</span> <span style="color:red;">over potential observations $y$</span> which are Gaussian distributed as a result of our $\mathcal{GP}$ model.
$$\alpha_{EI}(x;\mathcal{D}) = {\color{red}\int} \left[{\color{green}\max \mu_{\mathcal{D}'}} - {\color{blue}\mu^*}\right]{\color{red}\mathcal{N}(y;\mu,s^2)\text{d}y}$$ {#eq-ei_noisy}

By formulating the expression with respect to the improvement in the mean of the current and subsequent Gaussian process models, **we are expressing our interest in optimising the signal** and not the noise or values of specific observations.

:::{.cleanbox}
**Important:**
Given a hypothetical observation $y$, the value of the mean of the resulting Gaussian process $\mu_{\mathcal{D}'}$ at given set of potential locations $\mathbf{x}'$ is 

$$ \mu_{\mathcal{D}'} =  \mu_{\mathcal{D}} + \frac{K_\mathcal{D}(\mathbf{x}',x)}{s}\frac{y-\mu}{s},$$

where $\mu$ and $s^2$ are the mean and standard deviation of the distribution of potential values $y$ could take. 
:::

When we express this distribution in terms of the standard normal distribution $z := \mathcal{N}(0,1)$, we have $y = \mu + sz$ and as a result

$$ \mu_{\mathcal{D}'} =  \mu_{\mathcal{D}}(\mathbf{x}') + \frac{K_\mathcal{D}(\mathbf{x}',x)}{s}z.$$

Putting this back into @eq-ei_noisy, we now only have to take the expectation over the standard normal distribution resulting in the following. 

$$\alpha_{EI}(x,\mathcal{D}) = \int \max_{\mathbf{x}'} \left(\mu_{\mathcal{D}}(\mathbf{x}') + \frac{K_\mathcal{D}(\mathbf{x}',x)}{s}z \right)\phi(z)\;\text{d}z - \mu^*$${#eq-ei_noisy2}

To begin with we will sample some initial data:

```{python}
# our initial dataset
x_data = np.random.uniform(0,3*np.pi,8)
y_data = np.array([f(x_i) for x_i in x_data])
```

```{python}
#| echo: false
x_data = torch.from_numpy(x_data)
y_data = torch.from_numpy(y_data)
```

```{python}
#| code-fold: true
#| code-summary: "GP model definition and training..."
class ExactGPModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood):
        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())
    
    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)

def build_GP(x,y,its):
  likelihood = gpytorch.likelihoods.GaussianLikelihood()
  model = ExactGPModel(x, y, likelihood)

  model.train()
  likelihood.train()

  optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters

  mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)

  for i in range(its):
      optimizer.zero_grad()
      output = model(x)
      loss = -mll(output, y)
      loss.backward()
      optimizer.step()
  return model,likelihood
model,likelihood = build_GP(x_data,y_data,2000)
GP = {'model':model,'likelihood':likelihood}
```
@fig-initial shows this data with an initial Gaussian process (importantly assuming in-exact observations).

```{python}
#| echo: false
#| label: fig-initial
#| fig-align: center
#| fig-cap: "A Gaussian process fit to the initial dataset."
def plot_model(ax,GP,x,y,in_loop=False):
  model = GP['model']
  likelihood = GP['likelihood']
  model.eval()
  likelihood.eval()
  ax.set_xlabel(r'$x$')
  ax.set_ylabel(r'$f(x)$')
  ax.spines['right'].set_visible(False)
  ax.spines['top'].set_visible(False)
  ax.scatter(x,y,c='k',marker='+',s=70,lw=0.75,label='Data')
  if in_loop != False:
    ax.scatter(x[-1],y[-1],c='k',marker='o',s=60,lw=1.5,label='Latest Observation')
    ax.set_title('Iteration: '+str(in_loop))

  model.eval()
  likelihood.eval()

  with torch.no_grad(), gpytorch.settings.fast_pred_var():
    test_x = torch.linspace(0, 3*np.pi, 300)
    observed_pred = likelihood(model(test_x))
  lower, upper = observed_pred.confidence_region()
  ax.plot(test_x.numpy(), observed_pred.mean.numpy(), 'k', lw=1, label='GP')
  ax.fill_between(test_x.numpy(), lower.numpy(), upper.numpy(), alpha=0.1, color='k',lw=0,label='$2\sigma$')
  ax.set_xlim([0,3*np.pi])
  if in_loop == False:
    ax.legend(frameon=False)
  return ax

fig,ax = plt.subplots(1,1,figsize=figsize)
ax = plot_model(ax,GP,x_data,y_data)
plt.show();
```

Now we will naively construct @eq-ei_noisy2 using 40 samples from the standard normal distribution, and naively calculate an approximation to the inner maximisation by evaluating at 100 location throughout the space (which defines our vector $\mathbf{x}' \in\mathbb{R}^{100}$) and returning the max value from these. 

```{python}
def noisy_EI(x,GP):
  model = GP['model']; model.eval()
  likelihood = GP['likelihood']; likelihood.eval()
  N = torch.distributions.Normal(0,1)
  predicted_output = likelihood(model(x))
  mean = predicted_output.mean
  var = predicted_output.variance
  x_prime_vals = torch.linspace(0,3*np.pi,100).reshape(-1,1)    
  mu_vals = likelihood(model(x_prime_vals)).mean
  cov_vals = model.covar_module(x_prime_vals,x)
  integral = 0 
  samples = 2000
  z_vals = N.sample((samples,1))[:,0]
  for z in z_vals:
    integral += torch.max(mu_vals + (cov_vals*z)[:,0]) 
  return integral / samples
```


Now if we plot this function alongside the time taken to evaluate the acquisition function at 100 locations.

```{python}
#| echo: false
#| fig-align: center

def plot_aq(ax,GP,rec_time=False):
  GP['model'].eval()
  GP['likelihood'].eval()
  n_eval = 100
  x_vals = np.array(np.linspace(0,3*np.pi,n_eval).reshape(-1,1),dtype=np.float32)
  s = time.time()
  y_vals = [noisy_EI(x_val,GP) for x_val in torch.from_numpy(x_vals)]
  e = time.time()
  y_vals = np.array([y.detach().numpy() for y in y_vals],dtype=np.float32)
  x_vals = np.array(x_vals,dtype=np.float32)[:,0]
  ax.plot(x_vals,y_vals,c='k',lw=1)
  ax.fill_between(x_vals,np.ones(n_eval)*np.min(y_vals),y_vals,alpha=0.1,color='k',lw=0)
  ax.set_xlabel(r'$x$')
  ax.set_ylabel(r'$\alpha_{EI}(x)$')
  ax.set_xlim([0,3*np.pi])
  ax.set_ylim([np.min(y_vals),np.max(y_vals)])
  ax.spines['right'].set_visible(False)
  ax.spines['top'].set_visible(False)
  if rec_time:
    print('NAIVE TIME TAKEN: ',np.round(e-s,4),'s. AVERAGE TIME PER EVALUATION: ',np.round((e-s)/n_eval,4),'s')
  ax.set_yticks([],[])
  return ax

fig,ax = plt.subplots(1,1,figsize=figsize)
ax = plot_aq(ax,GP,rec_time=True)
plt.show();
```


:::{.cleanbox}
**Important:** For a fixed set of 'improvement locations' $\mathbf{x}'$, the resulting posterior mean at each location can be interpreted as a 1D line as a function of $z$:

$$\mu_{\mathcal{D}'}(z|x') = \mu_{\mathcal{D}}(x') + \frac{K_{\mathcal{D}}(x',x)}{s}z \quad \forall x'\in \mathbf{x}'$${#eq-posterior_mean}

Therefore finding the inner maximum new posterior mean as a function of $z$ can be translated to calculating the upper envelope of a set of lines. This upper envelope represents for a given $z$, the maximum posterior mean across all of the locations in $\mathbf{{x}'}$. The main idea is to remove the inner $\max$ operator and replace it with something more tractable enabling analytical integration.

$$\int \max \left[\text{lines} (z)\right] \phi(z) \;\text{d}z \rightarrow \int \text{upper envelope} (z) \phi(z) \;\text{d}z \rightarrow \text{analytical solution}$$

Which has an analytical solution for a piecewise linear upper envelope. To do so:

```{python}
n = 100
a = np.random.uniform(0,0.2,n)
b = np.random.uniform(-10,10,n)
```
```{python}
#| echo: false
z = np.linspace(-5,5,2)
fig,ax = plt.subplots(1,1,figsize=figsize)
for i in range(n):
  ax.plot(z,a[i]*z+b[i],c='k',lw=1,alpha=0.3)
ax.set_xlabel('$z$')
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
ax.set_ylabel(r'''$\mu_{\mathcal{D}'}(z)$''')
plt.show();
```

We will first get rid of any lines that are definitely dominated between -5 and 5 (the range of $z$ values we are interested in). I'm not going to explain this too much but it's quite easy to derive this condition.

```{python}
l_store = []; u_store = []
zl = -5; zu = 5
for i in range(len(a)):
    l = a[i]*zl + b[i]
    u = a[i]*zu + b[i]
    l_store.append(l)
    u_store.append(u)

L_i = np.argmax(l_store)
U_i = np.argmax(u_store)

del_i = []
for i in range(len(a)):
    if l_store[i] < l_store[U_i] and u_store[i] < u_store[L_i]:
        del_i.append(i)

a = np.delete(a,del_i)
b = np.delete(b,del_i)
```

Now we will sort the lines by gradient 
```{python}
sorted_indices = np.argsort(a)
a = a[sorted_indices]; 
b = b[sorted_indices]; 
```

```{python}
#| echo: false
n = len(a)
a_og = a.copy()
b_og = b.copy()
```


```{python}
#| echo: false
z = np.linspace(-5,5,2)
fig,ax = plt.subplots(1,1,figsize=figsize)
# cols for each line 
cols = np.linspace(0,1,n)

for i in range(n):
  im = ax.plot(z,a[i]*z+b[i],c=plt.cm.Spectral(cols[i]),lw=2)
ax.set_xlabel('$z$')
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False)
ax.set_ylabel(r'''$\mu_{\mathcal{D}'}(z)$''')
plt.show();
```

Then initialise the set of dominated lines (a and b values respectively), respective intervals, and the current largest $z$ on the envelope.
```{python}
dom_a = []
dom_b = []
interval_store = []
envelope_z = -5
```

For each line in order of increasing gradient (here we start with the first line indexed at $j=0$)
```{python}
j = 0 
```

Add the line to the set of dominating lines 
```{python}
dom_a.append(a[j])
dom_b.append(b[j])
```

Calculate the $z$ intercept of the line with all lines of a larger gradient. As we only calculate this intersection with these remaining lines, the overall algorithm has $\mathcal{O}(n\log n)$ complexity. 
```{python}
z_intercept_store = []
for i in range(j+1,n):
    z_intercept = (dom_b[-1] - b[i])/(a[i] - dom_a[-1])
    z_intercept_store.append(z_intercept)
```

```{python}
#| echo: false
#| fig-align: center

fig,ax = plt.subplots(1,1,figsize=figsize)

# xl = min(-5,np.min(z_intercept_store))
# xu = max(5,np.max(z_intercept_store))
# zl = np.min(z_intercept_store)
# zu = np.max(z_intercept_store)
ax.plot([zl,zu],[dom_a[-1]*zl + dom_b[-1],dom_a[-1]*zu + dom_b[-1]],color='black',lw=3)
for i in range(n):
    ax.plot([zl,zu],[a[i]*zl + b[i],a[i]*zu + b[i]],color=plt.cm.Spectral(cols[i]))

for i in range(len(z_intercept_store)):
    z_i = z_intercept_store[i]
    ax.scatter(z_i,dom_a[-1]*z_i + dom_b[-1],color='w',edgecolor='k',s=30,lw=0.5,zorder=3, label=r'$\mu$ intercepts' if i == 0 else None)
ax.legend(frameon=False)
ax.set_xlabel('$z$')
ax.set_ylabel('$\mu$')
ax.set_xlim([-5,5])
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False);
```

The intercept with the lowest value of $z$ will be a potential vertex for the envelope, but we must check that there are no lines above it. 

To do so we calculate the $\mu$ value of each line at $min({\mathbf{z})}$.


```{python}
mu_vals = []
z_intercept = np.min(z_intercept_store)
mu_intercept = dom_a[-1]*z_intercept + dom_b[-1]

for i in range(j+1,n):
    mu_vals.append(a[i]*z_intercept + b[i])
```

```{python}
#| echo: false
#| fig-align: center

fig,ax = plt.subplots(1,1,figsize=figsize)

# xl = min(-5,np.min(z_intercept_store))
# xu = max(5,np.max(z_intercept_store))
ax.plot([zl,zu],[dom_a[-1]*zl + dom_b[-1],dom_a[-1]*zu + dom_b[-1]],color='black',lw=3)
for i in range(n):
    ax.plot([zl,zu],[a[i]*zl + b[i],a[i]*zu + b[i]],color=plt.cm.Spectral(cols[i]))
for i in range(len(mu_vals)):
    ax.scatter(z_intercept,mu_vals[i],color='black',lw=0.5,edgecolor='w',s=30,zorder=3,label=r'$z$ intercepts' if i == 0 else None)
for i in range(len(z_intercept_store)):
    z_i = z_intercept_store[i]
    ax.scatter(z_i,dom_a[-1]*z_i + dom_b[-1],color='w',edgecolor='k',s=30,lw=0.5,zorder=3, label=r'$\mu$ intercepts' if i == 0 else None)
ax.legend(frameon=False)
ax.set_xlabel('$z$')
ax.set_xlim([-5,5])
ax.set_ylabel('$\mu$')
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False);
```

If the maximum value of $\mu$ is on the dominating line, and this value is larger than the current largest $z$ value in the envelope...then we know this must lie on the upper envelope. _Otherwise_ forget about this dominating line by setting it's definition to ```None```.

```{python}
if abs(mu_intercept-np.max(mu_vals)) < 1e-5 and z_intercept > envelope_z:
  interval_store.append([envelope_z,z_intercept])
  envelope_z = z_intercept
else:
  dom_a[-1] = None
  dom_b[-1] = None
  interval_store.append([None,None])
```

Now we can plot the interval we have just calculated.

```{python}
#| echo: false
#| fig-align: center

fig,ax = plt.subplots(1,1,figsize=figsize)

interval_store = np.array(interval_store)
zl = interval_store[0,0]
zu = interval_store[0,1]
# zl = np.min(z_intercept_store)
# zu = np.max(z_intercept_store)
ax.plot([zl,zu],[dom_a[-1]*zl + dom_b[-1],dom_a[-1]*zu + dom_b[-1]],color='black',lw=4,alpha=1,ls='dashed',label='Upper Envelope')
zl = -5; zu = 5
for i in range(n):
    ax.plot([zl,zu],[a[i]*zl + b[i],a[i]*zu + b[i]],color=plt.cm.Spectral(cols[i]))
ax.set_xlabel('$z$')
ax.set_ylabel('$\mu$')
ax.legend(frameon=False)
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False);
```

If we now repeat this procedure starting with the line with the next largest gradient we can search for next vertex on the upper envelope. 

```{python}
#| code-fold: true
#| code-summary: "Full algorithm for calculating upper envelope of a set of lines..."

def upper_env(a,b):

  l_store = []; u_store = []
  zl = -5; zu = 5
  for i in range(len(a)):
      l = a[i]*zl + b[i]
      u = a[i]*zu + b[i]
      l_store.append(l)
      u_store.append(u)

  L_i = np.argmax(l_store)
  U_i = np.argmax(u_store)

  del_i = []
  for i in range(len(a)):
      if l_store[i] < l_store[U_i] and u_store[i] < u_store[L_i]:
          del_i.append(i)

  a = np.delete(a,del_i)
  b = np.delete(b,del_i)
  n = len(a)

  sorted_indices = np.argsort(a)
  a = a[sorted_indices]
  b = b[sorted_indices]

  dom_a = []
  dom_b = []
  interval_store = []
  envelope_z = -5

  for j in range(n):
    dom_a.append(a[j])
    dom_b.append(b[j])

    z_intercept_store = []
    for i in range(j+1,n):
        z_intercept = (dom_b[-1] - b[i])/(a[i] - dom_a[-1])
        z_intercept_store.append(z_intercept)

    mu_vals = []
    try:
      z_intercept = np.min(z_intercept_store)
    except:
      interval_store.append([envelope_z,5])
      break 
    mu_intercept = dom_a[-1]*z_intercept + dom_b[-1]
    for i in range(j+1,n):
        mu_vals.append(a[i]*z_intercept + b[i])

    if abs(mu_intercept-np.max(mu_vals)) < 1e-9 and z_intercept > envelope_z:
      interval_store.append([envelope_z,z_intercept])
      envelope_z = z_intercept
    else:
      dom_a[-1] = None
      dom_b[-1] = None
      interval_store.append([None,None])

  del_store = []
  for i in range(len(dom_a)):
      if dom_a[i] == None:
          del_store.append(i)

  dom_a = np.delete(dom_a,del_store)
  dom_b = np.delete(dom_b,del_store)
  interval_store = np.delete(interval_store,del_store,axis=0)

  zl = -5
  zu = 5

  interval_store[interval_store > zu] = zu
  interval_store[interval_store < zl] = zl

  return dom_a,dom_b,interval_store
```

```{python}
#| echo: false
#| fig-align: center
dom_a,dom_b,interval_store = upper_env(a_og,b_og)
zl = -5
zu = 5
fig,ax = plt.subplots(1,1,figsize=figsize)
for i in range(n):
    ax.plot([zl,zu],[a[i]*zl + b[i],a[i]*zu + b[i]],color=plt.cm.Spectral(cols[i]))
for i in range(len(dom_a)):
    zl = interval_store[i,0]
    zu = interval_store[i,1]
    ax.plot([zl,zu],np.array([dom_a[i]*zl + dom_b[i],dom_a[i]*zu + dom_b[i]]),color='black',lw=4,alpha=1,ls='dashed',label='Upper Envelope' if i == 0 else None)
ax.set_xlim([-5,5])
ax.set_xlabel('$z$')
ax.set_ylabel('$\mu$')
ax.spines['right'].set_visible(False)
ax.spines['top'].set_visible(False);
ax.legend(frameon=False);
```

The expected value over $z$ (integrating over potential experimental outputs) of this upper envelope (which we use inplace of the inner $\max$ operator), where $z \sim \mathcal{N}(0,1)$ is given by:

$$
\sum_i \int^{c_{i+1}}_{c_{i}}(a_iz+b_i)\phi(z) \; \text{d}z \rightarrow \sum_i b_i [\Phi (c_{i+1})-\Phi(c_i)] + a_i[\phi(c_i) - \phi(c_{i+1})]$$

where $\Phi$ is the standard cumulative normal probability function, $\phi$ is the standard normal probability function, $c$ are the $z$ values of the upper envelope intervals, and $a$ and $b$ are the gradient and intercept of upper envelope intervals corresponding to @eq-posterior_mean.

_Intuitively_, we are solving the expected improvement integral analytically for each line segement within the upper envelope of lines (which we use as it is equivalent to the inner $\max$ operator). 

:::

For the sake of completeness we will wrap this up into an acquisition function, and run a basic Bayesian optimisation loop

```{python}
def noisy_EI_lines(x,GP):
  with torch.no_grad(), gpytorch.settings.fast_pred_var():
    model = GP['model']
    likelihood = GP['likelihood']
    model.eval()
    likelihood.eval()
    x_prime_vals = torch.linspace(0,3*np.pi,200)
    predicted_output = likelihood(model(x))
    mean = predicted_output.mean.item()
    var = predicted_output.variance.item()
    a = []
    b = likelihood(model(x_prime_vals)).mean.numpy()
    for x_prime in x_prime_vals.reshape(-1,1):
      conc_x = torch.cat((x,x_prime))
      covar = likelihood(model(conc_x)).covariance_matrix
      a.append((covar[0,1]/var).item())
    dom_a,dom_b,interval_store = upper_env(np.array(a),np.array(b))
    N = torch.distributions.Normal(0,1)
    sum = 0
    for i in range(len(interval_store)):
      c_i = torch.tensor(interval_store[i,0])
      c_i1 = torch.tensor(interval_store[i,1])
      sum += dom_b[i]*(N.cdf(c_i1) - N.cdf(c_i)) + dom_a[i]*(np.exp(N.log_prob(c_i)) - np.exp(N.log_prob(c_i1)))
  return sum.item()
```

```{python}
#| echo: false
#| fig-align: center
model,likelihood = build_GP(x_data,y_data,2000)
GP = {'model':model,'likelihood':likelihood}

def plot_aq_lines(ax,GP,rec_time=False):
  GP['model'].eval()
  GP['likelihood'].eval()
  n_eval = 100
  x_vals = np.array(np.linspace(0,3*np.pi,n_eval).reshape(-1,1),dtype=np.float32)
  s = time.time()
  y_vals = [noisy_EI_lines(x_val,GP) for x_val in torch.from_numpy(x_vals)]
  e = time.time()
  y_vals = np.array(y_vals,dtype=np.float32)
  x_vals = np.array(x_vals,dtype=np.float32)[:,0]
  ax.plot(x_vals,y_vals,c='k',lw=1)
  ax.fill_between(x_vals,np.ones(n_eval)*np.min(y_vals),y_vals,alpha=0.1,color='k',lw=0)
  ax.set_xlabel(r'$x$')
  ax.set_ylabel(r'$\alpha_{EI}(x)$')
  ax.set_xlim([0,3*np.pi])
  ax.set_ylim([np.min(y_vals),np.max(y_vals)])
  ax.spines['right'].set_visible(False)
  ax.spines['top'].set_visible(False)
  if rec_time:
    print('UPPER ENVELOPE TIME TAKEN: ',np.round(e-s,4),'s. AVERAGE TIME PER EVALUATION: ',np.round((e-s)/n_eval,4),'s')
  ax.set_yticks([],[])
  return ax

fig,ax = plt.subplots(2,1,figsize= (figsize[0],figsize[1]*2),sharex=True,constrained_layout=True)
ax[0] = plot_aq(ax[0],GP,rec_time=True)
ax[1] = plot_aq_lines(ax[1],GP,rec_time=True)
ax[0].set_title('Naive Implementation')
ax[1].set_title('Analytical Upper Envelope')
fig.tight_layout()
plt.show();

```