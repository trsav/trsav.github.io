---
title: "The Potential Literature Horseshoe"
subtitle: "The First Large Language Model or: Has Machine Learning Solved Oulipo?"
author: "Tom Savage"
categories: [Art, Machine Learning]
toc: true
resources:
- logo.png
- other_logo.png
- dep.png
- dep2.png
- dep3.png
- nn.png
- nn2.png
- oulipo.jpeg
- raymond.jpeg
toc-location: body
format:
    # pdf: default
    html: 
      grid: 
        margin-width: 350px
date: "01/09/2024"
fig-cap-location: margin
citation-location: margin
reference-location: margin
bibliography: index.bib

---
```{ojs}
//| echo: false
defines = ['defined', 'specified'];
structure = ['structure','form','composition']
may = ['may','could','might','may well']
language = ['language','literature','text','writing'];
books = ['3000 year old Beowolf','600 year old Sir Gawain and the Green Knight'];
purely = ['purely','solely']
millennia = ['millennia','hundreds of years','thousands of years','centuries']
produce = ['produce','construct','create']
techniques = ['techniques','methods']
writers = ['writers','linguists','proponents','members'];
```



::: {.cleanbox}
If you refresh this page, the article will be randomised. There are  $2.66 \times 10^{19}$ potential combinations of images and text, resulting in approximately Twenty Billion Billion articles, equivalent to the number of insects on earth. _Vingt Milliards de Milliards_.
:::


```{ojs}
//| echo: false
rnd = Math.floor(Math.random() * 2)
s_string = {
  if (rnd === 0) {
    return html`<img src="logo.png" width=70% title="Basile Morin, [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0)">`
  }
  else {
    return html`<img src="other_logo.png" width=60% title="Basile Morin, [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0)">`
  }
}
```


::: {.center}
${s_string}
:::
:::{.column-margin}
Basile Morin, [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0)
:::

Ouvroir de Littérature Potentielle or _Oulipo_ has been ${defines[Math.floor(Math.random() * defines.length)]} as "the search for new structures, which may be used by writers in any way they see fit" [@motte1998oulipo,p2-3]. 
For ${millennia[Math.floor(Math.random() * millennia.length)]}, authors have been guided by language constraints resulting in ${defines[Math.floor(Math.random() * defines.length)]} ${structure[Math.floor(Math.random() * structure.length)]}s; From the alliterative verse of the ${books[Math.floor(Math.random() * books.length)]}, to the syllabic rigidity of Japanese haikus. By formalising a number of individual vocations, the original Oulipo ${writers[Math.floor(Math.random() * writers.length)]} in the early 1960s were embarking on the revitalisation of ${language[Math.floor(Math.random() * language.length)]} generation in search of discovering something deeper, sound familiar?

```{ojs}
//| echo: false
rndl = Math.floor(Math.random() * 2)
l_string = {
  if (rnd === 0) {
    return html`<a href="https://en.wikipedia.org/wiki/A_Void">not using the letter e</a>`
  }
  else {
    return html`replacing every noun with the seventh next noun in the dictionary (S+7 rule)`
  }
}

o_string = {
  if (rnd === 0) {
    return html`replacing every noun with the seventh next noun in the dictionary (S+7 rule)`
  }
  else {
    return html`<a href="https://en.wikipedia.org/wiki/A_Void">not using the letter e</a>`
  }
}

```

In order to define/identify new ${structure[Math.floor(Math.random() * structure.length)]}s, ${writers[Math.floor(Math.random() * writers.length)]} in France turned to automated transformation ${techniques[Math.floor(Math.random() * techniques.length)]}.
These often instantiated as rules or constraints, such as ${l_string}, ${o_string}, or [only using a single vowel letter](https://en.wikipedia.org/wiki/Eunoia_(book)). 
As the originators of Oulipo explored ways to manipulate language and text in search of new ${structure[Math.floor(Math.random() * structure.length)]}s, they turned to mathematics for inspiration, thus leading to the final vocation; the transposition of mathematics to words. Naturally the extension was made to computers, and the combinatorial nature of ${language[Math.floor(Math.random() * language.length)]} generation was quickly highlighted, as mathematician Claude Berge writes [@motte1998oulipo,p152] :

> [...] we believe, that the aid of a computer, far from _replacing_ the creative act of the artist, permits the latter rather to liberate himself from the combinatory search, allowing him also the best chance of concentrating on this "clinamen"^[Clinamen (/klaɪˈneɪmən/;) is the Latin name Lucretius gave to the unpredictable swerve of atoms, in order to defend the atomistic doctrine of Epicurus. [...] it has come more generally to mean an inclination or a bias. [Source](https://en.wikipedia.org/wiki/Clinamen)] which, alone, can make of the text a true work of art.
 
In applying _combinatory literature_, Oulipo ${writers[Math.floor(Math.random() * writers.length)]}, most notably Raymond Queneau in his 1961 work _Cent Mille Milliards de Poemes_ (Hundred Thousand Billion Poems), have highlighted the impossible complexity of language. Elegantly embracing this complexity, Queneau simply presents the complete set of lines within a sonnet in the form of cut out strips, any combination of which ${may[Math.floor(Math.random() * may.length)]} be constructed by the reader. 
In describing the combinatorial nature of _Cent Mille Milliards de Poemes_, Berge presents the following figure...

```{ojs}
//| echo: false
rnd_nn = Math.floor(Math.random() * 2)
nn_string = {
  if (rnd === 0) {
    return html`<img src="nn.png" width=70%>`
  }
  else {
    return html`<img src="nn2.png" width=100%>`
  }
}
```

:::{.center}
${nn_string}
:::

Verses act equivalently to neural network layers, and phrases correspond to discrete nodes... could we be seeing the early sparks of the use of neural networks for language modelling?^[The modern use of neural networks to model language can _probably_ be attributed to [Bengio et. al, 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf), 42 years later.]
Of course, what is missing is the mathematical transformations from layer to layer, token embeddings, attention etcetera... but it is interesting to consider the thematic similarities of complexity, ${structure[Math.floor(Math.random() * structure.length)]}, and distillation (or lack thereof) of ${language[Math.floor(Math.random() * language.length)]} that both members of Oulipo and machine learning researchers have successfully applied.

```{ojs}
//| echo: false
rnd_gp = Math.floor(Math.random() * 2)
gp_string = {
  if (rnd === 0) {
    return [html`<img src="oulipo.jpeg" width=60%>`,html`<em>Oulipo Group à Boulogne, avenue de la Reine, on September 23, 1975, in the garden of François Le Lionnais’s house. At the center Raymond Queneau and François Le Lionnais, on his side, with a nespaper in the hands. On the very left Italo Calvino. <a href="https://www.arshake.com/en/oulipo-ouvroir-de-litterature-potentielle/">Source</a>.</em>`]
  }
  else {
    return [html`<img src="raymond.jpeg" width=60%>`,html`<em>Raymond Queneau with the group Les Frères Jacques in 1954, Getty Images/Keystone.<a href="https://ici.radio-canada.ca/ohdio/premiere/emissions/aujourd-hui-l-histoire/segments/entrevue/351266/oulipo-richard-boivin">Source</a>.</em>`]
  }
}
```

::: {.center}
${gp_string[0]}
:::
${gp_string[1]}



Given that a modern day large language model such as [GPT-4](https://openai.com/research/gpt-4) ${may[Math.floor(Math.random() * may.length)]} reasonably be described as a very large set of well ${defines[Math.floor(Math.random() * defines.length)]} mathematical rules, could this ever ${produce[Math.floor(Math.random() * produce.length)]} a valid literary ${structure[Math.floor(Math.random() * structure.length)]}? How are LLMs distinct from more basic rules such as S+7? In addition, multi-model models can now see and hear as well as read. [According to David Chalmers](https://nips.cc/media/neurips-2022/Slides/55867.pdf) these additional senses ${may[Math.floor(Math.random() * may.length)]} result in fish-level consciousness in the next ten years. Consciousness aside, at the very least LLMs will _appear_ smarter than a human by then. What are the implications for Potential Literature when a set of rules and resulting ${structure[Math.floor(Math.random() * structure.length)]} can think for itself?

Unlike the output from an LLM, a haiku can be instantly recognised and verified as coming from its defining 5-7-5 syllabic writing ${structure[Math.floor(Math.random() * structure.length)]}. When the ${structure[Math.floor(Math.random() * structure.length)]} itself is visible in the output, the focus is turned ${purely[Math.floor(Math.random() * purely.length)]} to what Berge refers to as the _clinamen_. It is here that the meaning is gained. 

The difference between LLMs and a well-established language ${structure[Math.floor(Math.random() * structure.length)]} such as a haiku is one of complexity, and the ${language[Math.floor(Math.random() * language.length)]} that results. By extending a series of basic transformations and rules towards LLMs, a horseshoe effect is achieved. The sheer quantity of rules within an LLM reflects a _removal_ of linguistic constraints, order is returned from chaos, and the resulting underlying ${structure[Math.floor(Math.random() * structure.length)]} is obscured.^[Though for now (Jan 2024) LLMs produce hallucinations, incorrect facts and statements which in turn partially disclose the underlying structure of the neural network.]

```{ojs}
//| echo: false
rnd_d = Math.floor(Math.random() * 3)
dep_string = {
  if (rnd_d === 0) {
    return html`<img src="dep.png" width=60%">`
  }
  else if (rnd_d === 1) {
    return html`<img src="dep2.png" width=60%>`
  }
  else if (rnd_d === 2) {
    return html`<img src="dep3.png" width=60%>`
  }
}
```

::: {.center}
${dep_string}
:::

Herein lies a paradox for the use of language models as a direct tool for creativity, as a platform for a distinct linguistic structure. A language model is trained to ${produce[Math.floor(Math.random() * produce.length)]} text that is indistinguishable from the text within its training distribution.
However, as soon as it achieves this task, the underlying structure becomes obscured as complete 'order' is achieved, and the impact is lost. 
The relationship between the rules and their resulting linguistic ${structure[Math.floor(Math.random() * structure.length)]} is distinctly one-way, it is _nearly_^[This _nearly_ contains a level of nuance, as identifying whether an output has been generated by an LLM is an open research area (See [_On the Reliability of Watermarks for Large Language Models_](https://arxiv.org/abs/2306.04634) and [_A Watermark for Large Language Models_](https://arxiv.org/abs/2301.10226)). However, the point remains. To the average reader, without the use of cryptographic tools, modern LLM output (as of Jan 2024) remains practically indistinguishable from human-level text.] impossible to reason whether the content has been generated from a neural network, yet alone infer the parameters, or even recognise the architecture. Would a haiku still be a haiku if you couldn't immediately infer its syllabic structure?

To answer the question posted in the title: _Has Machine Learning Solved Oulipo?_ The answer, is no. The beauty of Potential Literature is that by enforcing literary constraints and therefore defining new ${structure[Math.floor(Math.random() * structure.length)]}s, we not only place the emphasis ${purely[Math.floor(Math.random() * purely.length)]} on the meaning, but also enable the reader to interpret ${language[Math.floor(Math.random() * language.length)]} in new ways through a sense of disorder. As a set of rules and constraints, an LLM succeeding in its objective only serves to obscure the resulting linguistic structure in a bid to 'seem human', regaining order from chaos.

It is clear that these two vocations in the field of Potential Literature: defining rules and constraints for ${language[Math.floor(Math.random() * language.length)]}, as well as identifying new language structures, are inherently linked. However, these two objectives conflict when rule set is as flexible as it is within an LLM.

As Italo Calvino wrote in late 1967, in a remarkable foreshadowing [@motte1998oulipo, Cybernetics and Ghosts (1967) p13-14]:

> The true literature machine will be one that itself feels the need to produce disorder, as a reaction against its preceding production of order: a machine that will produce avant-garde work to free its circuits when they are choked by too long a production of classicism. [...]. To gratify critics who look for similarities between things literary and things historical, sociological, or economic, the machine could correlate its changes of style to the variations in certain statistical indices of production, or income, or military expenditure, or the distribution of decision-making powers. **That indeed will be the literature that corresponds perfectly to a theoretical hypothesis: it will, at last, be _the_ literature.**

Calvino, I'm sure would argue, that we have the alignment of LLMs all wrong to ${produce[Math.floor(Math.random() * produce.length)]} _truly_ novel literature, and how would Raymond Queneau respond to the combinatorial possibilities of large-language models?

_Cent Mille Milliards de Poemes_ can be seen as analogous to a partially trained neural network _itself_, as a mathematical object. There is beauty in the disorder of the relationship between tokens, and the rules that can be combined to ${produce[Math.floor(Math.random() * produce.length)]} language. However as soon as an output is produced and the combinatorics collapses, the ${structure[Math.floor(Math.random() * structure.length)]} is obscured, and like a phantom the meaning disappears. 

