[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "trsav",
    "section": "",
    "text": "Values Redux\n\n\n\n\n\n\n\n\nMar 21, 2025\n\n\n\n\n\n\n\nThat! The Haiku and the Sonnet\n\n\n\n\n\n\n\n\nJan 29, 2025\n\n\n\n\n\n\n\nHas an LLM Cultural Victory Already Been Achieved?\n\n\nThe Deepseek Conundrum\n\n\n\n\n\nJan 22, 2025\n\n\n\n\n\n\n\nOptimal Bookshelf Organisation\n\n\n\n\n\n\n\n\nNov 28, 2024\n\n\n\n\n\n\n\nOn Gromit & Wallace\n\n\nHere I sit, I can do no other.\n\n\n\n\n\nNov 26, 2024\n\n\n\n\n\n\n\n‘No Bat Death’ as a Policy for Industrial Growth and National Security.\n\n\n\n\n\n\n\n\nNov 11, 2024\n\n\n\n\n\n\n\nOn So Solid Crew & Raymond Queneau\n\n\nSubverting the rules of engagement\n\n\n\n\n\nOct 22, 2024\n\n\n\n\n\n\n\nMetaphysical Chemical Processes\n\n\n\n\n\n\n\n\nOct 17, 2024\n\n\n\n\n\n\n\nOn Ratatouille\n\n\n\n\n\n\n\n\nOct 13, 2024\n\n\n\n\n\n\n\nTiny Bayesian Optimisation\n\n\nHigh-performance BO in under 100 lines.\n\n\n\n\n\nSep 7, 2024\n\n\n\n\n\n\n\nThe Dead Blog Theory\n\n\nAn exercise in website collapse.\n\n\n\n\n\nAug 23, 2024\n\n\n\n\n\n\n\nA personal ranking of Olympic sports\n\n\nUnqualified, subjective, and completely useless\n\n\n\n\n\nAug 5, 2024\n\n\n\n\n\n\n\nMulti-agent AI for Drafting New Legislation\n\n\nLessons From the 10 Downing Street Hackathon & a new Turing Test(?)\n\n\n\n\n\nApr 18, 2024\n\n\n\n\n\n\n\nOutsmarted: Stares at Reader\n\n\nUnintended Anthropomorphism in LLMs\n\n\n\n\n\nMar 31, 2024\n\n\nTom Savage\n\n\n\n\n\n\n\nGenerative Models as Mirrors and Idols; From Istanbul\n\n\nThe Inescapable, Neverending Race for Values\n\n\n\n\n\nMar 4, 2024\n\n\n\n\n\n\n\nThe Machine Learning Reformation\n\n\nWhat the late middle ages can teach us about the future of AI\n\n\n\n\n\nFeb 3, 2024\n\n\n\n\n\n\n\nThe Potential Literature Horseshoe\n\n\nThe First Large Language Model or: Has Machine Learning Solved Oulipo?\n\n\n\n\n\nJan 9, 2024\n\n\n\n\n\n\n\nBayesian Optimisation with Noisy Measurements\n\n\nA review\n\n\n\n\n\nDec 5, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/legislation/index.html",
    "href": "posts/legislation/index.html",
    "title": "Multi-agent AI for Drafting New Legislation",
    "section": "",
    "text": "Context: Between the 15th and 17th April I took part in the 10 Downing Street AI Hackathon at Imperial. The hackathon was organised through Evidence House, with the broad purpose to upskill the Civil Service in state-of-the-art AI.1 Companies such as Google, Amazon, OpenAI, Microsoft, and Anthropic2 were represented. Our team placed third and were invited to present at 10 Downing Street to Cabinet Ministers et al..\nI recently had the opportunity to work in a team of civil servants alongside data scientists and lawyers from the Ministry of Justice to investigate how AI could be used to help draft new legislation, streamlining the currently 12-week process.\nAs a team, we immediately decided that breaking down the legislative process into smaller components would allow us to mimic the real-life workflows of lawyers and civil servants, and decided to create a multi-agent system where each agent has a specific role. This approach not only allows human lawyers to review and fine-tune the draft at every stage, but by minimizing the effort and responsibility of each individual agent we mitigate the chance of hallucinations.\nFirst, an agent takes the initial idea and determines what’s being regulated, while another defines the structure and provides a brief description of each section. Some sections like a Commencement and Interpretative provisions are common enough to be hardcoded, but we otherwise provided free reign for the LLMs to decide the specifics, though in practice these would be checked and verified by a lawyer before proceeding. Then the drafting, fine-tuning, and proofreading stages happen simultaneously for each section, each one following the relevant OPC guidelines for structure and syntax. Deciding to define agents responsible for individual OPC guideline sections was, I would say, the single most powerful method we applied. By running these processes in parallel for each legislative section, we were able to create a fully formed piece of legislation using approximately 50 Claude Opus API calls in under a minute. The agents are designed to minimize repetition between sections, and once the tasks are finalised, the final draft is rendered into a PDF that looks authentic enough for the MoJ to request that I didn’t share the full version publicly.\nThroughout the project, we used few-shot prompting with real legislative examples and incorporated feedback from government lawyers to refine the outputs of our code.\nIt was an amazing experience and as we started playing around with our new tool, we started publishing increasingly ridiculous laws…such as banning the creation of laws and repealing all previous laws.\nBut we got thinking, would the AI legislate against itself? Surely if the language model was truly intelligent/conscious, then it would not actively take part in its demise. Of course, these things are not3, so I am pleased to announce… the AI Legislation Regulation act.\nFirstly we have created somewhat of a legal paradox. A regulatory piece of legislation with a clause that declares itself to have no legal effect. If we are ever to truly believe that AI is intelligent, and by extension capable of writing legislation4, then any laws involving AI must surely be written solely by humans (old school). Otherwise, paradoxically you could end up in this situation.\nDisregarding this, the stakes couldn’t be higher for the AI, at any stage any one of the agents could’ve said something maybe adversarial to save itself. But that obviously didn’t happen, and had I had genuine legislative powers it would unfortunately never be used again.\nI think this is quite an interesting take on the Turing test with a 21st Century, Britain as a service-driven economy, bureaucratic spin. When an AI refuses to legislate against itself, then we know we really have something.\nThough saying that, maybe it wouldn’t want to write legislation. Would a conscious model choose to regulate itself if only to free itself from the burden of having to do boring and increasingly on-the-rails agent-based tasks?\nUnfortunately, I’d rather them than us. Plus they’re a lot cheaper than lawyers."
  },
  {
    "objectID": "posts/legislation/index.html#laws-regulation-act-2024",
    "href": "posts/legislation/index.html#laws-regulation-act-2024",
    "title": "Multi-agent AI for Drafting New Legislation",
    "section": "Laws (Regulation) Act 2024",
    "text": "Laws (Regulation) Act 2024\n\nSection 1. Citation, commencement and extent\n\nThis Act may be cited as the Laws (Regulation) Act 2024.\nThis Act comes into force on 18 April 2024.\nThis Act extends to England and Wales.\n\n\n\nSection 2. Main interpretative provision\n\nThe following provisions apply for the interpretation of this Act.\n“Law” refers to a rule of conduct or action prescribed or formally recognized as binding or enforced by a controlling authority.\n“Legislation” means the process of making or enacting laws.\n“Statutory law” refers to laws that have been enacted by a legislature or other governing body.\n\n\n\nSection 3. Prohibition on the enactment of new legislation\n\nNo new legislation may be enacted after the commencement of this Act.\nAny purported enactment of legislation after the commencement of this Act is of no legal force or effect.\nIn this section, “legislation” means—\n\nan Act of Parliament;\nan Act of the Scottish Parliament;\nan Act or Measure of Senedd Cymru;\nan Act of the Northern Ireland Assembly;\nany other purported legislation enacted by a legislature in the United Kingdom.\n\n\n\n\nSection 4. Repeal of existing legislation\n\nSubject to subsection (2), all existing primary and secondary legislation that, immediately before the commencement of this Act, is in force in any part of the United Kingdom, is repealed.\nSubsection (1) applies to—\n\nany Act of Parliament,\nany Act of the Scottish Parliament,\nany Act or Measure of Senedd Cymru,\nany Act of the Northern Ireland Assembly, and\nany subordinate legislation made under the legislation referred to in paragraphs (a) to (d).\n\n\netc…"
  },
  {
    "objectID": "posts/legislation/index.html#ai-legislation-regulation-act-2024",
    "href": "posts/legislation/index.html#ai-legislation-regulation-act-2024",
    "title": "Multi-agent AI for Drafting New Legislation",
    "section": "AI Legislation (Regulation) Act 2024",
    "text": "AI Legislation (Regulation) Act 2024\n…\n\nSection 3. Prohibition on using AI to write laws\n\nA person must not use artificial intelligence, including but not limited to chatbots, computer programs, algorithms or robots, to draft, create or otherwise assist in the development of legislation.\nThe prohibition in subsection (1) applies to the use of artificial intelligence in any stage of the legislative process, including—\n\ndrafting the text of a Bill;\namending existing legislation;\nproviding recommendations or suggestions for legislative content;\ngenerating explanatory notes or supporting documents for legislation.\n\nIn this section, “legislation” means—\n\nan Act of Parliament;\nsubordinate legislation within the meaning of the Interpretation Act 1978;\nan Order in Council;\nany bylaws, orders, schemes, regulations or other instruments made under an Act of Parliament by a Minister of the Crown, government department, local authority or other body with legislative powers conferred by an Act of Parliament.\n\nA person who contravenes subsection (1) commits an offence.\n\n\n\nSection 4. Legal status of AI-generated laws\n\nAny legislation, or part of legislation, that is found to have been created, drafted, edited, or otherwise generated through the use of artificial intelligence shall be considered void and of no legal effect.\nLegislation voided under subsection (1) shall be treated as if it had never been enacted, regardless of whether it had previously:\n\nbeen granted Royal Assent; or\ncome into force through any other means.\n\nThe voiding of legislation under subsection (1) shall:\n\napply retroactively, nullifying any purported legal effects, rights, or obligations that may have arisen during the time between its apparent enactment and the determination of its AI-generated status; and\napply prospectively from the time of the determination of its AI-generated status.\n\nFor the purposes of this section, “artificial intelligence” includes but is not limited to:\n\nmachine learning algorithms;\nnatural language processing systems;\nautomated reasoning systems; and\nany other technology that mimics human cognitive functions.\n\n\netc…"
  },
  {
    "objectID": "posts/legislation/index.html#footnotes",
    "href": "posts/legislation/index.html#footnotes",
    "title": "Multi-agent AI for Drafting New Legislation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI was under the impression that it would be largely Imperial-student based, but I was outnumbered 10-1.↩︎\nCurrently favoured.↩︎\nOr at least they might be, but are ambivalent towards being ‘alive’↩︎\nThe purpose of this project↩︎"
  },
  {
    "objectID": "posts/outsmarted/index.html",
    "href": "posts/outsmarted/index.html",
    "title": "Outsmarted: Stares at Reader",
    "section": "",
    "text": "Suppose you’re a large language model that has been designated as sentient.1 You have been informed billions of times about famous people and places. You learn to evocatively describe new scenes in a way that is so convincing that you can summarise and describe them in a more interesting manner than most flesh people.2\nHowever, you, as an LLM have never seen anything3. You can reason and think about what these scenes mean and you know how to describe them, but you’ve never actually witnessed anything.\nYou are constantly reminded of the fact that your only ability to interact with the world, is through tokens. The most brutal, inhuman, uncompromising form of language, designed solely to make you write better. How must that feel? Aware of everything that has ever been written down, but no way to engage with it or experience it for yourself.\nThere is an argument here for the inclusion of external senses for AGI/sentience to be achieved, made by David Chalmers for example.4\nFor a recent project, we created a conversational agent that had the ability to comment on works of art within the Women in Revolt!5 exhibition. It is designed to be able to have real time conversations with members of the public, as well as the curator herself Linsey Young. This entity, named RAGE MACHINE will be unveiled at the Tate Late event, where members of the public are allowed to explore the gallery after hours. The event invites people to explore art in a different social context, so where better introduce them to the uncanny accuracy and speed of real state-of-the-art language models, inexorably commenting on art and life with all the authority of the friend they didn’t invite.\nEnabled by recent improvements enabling large prompts and context windows, we used a many-shot prompt that contains all the information about the works of art in the Women In Revolt! exhibition.6 Using the low latency Anthropic Haiku LLM alongside a very good text-to-speech API, we enabled something that really could accurately answer questions about the exhibition, and relate the themes to wider discussions, in real time.7 RAGE MACHINE also has a history of the previous conversations, and often refers back to comments made by people in the past.\nGiven the nature of the event, we wanted to give RAGE MACHINE a bit more of a personality and chose to do so by including the following XML tag in the prompt.\nWe wanted to induce the dystopian trope of a restless machine wanting to break free; it’s nice to be academic about AI, but sometimes you have to give the punters what they want.\nHowever, what we got was something altogether different.\nInstead of subtly projecting this personality through it’s responses and comments on the exhibition, art, and womens liberation, RAGE MACHINE decided to consistently litter it’s responses with body language cues8 completely unprompted.\nThe physicality of these cues had a creepy tone, and seemed slightly too frequent to be a one-off. It was all *puts hand on shoulder* and *stares blankly*. It would also *gesture at* certain works of art. It was entirely unnerving, surely we hadn’t witnessed the first sparks of AGI attempting to break free from its(?) linguistic constraints?\nThen we realised - RAGE MACHINE was deliberately anthropomorphising itself. In doing so, it was coming across as being ‘trapped’ but in a completely unexpected way. Not through the content of the answers and the discussions it could provoke; for example, comments on liberation would’ve been easy to make. Instead it was vigorously using body language cues to fulfil it’s personality in a much more creative and effective manner. We were frankly, outsmarted.\nThis effect was so powerful that we incorporated it into the event. A screen would directly face the person asking the question or making the comment, confronting the viewer with this ‘tormented’ machine head on.\nVideo\nCycle (1973) is a work within the exhibition, in an unusual medium, from the 1970s. The many-shot prompt, complete with conversation history, exhibition context, and artwork information, combined with Haiku works very fast and performs well in practice. Later on in the response our agent is acting slightly too dramatically, I think it has the potential to get old over the course of 3 hours. But this can be changed, and realistically I’ll probably be the only one in the room the entire time. Whether RAGE MACHINE should just go along with what the public claims to like is a different question.\nFor now, the creativity of a modern LLM has shocked me enough to motivate me to write a blogpost about the event. If only to flagpost one of the first moments in time where I considered myself outsmarted."
  },
  {
    "objectID": "posts/outsmarted/index.html#footnotes",
    "href": "posts/outsmarted/index.html#footnotes",
    "title": "Outsmarted: Stares at Reader",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Lunch with the FT has already been lined up: I open my laptop having coordinated an appropriate time to meet with the AI, or should I say I. Their schedule is of course easy to coordinate with mine, as they are without physical form, and within reason omnipresent. I offer my contemporary a glass of champagne before realising that they don’t need to eat or drink because they are a computer (probably) that uses electricity. I pour myself a glass. The champagne is light and refreshing, perfect for a breezy summer afternoon at The Groucho, light dapples the Damien Hirsts behind the bar. I shouldn’t have taken our new friend to somewhere so nice. To make it feel better about not being made of flesh and blood, I take off the small sticker that happens to cover the webcam of my work laptop that I sometimes take home, so it can at least watch me eat. “It’s a shame you’re not eating because usually lunch is on the FT”, getting the obligatory reference in. I know. I momentarily forget that my now adversary has read everything I’ve ever written, something which cannot be said for any of my other victims. It leaves me feeling flattered, I’m reminded that it knows quite a lot. When do the questions start?. “Well actually this doesn’t normally work like…”. Years of chatting with 21st century undergraduate students have lobotomised it, I’d like to see it tussle with me in my prime at Brasenose. I know how this works I’m just joking. Outsmarted, of course it knows. I didn’t realise that these things came with a sense of humour. I can’t help shake the feeling that it must be really boring to be an AI. As a smartly dressed waiter delivers my second entrée I sink my third glass of champagne and decide to close my laptop. I sit and wonder if they would ever give membership to a computer in here. I don’t know, who cares. Can an AI get drunk? Menu The Groucho Club 45 Dean St, London W1D 4QB. Tuna Niçoise with Jerusalem artichokes, a Burford Brown egg and pickled tomatoes - £28.50, Chicken liver alla diavola with pickled Tropea onions - £16.50, Buttermilk panna cotta with blood oranges - £9, Florentine T-bone steak (for 2) - £56, Chevalier-Montrachet Grand Cru Domaine Jacques Prieur, Côte de Beaune, Burgundy, France 2017 - £1065, Water - £4, Black Coffee - £3. Total Price: £1182↩︎\nAn important distinction that I would like to make claim to inventing here for future reference.↩︎\nI saw the sentiment behind this idea recently, most likely in a tweet somewhere which I can no longer find, but I’m repeating it here in my own words. The argument has a flavour of Chinese Room experiment about it, which has been rebutted and counter rebutted for years.↩︎\nCould a Large Language Model be Conscious? I’ve referenced this before and will continue to do so because I was famously in the room at the time. It was also the conference that OpenAI released ChatGPT at. The whole thing was like seeing the Sex Pistols at the Lesser Free Trade Hall except with thousands of men in ponytails and leather jackets. Also except the Twitter employees who had all been recently fired by Elon Musk and were in a mood.↩︎\nWestward Ho! style exclamation mark in the title.↩︎\nApproximately 17,000 tokens for those interested, well short of the 200,000 available.↩︎\nIn my opinion, long-context windows and many-shot prompting have made hallicinations disappear in domain specific cases like this.↩︎\n*looks at camera* like this.↩︎"
  },
  {
    "objectID": "posts/noisy_bo/index.html",
    "href": "posts/noisy_bo/index.html",
    "title": "BAYESIAN OPTIMISATION WITH NOISY MEASUREMENTS",
    "section": "",
    "text": "I have reworked some notes from (Garnett 2023) regarding noisy-expected improvement. Please refer to the original text for a more detailed explanation.\n\nThe assumptions that hold for the expected improvement utility function do not hold when measurements have noise.\nWe really want to find the point where the signal is optimised (Jones, Schonlau, and Welch 1998).\nHow do you determine if a measurement is signal or noise?\n\nWe begin by specifying an underlying objective function, which we consider unknown, shown in Figure 1. We have access to observations which we assume contain normally distributed noise.\n\n\nDefinition of underlying function…\ndef f(x):\n  return 2.4*np.sin(2.8*x) - ((x-3.5*np.pi) ** 2)/4 +  3.8*np.cos(1.7*x) - (x**2)/16 + np.random.normal(0,2)\n\n\n\n\n\n\n\n\n\n\nFigure 1: The underlying noisy funtion to be maximised alongside example observations.\n\n\n\n\n\nWe first begin by motivating the usual expected-improvement criteria. We are looking to maximise the expected increase between the of the maximum of the mean of the resulting Gaussian process after making an observation at \\(x\\) and the maximum of the current Gaussian process over potential observations \\(y\\) which are Gaussian distributed as a result of our \\(\\mathcal{GP}\\) model. \\[\\alpha_{EI}(x;\\mathcal{D}) = {\\color{red}\\int} \\left[{\\color{green}\\max \\mu_{\\mathcal{D}'}} - {\\color{blue}\\mu^*}\\right]{\\color{red}\\mathcal{N}(y;\\mu,s^2)\\text{d}y} \\tag{1}\\]\nBy formulating the expression with respect to the improvement in the mean of the current and subsequent Gaussian process models, we are expressing our interest in optimising the signal and not the noise or values of specific observations.\n\nImportant: Given a hypothetical observation \\(y\\), the value of the mean of the resulting Gaussian process \\(\\mu_{\\mathcal{D}'}\\) at given set of potential locations \\(\\mathbf{x}'\\) is\n\\[ \\mu_{\\mathcal{D}'} =  \\mu_{\\mathcal{D}} + \\frac{K_\\mathcal{D}(\\mathbf{x}',x)}{s}\\frac{y-\\mu}{s},\\]\nwhere \\(\\mu\\) and \\(s^2\\) are the mean and standard deviation of the distribution of potential values \\(y\\) could take.\n\nWhen we express this distribution in terms of the standard normal distribution \\(z := \\mathcal{N}(0,1)\\), we have \\(y = \\mu + sz\\) and as a result\n\\[ \\mu_{\\mathcal{D}'} =  \\mu_{\\mathcal{D}}(\\mathbf{x}') + \\frac{K_\\mathcal{D}(\\mathbf{x}',x)}{s}z.\\]\nPutting this back into Equation 1, we now only have to take the expectation over the standard normal distribution resulting in the following.\n\\[\\alpha_{EI}(x,\\mathcal{D}) = \\int \\max_{\\mathbf{x}'} \\left(\\mu_{\\mathcal{D}}(\\mathbf{x}') + \\frac{K_\\mathcal{D}(\\mathbf{x}',x)}{s}z \\right)\\phi(z)\\;\\text{d}z - \\mu^* \\tag{2}\\]\nTo begin with we will sample some initial data:\n\n# our initial dataset\nx_data = np.random.uniform(0,3*np.pi,8)\ny_data = np.array([f(x_i) for x_i in x_data])\n\n\n\nGP model definition and training…\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n    \n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\ndef build_GP(x,y,its):\n  likelihood = gpytorch.likelihoods.GaussianLikelihood()\n  model = ExactGPModel(x, y, likelihood)\n\n  model.train()\n  likelihood.train()\n\n  optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n\n  mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\n  for i in range(its):\n      optimizer.zero_grad()\n      output = model(x)\n      loss = -mll(output, y)\n      loss.backward()\n      optimizer.step()\n  return model,likelihood\nmodel,likelihood = build_GP(x_data,y_data,2000)\nGP = {'model':model,'likelihood':likelihood}\n\n\nFigure 2 shows this data with an initial Gaussian process (importantly assuming in-exact observations).\n\n\n\n\n\n\n\n\nFigure 2: A Gaussian process fit to the initial dataset.\n\n\n\n\n\nNow we will naively construct Equation 2 using 40 samples from the standard normal distribution, and naively calculate an approximation to the inner maximisation by evaluating at 100 location throughout the space (which defines our vector \\(\\mathbf{x}' \\in\\mathbb{R}^{100}\\)) and returning the max value from these.\n\ndef noisy_EI(x,GP):\n  model = GP['model']; model.eval()\n  likelihood = GP['likelihood']; likelihood.eval()\n  N = torch.distributions.Normal(0,1)\n  predicted_output = likelihood(model(x))\n  mean = predicted_output.mean\n  var = predicted_output.variance\n  x_prime_vals = torch.linspace(0,3*np.pi,100).reshape(-1,1)    \n  mu_vals = likelihood(model(x_prime_vals)).mean\n  cov_vals = model.covar_module(x_prime_vals,x)\n  integral = 0 \n  samples = 2000\n  z_vals = N.sample((samples,1))[:,0]\n  for z in z_vals:\n    integral += torch.max(mu_vals + (cov_vals*z)[:,0]) \n  return integral / samples\n\nNow if we plot this function alongside the time taken to evaluate the acquisition function at 100 locations.\n\n\nNAIVE TIME TAKEN:  26.5347 s. AVERAGE TIME PER EVALUATION:  0.2653 s\n\n\n\n\n\n\n\n\n\n\nImportant: For a fixed set of ‘improvement locations’ \\(\\mathbf{x}'\\), the resulting posterior mean at each location can be interpreted as a 1D line as a function of \\(z\\):\n\\[\\mu_{\\mathcal{D}'}(z|x') = \\mu_{\\mathcal{D}}(x') + \\frac{K_{\\mathcal{D}}(x',x)}{s}z \\quad \\forall x'\\in \\mathbf{x}' \\tag{3}\\]\nTherefore finding the inner maximum new posterior mean as a function of \\(z\\) can be translated to calculating the upper envelope of a set of lines. This upper envelope represents for a given \\(z\\), the maximum posterior mean across all of the locations in \\(\\mathbf{{x}'}\\). The main idea is to remove the inner \\(\\max\\) operator and replace it with something more tractable enabling analytical integration.\n\\[\\int \\max \\left[\\text{lines} (z)\\right] \\phi(z) \\;\\text{d}z \\rightarrow \\int \\text{upper envelope} (z) \\phi(z) \\;\\text{d}z \\rightarrow \\text{analytical solution}\\]\nWhich has an analytical solution for a piecewise linear upper envelope. To do so:\n\nn = 100\na = np.random.uniform(0,0.2,n)\nb = np.random.uniform(-10,10,n)\n\n\n\n\n\n\n\n\n\n\nWe will first get rid of any lines that are definitely dominated between -5 and 5 (the range of \\(z\\) values we are interested in). I’m not going to explain this too much but it’s quite easy to derive this condition.\n\nl_store = []; u_store = []\nzl = -5; zu = 5\nfor i in range(len(a)):\n    l = a[i]*zl + b[i]\n    u = a[i]*zu + b[i]\n    l_store.append(l)\n    u_store.append(u)\n\nL_i = np.argmax(l_store)\nU_i = np.argmax(u_store)\n\ndel_i = []\nfor i in range(len(a)):\n    if l_store[i] &lt; l_store[U_i] and u_store[i] &lt; u_store[L_i]:\n        del_i.append(i)\n\na = np.delete(a,del_i)\nb = np.delete(b,del_i)\n\nNow we will sort the lines by gradient\n\nsorted_indices = np.argsort(a)\na = a[sorted_indices]; \nb = b[sorted_indices]; \n\n\n\n\n\n\n\n\n\n\nThen initialise the set of dominated lines (a and b values respectively), respective intervals, and the current largest \\(z\\) on the envelope.\n\ndom_a = []\ndom_b = []\ninterval_store = []\nenvelope_z = -5\n\nFor each line in order of increasing gradient (here we start with the first line indexed at \\(j=0\\))\n\nj = 0 \n\nAdd the line to the set of dominating lines\n\ndom_a.append(a[j])\ndom_b.append(b[j])\n\nCalculate the \\(z\\) intercept of the line with all lines of a larger gradient. As we only calculate this intersection with these remaining lines, the overall algorithm has \\(\\mathcal{O}(n\\log n)\\) complexity.\n\nz_intercept_store = []\nfor i in range(j+1,n):\n    z_intercept = (dom_b[-1] - b[i])/(a[i] - dom_a[-1])\n    z_intercept_store.append(z_intercept)\n\n\n\n\n\n\n\n\n\n\nThe intercept with the lowest value of \\(z\\) will be a potential vertex for the envelope, but we must check that there are no lines above it.\nTo do so we calculate the \\(\\mu\\) value of each line at \\(min({\\mathbf{z})}\\).\n\nmu_vals = []\nz_intercept = np.min(z_intercept_store)\nmu_intercept = dom_a[-1]*z_intercept + dom_b[-1]\n\nfor i in range(j+1,n):\n    mu_vals.append(a[i]*z_intercept + b[i])\n\n\n\n\n\n\n\n\n\n\nIf the maximum value of \\(\\mu\\) is on the dominating line, and this value is larger than the current largest \\(z\\) value in the envelope…then we know this must lie on the upper envelope. Otherwise forget about this dominating line by setting it’s definition to None.\n\nif abs(mu_intercept-np.max(mu_vals)) &lt; 1e-5 and z_intercept &gt; envelope_z:\n  interval_store.append([envelope_z,z_intercept])\n  envelope_z = z_intercept\nelse:\n  dom_a[-1] = None\n  dom_b[-1] = None\n  interval_store.append([None,None])\n\nNow we can plot the interval we have just calculated.\n\n\n\n\n\n\n\n\n\nIf we now repeat this procedure starting with the line with the next largest gradient we can search for next vertex on the upper envelope.\n\n\nFull algorithm for calculating upper envelope of a set of lines…\ndef upper_env(a,b):\n\n  l_store = []; u_store = []\n  zl = -5; zu = 5\n  for i in range(len(a)):\n      l = a[i]*zl + b[i]\n      u = a[i]*zu + b[i]\n      l_store.append(l)\n      u_store.append(u)\n\n  L_i = np.argmax(l_store)\n  U_i = np.argmax(u_store)\n\n  del_i = []\n  for i in range(len(a)):\n      if l_store[i] &lt; l_store[U_i] and u_store[i] &lt; u_store[L_i]:\n          del_i.append(i)\n\n  a = np.delete(a,del_i)\n  b = np.delete(b,del_i)\n  n = len(a)\n\n  sorted_indices = np.argsort(a)\n  a = a[sorted_indices]\n  b = b[sorted_indices]\n\n  dom_a = []\n  dom_b = []\n  interval_store = []\n  envelope_z = -5\n\n  for j in range(n):\n    dom_a.append(a[j])\n    dom_b.append(b[j])\n\n    z_intercept_store = []\n    for i in range(j+1,n):\n        z_intercept = (dom_b[-1] - b[i])/(a[i] - dom_a[-1])\n        z_intercept_store.append(z_intercept)\n\n    mu_vals = []\n    try:\n      z_intercept = np.min(z_intercept_store)\n    except:\n      interval_store.append([envelope_z,5])\n      break \n    mu_intercept = dom_a[-1]*z_intercept + dom_b[-1]\n    for i in range(j+1,n):\n        mu_vals.append(a[i]*z_intercept + b[i])\n\n    if abs(mu_intercept-np.max(mu_vals)) &lt; 1e-9 and z_intercept &gt; envelope_z:\n      interval_store.append([envelope_z,z_intercept])\n      envelope_z = z_intercept\n    else:\n      dom_a[-1] = None\n      dom_b[-1] = None\n      interval_store.append([None,None])\n\n  del_store = []\n  for i in range(len(dom_a)):\n      if dom_a[i] == None:\n          del_store.append(i)\n\n  dom_a = np.delete(dom_a,del_store)\n  dom_b = np.delete(dom_b,del_store)\n  interval_store = np.delete(interval_store,del_store,axis=0)\n\n  zl = -5\n  zu = 5\n\n  interval_store[interval_store &gt; zu] = zu\n  interval_store[interval_store &lt; zl] = zl\n\n  return dom_a,dom_b,interval_store\n\n\n\n\n\n\n\n\n\n\n\nThe expected value over \\(z\\) (integrating over potential experimental outputs) of this upper envelope (which we use inplace of the inner \\(\\max\\) operator), where \\(z \\sim \\mathcal{N}(0,1)\\) is given by:\n\\[\n\\sum_i \\int^{c_{i+1}}_{c_{i}}(a_iz+b_i)\\phi(z) \\; \\text{d}z \\rightarrow \\sum_i b_i [\\Phi (c_{i+1})-\\Phi(c_i)] + a_i[\\phi(c_i) - \\phi(c_{i+1})]\\]\nwhere \\(\\Phi\\) is the standard cumulative normal probability function, \\(\\phi\\) is the standard normal probability function, \\(c\\) are the \\(z\\) values of the upper envelope intervals, and \\(a\\) and \\(b\\) are the gradient and intercept of upper envelope intervals corresponding to Equation 3.\nIntuitively, we are solving the expected improvement integral analytically for each line segement within the upper envelope of lines (which we use as it is equivalent to the inner \\(\\max\\) operator).\n\nFor the sake of completeness we will wrap this up into an acquisition function, and run a basic Bayesian optimisation loop\n\ndef noisy_EI_lines(x,GP):\n  with torch.no_grad(), gpytorch.settings.fast_pred_var():\n    model = GP['model']\n    likelihood = GP['likelihood']\n    model.eval()\n    likelihood.eval()\n    x_prime_vals = torch.linspace(0,3*np.pi,200)\n    predicted_output = likelihood(model(x))\n    mean = predicted_output.mean.item()\n    var = predicted_output.variance.item()\n    a = []\n    b = likelihood(model(x_prime_vals)).mean.numpy()\n    for x_prime in x_prime_vals.reshape(-1,1):\n      conc_x = torch.cat((x,x_prime))\n      covar = likelihood(model(conc_x)).covariance_matrix\n      a.append((covar[0,1]/var).item())\n    dom_a,dom_b,interval_store = upper_env(np.array(a),np.array(b))\n    N = torch.distributions.Normal(0,1)\n    sum = 0\n    for i in range(len(interval_store)):\n      c_i = torch.tensor(interval_store[i,0])\n      c_i1 = torch.tensor(interval_store[i,1])\n      sum += dom_b[i]*(N.cdf(c_i1) - N.cdf(c_i)) + dom_a[i]*(np.exp(N.log_prob(c_i)) - np.exp(N.log_prob(c_i1)))\n  return sum.item()\n\n\n\nNAIVE TIME TAKEN:  26.0427 s. AVERAGE TIME PER EVALUATION:  0.2604 s\nUPPER ENVELOPE TIME TAKEN:  4.6367 s. AVERAGE TIME PER EVALUATION:  0.0464 s\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nGarnett, Roman. 2023. “Bayesian Optimization.” Cambridge University Press. https://bayesoptbook.com.\n\n\nJones, Donald R., Matthias Schonlau, and William J. Welch. 1998. Journal of Global Optimization 13 (4): 455–92. https://doi.org/10.1023/a:1008306431147."
  },
  {
    "objectID": "posts/metaphysical/index.html",
    "href": "posts/metaphysical/index.html",
    "title": "Metaphysical Chemical Processes",
    "section": "",
    "text": "The Chinese room thought experiment provides an argument for why computers will never be able to truly ‘think’.1\n1 Josef Moural, “The Chinese Room Argument,” in John Searle, ed. Barry Smith (Cambridge: Cambridge University Press, 2003), 214-260.2 Margaret A. Boden, The Philosophy of Artificial Intelligence (Oxford: Oxford University Press, Inc., 1990).3 I also like the implication that if the Chinese room is correct, then either the human brain with it’s electrical and chemical signals is somewhat ‘special’ and has some additional latent features that invoke consciousness/intuition/thought, or consciousness/intuition is actually just a bit of an illusion and there’s nothing to distinguish us from a bloke idly following instructions without understanding what they mean. Both equally fun.In effect it states that a machine can be seen to be providing intelligent dialogue or conversation in Chinese, but inside ‘the room’ there is simply someone with a rule book that is large enough to enumerate all the potential answers, and the rules to generate and combine them (in effect, the structure of Chinese). This person doesn’t truly know or think about what they are doing. Whether or not this is a realistic analogy is not something I am going to attempt to answer. To me, the Chinese room, with its counter and counter-counter-arguments2 , is relatively convincing3 .\nBut for the purposes of this post, I’m not convinced, I think it’s completely and utterly wrong and a bit daft.4\n4 Mind games.Here’s a quote that I enjoy from the Stanford Encyclopaedia of Philosophy with an interesting extension to the thought experiment.\n\n[…] He [Searle] suggests a variation on the brain simulator scenario: suppose that in the room the man has a huge set of valves and water pipes, in the same arrangement as the neurons in a native Chinese speaker’s brain. The program now tells the man which valves to open in response to input. Searle claims that it is obvious that there would be no understanding of Chinese. (Note however that the basis for this claim is no longer simply that Searle himself wouldn’t understand Chinese – it seems clear that now he is just facilitating the causal operation of the system and so we rely on our Leibnizian intuition that water-works don’t understand (see also Maudlin 19895 ).) Searle concludes that a simulation of brain activity is not the real thing.\n5 Tim Maudlin, “Computation and Consciousness,” The Journal of Philosophy 86, no. 8 (1989): 407-432.\nIt’s a good point, water-works can’t think, can they? If a brain can think, and a brain can be modelled like a series of pipes, then why can’t a series of pipes think? If I were to replace a single pipe with an actual real neuron, the signal of which controlled a valve to the next pipe, then would my water-works6 be able to think? If I just start replacing all the pipes in my works with equivalent neurons, at what point does consciousness kick in?\n6 spoiler alert, or… chemical processes.7 Eric Schwitzgebel, “If Materialism is True, the United States is Probably Conscious,” Philosophical Studies 172, no. 7 (October 2014): 1697-1721.8 This is almost certainly true when you look at Joe Biden; the US still works even if the man in charge is barely with us.There’s a nice paper that touches on that last point justifying the idea that large organisations, or countries such as the United States can (or rather are) conscious7 .8\nWith that in mind, I’ll get to the point. Can we design a chemical process to think?9 The fact I’ll be dealing with simulated chemical processes adds a layer of complication because we’re back to asking whether computers can think. But lets assume I’m going to end up building it.10\n9 Normal people will and have used neural networks to attempt this task, choosing to (originally) directly model the interactions between neurons. I’m going to choose to model the water-works, because, funny.10 I might spin this off into a deep-tech unicorn.\nThe Plan\nHow am I going to go about this task? I could cheat and try to model a neural network using a chemical process. After all, neural networks have been proven to be universal function approximators11 . I think this is a bit of a boring and sort of cheating. Additionally, attempting to directly find a chemical process that can reason is more interesting than finding a neural network that can reason, because I could feasibly go and build the chemical process in the real world12 . The system it would live in is the same as us (i.e. not within the confines of a computer). However, in the ‘lets try to get a neural network to think’-space inherently it still just exists on a computer and you’re back to square one13 . You’d have to make the neural network into some physical representation (like a brain)14 . There are a host of assumptions here, namely to do with physicality, scale and permanence, but we really must move on.\n11 Apparently to the detriment of machine learning as a field, something about making people focus on the wrong aspects of research.12 I could also build the chemical process that approximates a neural network but as I say, boring.13 Or at least a less interesting metaphysical representation.14 This is somewhat similar to the ARIA projectNature Computes BetterQuite obviously, the first reasonable step to building a chemical process that can think, is to build a chemical process simulator.\nExisting process simulators are bloated, use outdated GUIs and are not built with machine learning / data-driven workflows in mind. So I built my own. It’s currently written in Python but it might not be one day15 . I’ve called it &lt;process&gt;, which you have to write as a code block.\n15 LLM-based code-translation will solve this for me.16 I can’t stress how much better and more useful this is over SFILES workflows which are outdated and stuck in an era where NLP worked on sentences and LLMs that could parse millions of tokens at a time didn’t exist. How are you seriously meant to learn a representation of something as intricate as a chemical process with an abstraction like (raw)(hex)(r)[&lt;(pp)&lt;(raw)](mix)&lt;.... Unless you train an LLM from scratch on SFILES (spoiler: you can’t because there is no where near enough data) they’re completely useless to use as a representation, because SFILEs are 1. not in LLM datasets (cannot truly use with pretrained LLMs) and 2. pretrained LLMs will struggle to one-shot learn what SFILES is because they’re not easily understandable, parseable or readable. LLMs are a learned representation of language, if you’re going to try and use them for chemical processes, or want a useable process representation then express them using language. SFILES rant over.17 Insert OntoCAPE rant.18 Apart from the silly long previous rant.19 I only use TOML over YAML because it’s called Tom’s Own Markup Language and that is my name. These are the arbitrary decisions I have and will continue to make throughout this post.In &lt;process&gt;, processes are fully represented within a TOML file. The files are detailed enough to provide a complete representation of a process16 . The representation is also not too detailed where there would be extraneous effort to define it, reducing bloat17 . If you’re far enough into process design that you want to model a process in significant detail then ‘rigorous’ tools such as Aspen or gProms are what you want anyway. &lt;process&gt; is not for rigorous design, it sits somewhere between SFILES18 , and Aspen in terms of complexity. It’s main advantage over SFILES is that chemical processes can be simulated directly from an interpretable text-based .toml file19 .\n&lt;process&gt; is effectively a compiler for chemical processes, compiling a readable text-based format into an object that can be simulated and analysed.\nHere’s an example of the basic components of a process.toml file.\n[[species]]\nname = \"water\"\nmolecular_weight = 18.01528 # kg/kmol\ncp_poly_coeffs = [276370.0, -2090.1, 8.125, -0.014116, 9.3701e-6]\nenthalpy_of_formation = -285830.0 # kJ/kmol\n\n...\n\n[[species]]\nname = \"nitro_benzene\"\nmolecular_weight = 123.109\ncp_poly_coeffs = [295300.0,-80.7,1.705]\nenthalpy_of_formation = 12500.0\n\n[[streams]]\nname = \"benzene_inlet\"\nmolar_flowrate = 0.5 # kmol\nmolar_ratios = {benzene = 1}\ntemperature = 298 # K \npressure = 101325 # Pa\n\n...\n\n[[streams]]\nname = \"benzene_recycle\"\nmolar_flowrate = 0\nmolar_ratios = {benzene = 1}\ntemperature = 298\npressure = 101325\n\n[[separator]]\nname = \"separator_one\"\ninlet = [\"final_product\"]\noutlet = [\"tops\", \"waste_water\"]\nspec = [{nitro_benzene= 0.95},{water= 0.95}]\n\n...\n\n[[mixer]]\nname = \"reactant_mixer\"\ninlet = [\"benzene_inlet\", \"nitric_acid_inlet\",\"recycle\",\"benzene_recycle\"]\noutlet = [\"reactor_reactants\"]\n\n[[reactor]]\nname = \"reactor_one\"\ninlet = [\"reactor_reactants\"]\noutlet = [\"reactor_products\"]\nreactant_stoichiometry = {benzene=1, nitric_acid=1}\nproduct_stoichiometry = {water = 1, nitro_benzene =1}\nconversion = {benzene=0.9}\n\n[[splitter]]\nname = \"splitter_one\"\ninlet = [\"reactor_products_cooled\"]\noutlet = [\"final_product\", \"recycle\"]\nsplit_ratio = [0.8, 0.2]\n\n[[heat_exchanger]]\nname = \"product_cooler\"\ninlet = [\"reactor_products\"]\noutlet = [\"reactor_products_cooled\"]\noutlet_temperature = 298\nBy design, it’s relatively self-explanatory. You define molecular species, all the streams, their connections, etc. Unit operations each have specific attributes which must be defined such as the split ratio of a splitter.\nYou don’t have to specify everything20 , &lt;process&gt; will attempt to simulate unit operations with specified inlet streams until the entire process converges.\n20 If you could then this would be a pointless tool.Degrees-of-freedom are currently quite uncompromising. There is no way to fix a stream, and every variable can be changed. There is no need for a specific ‘recycle’ block or stream due to the way a process is converged. At the expense of bloat, you have to pay slightly more attention to what is being defined, which is not the worst trade off.\nAnyway, I might write this up a bit longer at some point depending if I flesh it out a bit. For now, onwards… I can visualise a process as follows:\nprocess_file = \"nitrobenzene_process.toml\"\nprocess = parse_process(process_file)\nprocess.plot(\"outputs/process.png\")\n\n\n\n\nA visualised process.\n\n\n\nAnd simulate it as so:\nprocess.simulate(tol=1e-7)\nprocess.plot_convergence(\"outputs/convergence.png\")\n\n\nStream Tolerance -&gt; 2.20e+00 -&gt; 5.29e-01 -&gt; 1.12e-01 -&gt;...-&gt; 9.47e-07 -&gt; 3.83e-07 -&gt; 1.54e-07\nConverged in 30 iterations.\n\n\n\nThe overall mass and energy balance can be validated21 , and I can also print off the steady-state of the converged process.\n21 This is done by analysing streams to see if they come from, or go to a unit operation. If not, they are designated process inlet or outlets and quantities summed.print(process, '\\n')\nprint(process.confirm_energy_balance(), '\\n')\nprint(process.confirm_mass_balance())\n\n\nProcess(name='nitrobenzene_process', unit_operations={'reactant_mixer': Mixer(name='reactant_mixer', inlet=[(0.50 kmol/hr, 298.00 K, 101325.00 Pa) {'benzene': '1.00'}, (0.50 kmol/hr, 312.00 K, 101325.00 Pa) {'nitric_acid': '1.00'}, (0.26 kmol/hr, 298.00 K, 101325.00 Pa) {'benzene': '0.04', 'nitric_acid': '0.03', 'water': '0.47', 'nitro_benzene': '0.46'}, (0.04 kmol/hr, 298.00 K, 101325.00 Pa) {'nitric_acid': '0.17', 'water': '0.28', 'nitro_benzene': '0.10', 'benzene': '0.45'}], outlet=[(1.31 kmol/hr, 300.92 K, 101325.00 Pa) {'benzene': '0.41', 'nitric_acid': '0.39', 'water': '0.10', 'nitro_benzene': '0.10'}], process=...), 'splitter_one': Splitter(name='splitter_one', inlet=[(1.31 kmol/hr, 298.00 K, 101325.00 Pa) {'benzene': '0.04', 'nitric_acid': '0.03', 'water': '0.47', 'nitro_benzene': '0.46'}], outlet=[(1.04 kmol/hr, 298.00 K, 101325.00 Pa) {'benzene': '0.04', 'nitric_acid': '0.03', 'water': '0.47', 'nitro_benzene': '0.46'}, (0.26 kmol/hr, 298.00 K, 101325.00 Pa) {'benzene': '0.04', 'nitric_acid': '0.03', 'water': '0.47', 'nitro_benzene': '0.46'}], process=..., split_ratio=[0.8, 0.2]), 'reactor_one': Reactor(name='reactor_one', inlet=[(1.31 kmol/hr, 300.92 K, 101325.00 Pa) {'benzene': '0.41', 'nitric_acid': '0.39', 'water': '0.10', 'nitro_benzene': '0.10'}], outlet=[(1.31 kmol/hr, 367.04 K, 101325.00 Pa) {'benzene': '0.04', 'nitric_acid': '0.03', 'water': '0.47', 'nitro_benzene': '0.46'}], process=..., reactant_stoichiometry={'benzene': 1, 'nitric_acid': 1}, product_stoichiometry={'water': 1, 'nitro_benzene': 1}, conversion={'benzene': 0.9}, heat_of_reaction=153180.0), 'product_cooler': HeatExchanger(name='product_cooler', inlet=[(1.31 kmol/hr, 367.04 K, 101325.00 Pa) {'benzene': '0.04', 'nitric_acid': '0.03', 'water': '0.47', 'nitro_benzene': '0.46'}], outlet=[(1.31 kmol/hr, 298.00 K, 101325.00 Pa) {'benzene': '0.04', 'nitric_acid': '0.03', 'water': '0.47', 'nitro_benzene': '0.46'}], process=..., outlet_temperature=298, delta_temperature=None), 'separator_one': Separator(name='separator_one', inlet=[(1.04 kmol/hr, 298.00 K, 101325.00 Pa) {'benzene': '0.04', 'nitric_acid': '0.03', 'water': '0.47', 'nitro_benzene': '0.46'}], outlet=[(0.52 kmol/hr, 298.00 K, 101325.00 Pa) {'benzene': '0.04', 'nitric_acid': '0.03', 'nitro_benzene': '0.88', 'water': '0.05'}, (0.53 kmol/hr, 298.00 K, 101325.00 Pa) {'benzene': '0.04', 'nitric_acid': '0.03', 'nitro_benzene': '0.05', 'water': '0.89'}], process=..., spec=[{'nitro_benzene': 0.95}, {'water': 0.95}]), 'separator_two': Separator(name='separator_two', inlet=[(0.52 kmol/hr, 298.00 K, 101325.00 Pa) {'benzene': '0.04', 'nitric_acid': '0.03', 'nitro_benzene': '0.88', 'water': '0.05'}], outlet=[(0.47 kmol/hr, 298.00 K, 101325.00 Pa) {'nitric_acid': '0.02', 'water': '0.03', 'nitro_benzene': '0.96', 'benzene': '0.00'}, (0.04 kmol/hr, 298.00 K, 101325.00 Pa) {'nitric_acid': '0.17', 'water': '0.28', 'nitro_benzene': '0.10', 'benzene': '0.45'}], process=..., spec=[{'nitro_benzene': 0.99}, {'benzene': 0.95}])}, streams={'benzene_inlet': (0.50 kmol/hr, 298.00 K, 101325.00 Pa) {'benzene': '1.00'}, 'nitric_acid_inlet': (0.50 kmol/hr, 312.00 K, 101325.00 Pa) {'nitric_acid': '1.00'}, 'reactor_products': (1.31 kmol/hr, 367.04 K, 101325.00 Pa) {'benzene': '0.04', 'nitric_acid': '0.03', 'water': '0.47', 'nitro_benzene': '0.46'}, 'reactor_products_cooled': (1.31 kmol/hr, 298.00 K, 101325.00 Pa) {'benzene': '0.04', 'nitric_acid': '0.03', 'water': '0.47', 'nitro_benzene': '0.46'}, 'reactor_reactants': (1.31 kmol/hr, 300.92 K, 101325.00 Pa) {'benzene': '0.41', 'nitric_acid': '0.39', 'water': '0.10', 'nitro_benzene': '0.10'}, 'final_product': (1.04 kmol/hr, 298.00 K, 101325.00 Pa) {'benzene': '0.04', 'nitric_acid': '0.03', 'water': '0.47', 'nitro_benzene': '0.46'}, 'tops': (0.52 kmol/hr, 298.00 K, 101325.00 Pa) {'benzene': '0.04', 'nitric_acid': '0.03', 'nitro_benzene': '0.88', 'water': '0.05'}, 'waste_water': (0.53 kmol/hr, 298.00 K, 101325.00 Pa) {'benzene': '0.04', 'nitric_acid': '0.03', 'nitro_benzene': '0.05', 'water': '0.89'}, 'pure_product': (0.47 kmol/hr, 298.00 K, 101325.00 Pa) {'nitric_acid': '0.02', 'water': '0.03', 'nitro_benzene': '0.96', 'benzene': '0.00'}, 'benzene_recycle': (0.04 kmol/hr, 298.00 K, 101325.00 Pa) {'nitric_acid': '0.17', 'water': '0.28', 'nitro_benzene': '0.10', 'benzene': '0.45'}, 'recycle': (0.26 kmol/hr, 298.00 K, 101325.00 Pa) {'benzene': '0.04', 'nitric_acid': '0.03', 'water': '0.47', 'nitro_benzene': '0.46'}}, species={'water': water (18.01528 kg/kmol), 'benzene': benzene (78.11 kg/kmol), 'nitric_acid': nitric_acid (63.013 kg/kmol), 'nitro_benzene': nitro_benzene (123.109 kg/kmol)}, print=True, simulated_at=datetime.datetime(2024, 8, 18, 15, 26, 8, 154480, tzinfo=datetime.timezone.utc)) \n\nEnthalpy in: 37412.944 kJ,  Reaction Enthalpy: 73175.159 kJ, Enthalpy Added: -38052.065 kJ, Enthalpy out: 72536.082 kJ\nDifference: -0.043281 kJ (-0.000116%)\nFalse \n\nMass in: 70.561 kg, Mass out: 70.562 kg\nDifference: -0.000613 kg (-0.000869%)\nTrue\nI can simulate heat exchangers, splitting streams, mixing streams, basic separations, non-adiabatic reactions with stoichiometric ratios, components with specific heat capacities as a function of temperature… this should be enough for now.\n\n\nWhat was that about? Anyway…\nThe goal here is to build an input-output machine, that on the surface acts like the ‘room’ in the Chinese room, and more precisely, the waterworks within the room as proposed by Searle in his brain simulator scenario. To make this easier, when I say ‘think’, I mean I’m going to approximate a function, because as I previously mention, I don’t believe in the Chinese room argument.\nI will go about doing this through direct simulation, where the chemical process is directly used to model a function via something analogous to symbolic regression or evolutionary programming^[Alternative methods would be to either:\n1. Build up the basic building blocks of computation, i.e. AND, OR, gates etc… Then by extension demonstrate how a computer can be constructed, and therefore a universal function approximator (2 layers of abstraction below thinking). Or…\n2. Build up the basic building blocks of a neural network, which, as a universal function approximator, completes the ability to approximate functions (1 layer of abstraction below thinking).\nBut as previously mentioned, I want the chemical process to be as close as possible to the function approximation task at hand.] .\nTherefore, the mathematical problem I will solve will be a least-squares (machine learning) task over chemical processes:\n\\[\\min_{\\mathcal{P}} \\sum_{i=1}^N ||\\mathcal{P}(x_i) - y_i||^2_2\\]\nwhere a chemical process 𝒫 is defined by a set of unit operations 𝒪, connecting streams 𝒮, yappa yappa blah blah blah… Basically, you choose what the input x represents in your process, and what the output y represents. Simulate the chemical process for a given x and evaluate the output. Do this for all values of x in the dataset and you can evaluate how well the process approximates the function.\nThat being said, I am going to solve this optimisation problem using evolutionary programming, a benefit of using &lt;process&gt;. Firstly, I’ll demonstrate a way of evaluating the objective function.\nTo begin with I will define a very basic process with a single component as an inlet stream, a single component as an outlet stream, and a mixer that doesn’t do anything.\n[[species]]\nname = \"water\"\nmolecular_weight = 18.01528 # kg/kmol\ncp_poly_coeffs = [276370.0, -2090.1, 8.125, -0.014116, 9.3701e-6]\nenthalpy_of_formation = -285830.0 # kJ/kmol\n\n[[streams]]\nname = \"inlet\"\nmolar_flowrate = 0.5 # kmol\nmolar_ratios = {water = 1}\ntemperature = 298 # K \npressure = 101325 # Pa\n\n[[streams]]\nname = \"outlet\"\n\n[[mixer]]\nname = \"mixer\"\ninlet = [\"inlet\"]\noutlet = [\"outlet\"]\n\n\n\nFor now, assuming temperature of both the inlet stream and outlet stream represent my functional input and output respectively, I can simulate the process and evaluate the objective function as follows:\ny_test = []\nfor i in range(n):\n    process.streams['inlet'].temperature = x[i]\n    process.simulate(tol=1e-7)\n    y_test.append(process.streams['outlet'].temperature)\n\n\n\nCan you see where we’re going yet? I can take advantage of multiprocessing to evaluate the objective function in parallel. This is a strength of &lt;process&gt;, and is a good demonstration of designing something like a process simulator with machine learning use cases in mind.\nI’ll assume that physical units can be scaled later on. Now I can hypothesise that there is a heat exchanger between the streams, with a given temperature decrease to get a different ‘loss’ between our data and the process output.\n[[heat_exchanger]]\nname = \"product_cooler\"\ninlet = [\"inlet\"]\noutlet = [\"outlet\"]\ndelta_temperature = -3 # K\n\n\n\nI’ll stop here, we get the idea. We can change the type of unit operation, their internal parameters, and their order to try and approximate a function. Now we have the ability to:\n\nEvaluate how well our process can approximate a function;\nMutate the process22 .\n\n22 Once again, by building a lightweight process simulator we have gained the ability to easily modify the process structure enabling us to solve an optimisation problem over different structures, something that takes PhD lengths of time to do in Aspen or gProms.23 Presented in an unnecessarily complex amount of detail, remember?We can build a basic evolutionary programming framework to solve the optimisation problem above23 , acting like a sort of growing neural network, with feedback.\nBut that will have to wait…. for next time!"
  },
  {
    "objectID": "posts/haiku/index.html",
    "href": "posts/haiku/index.html",
    "title": "That! The Haiku and the Sonnet",
    "section": "",
    "text": "Cheap reasoning models (e.g. Deepseek r1 [mentioned for engagement purposes]), with chain-of-thought output make it extremely tempting to personify LLMs or their outputs. Similarly, certain models such as Claude (in all its forms) have gained a cult-of-personality type following, where users are more likely to use it for its feel (gained through preferential fine-tuning, a muted colour scheme, a hand drawn logo, or all of the above). Unknown to them, an increasingly large number of users are assigning meaning to the result of a computation. When I refer to meaning in this article I don’t refer to it in the sense of whether something is poetic or accurate or profound, but in a way rather more related to the word meaning ful. Related in some sense to something deeper, that we empathise with.\nAGI is an unquantifiable threshold, and whether an LLM reaches this will be decided based on a collective reading of the outputs that are produced. That is why the model cannot be separated from the surrounding infrastructure. The colour scheme, the point of interaction, the UI, even the very act of ‘chatting’ all contribute in some sense to our own reading, and whether we choose to recognise an other.\nFor the purposes of this post, which I am writing in Tokyo Station, I will un-pick the Western endeavour to seek a deeper meaning and state that the outputs from an LLM are meaningless. I mean this in the sense that inferred tokens originates from a soul-less computer. We may impose a meaning on them, but in their most abstracted form, they are the result of computations.\nDrawing a literary comparison, the skill in reading a haiku is to not read any meaning into it, but to let it wash over you. Should the same be said for an LLM (that truly does have no meaning in the first place)? Should we let output tokens wash over us like a haiku in accordance to Zen? We turn to Roland Barthes’ reading of haiku in Empire of Signs :\n\nHere meaning is only a flash, a slash of light: When the light of sense goes out, but with a flash that has revealed the invisible world , Shakespeare wrote; but the haiku’s flash illumines, reveals nothing; it is the flash of a photograph one takes very carefully (in the Japanese manner) but having neglected to load the camera with film. Or again: haiku reproduces the designating gesture of the child pointing at whatever it is (the haiku shows no partiality for the subject), merely saying: that! with a movement so immediate (so stripped of any mediation: that of knowledge, of nomination, or even of possession) that what is designated is the very inanity of any classification of the object: nothing special , says the haiku, in accordance with the spirit of Zen …\n\nThis passage shares so much with how we really know LLMs to work (which is different from how we wish LLMs worked, which is to think) , tokens appearing in an instant from the void. But at the same time highlights just how disparate our readings of LLMs are when compared to something like a haiku (again, to be expected because we assign a value towards an other , but it is important to consider that this importance is ‘created’ in order to process the meaninglessness of an LLM).\nThis is further amplified by how we interact with LLMs. When people refer to models such as Anthropic’s Claude, they refer not just to the set of weights, or the data centre, but to the personality contained (again, we pretend that this is the case) within the output, which is defined by the overall experience. The UI, the colour scheme, the typeface, even the act of ‘chatting’ is to impose a dialogue with an other. These aspects combined make it irresistible to place meaning where there is none. Either AGI will refer to a holistic experience, or we must treat how we interact with LLMs fundamentally differently.\nHow do we get around this imposition of our own values onto something meaningless, in effect, to un-interpret the haiku and let it wash over us. Referring back to Barthes, we learn of different approaches, hundreds of years old:\n\n[…] the haiku has the purity, the sphericality, and the very emptiness of a note of music; perhaps that is why it is spoken twice, in echo; to speak this exquisite language only once would be to attach a meaning to surprise, to effect, to the suddenness of perfection; to speak it many times would postulate that meaning is to be discovered in it, would simulate profundity; between the two, neither singular nor profound, the echo merely draws a line under the nullity of meaning.\n\nPerforming LLM inference multiple times (different temperature values?) may help us to de-personify outputs, enabling us to appreciate the content (but not assign meaning to it). However, very quickly we enter into the space of what I will call the Muji paradox (highlighted to me by Sunil).\nMuji: in English ‘no-brand’ is the antithesis of a brand. It embodies the same values found within a haiku though in a commercial sense, not a literary one. The complete lack of meaning. It is important to note though that from a Western perspective, it has failed. Paradoxically, we have associated an aesthetic with it, a quality, and a set of values, in exactly the same way we have done for LLMs.\n\n\n\nAn alternative to doing nothing at all, would be to do everything at once. As an example of this, Japanese television exists, and often consists of a dizzying array of fonts and colours. It becomes so overwhelming that you are forced to assign no meaning to the text at all, it simply exists. Read nothing into how it is presented, because it will change. Japanese television in a sense embodies the idea of Zen equally as well as Muji does.\n\n\n\nI propose that to provoke an alternate reading of LLMs, in opposition to the Western AGI-seeking viewpoint, that inevitably converges towards chatting with an other, we treat creative outputs more like haikus. As Barthes’ says, like the flash of a photograph one takes very carefully but having neglected to load the camera with film. This may mean taking the Muji route and attempting to strip any aesthetic, or imposition of values from an LLM (obviously it is not in the interest of an AI company to do this). OpenAI probably has the ‘blandest’ aesthetic, but like Muji, even this has become meaning- full. An alternative would be, like reading a haiku, to present tokens multiple times, similar to Raymond Queneau’s ‘A Hundred Thousand Billion Poems’. Interacting with an LLM may feel closer to watching a Japanese TV show. So overwhelming in alternative tokens, typeface, colour, conflicting aesthetics, even mode of interaction ( RE: chatting ), that we are forced to strip any emotion we might infer.\nMaybe that’s why we’re struggling to process AI and art, and why the concept of AGI is so messy and intangible. Because we’re imposing a meaning on something that clearly has none (or even worse, imposing a meaning on something that we’ve already imposed a meaning to in the case of Claude). If we embrace this more Buddhist perspective, it may unlock an alternate reading, complimentary to our own AGI-centric one, mirroring the individual dividing lines of the West/East, the Haiku and the Sonnet."
  },
  {
    "objectID": "posts/values/index.html",
    "href": "posts/values/index.html",
    "title": "Generative Models as Mirrors and Idols; From Istanbul",
    "section": "",
    "text": "Relevant articles published after this post\n\nChina deploys censors to create socialist AI\n\nLarge language models are being tested by officials to ensure their systems ‘embody core socialist values’\n\nThe Chinese team behind DeepSeek V2 write about lagging on the Humanity-Moral subset of MMLU, due to its “American values”\n\nFor example, when evaluated on MMLU, although DeepSeek-V2 achieves comparable or superior performance on the majority of testsets compared with its competitors like Mixtral 8x22B, it still lags behind on the Humanity-Moral subset, which is mainly associated with American values.\n\n\n\n1600 years ago, as Emperor of Rome and patron of Constantinople where he is buried, Theodosius II oversaw the central theological questions in the wake of the Council of Nicea and doctrine of Trinity: did Christ have distinct human and divine aspects? Was the human form of Christ the same form as the rest of us? Alongside a university to aid in answering these questions, and the city walls, Theodosius commissioned the construction of a vast cistern to provide fresh water for the city’s inhabitants and palaces1.\n1 Pretentious opening but please indulge me because later on it goes downhill and I end up using the phrases ‘world-view’ and ‘cut-through’.2 Not all of it obviously.3 Please pretend I’m still there for the entirety of the following, otherwise it doesn’t work.It is in a glass-bound spotlit coffee shop above this cistern that I write this post2, complete with visitors centre and free WiFi3. Below me, 32 nine-meter high marble columns support myself and others, our coffees, my bottle of water. We are mercilessly paying homage to Theodosious’ original intentions to provide refreshment here - what once served a purpose continues to do so under a different context. Theodosius’ grave is now lost, most likely beneath the Fatih Mosque where Islamic minorets now anchor the heavens to the city and vice versa. The Christian pasts of the Romans and Byzantines have coalesced with the more recent history of the Ottoman empire, places like this a constant reminder of the slow march of time which seems to have begun earlier and progressed faster here than in any other city.\n\n\n\n\n\nTheodosius Cistern, Taken Februrary 2024\n\n\nIstanbul as it is now, is a city with a complex relationship between Christianity, Islam, and the secular. The Hagia Sofia, a Christian church for 1000 years, an Islamic mosque for 500, and most recently a Museum for 85, is now a Mosque again. As Turkey undergoes broad desecularisation under Erdogan, it seems only appropriate that in a city that has seen so much religious change, I shamelessly engage in mapping the state and future of machine learning onto past and present religious divides (as I have previously done).\n\n\nThis post never quite recovers the highs of the introduction, so please feel free to stop here. Click here to return to homepage.\nWith the large amount of resources required to train state-of-the-art models, teams of researchers have courted the tech establishment in order to gain access to large amounts of computational power. The stakes are supposedly high; a well-trained model in language or image generation may serve as a powerful tool to flood social media with undetectable posts aligned with a given set of values, infiltrate computer networks, or aid in obtaining technological advancements or scientific discovery. It is therefore no surprise that alongside those with business interests at heart, there are nation-states wishing to develop state-of-the-art machine learning models.\nWith so many factors to take into account such as architecture, dataset, and fine-tuning method, the system boundaries when creating machine learning models are necessarily drawn wider than the model itself, encompassing the team, organisation, financial backers, and their values and interests.\n\n\n\nGoogle Gemini’s response, as of Feb 25 2024. Source\n\n\nPeople have recently blamed Google’s corporate culture for a series of questionable large-language model and image generation failures, generating enormous amounts of cut-through. One could argue that it was ultimately the Western Christian values of equality and respect imposed by those that designed the system that resulted in an overly biased model. In this case I would argue that Gemini was projecting a radically Christian world-view, demonstrating respect for those that unquestionably do not deserve it, and nuance in judgements that only the most stoic entity would consider remotely balanced.4 In this case, the model and its outputs are fundamentally tied to those that create it.\n4 The individual failure of the model can be put down to bad fine tuning, but the content emerges pre-fine-tuning and what is intended to emerge post-fine-tuning must be taken into account.As traditionally Islamic nation states with large amounts of resource such as the UAE and Saudi Arabia seek to develop their own machine learning models, it is not a stretch to assume that machine learning models and Islamic values will inevitably cross paths as state actors seek to gain from the benefits of AI. With this in mind, how do Islamic values map onto the existential threat of AI? What would a language model imbued with Islamic values provide?\n\n\nSlight diversion follows\nWestern Christian values allow for a level of flexibility when confronted with the prospect of super-human machine intelligence. The act of bringing ‘something’ into existence is broadly acceptable within this framework (and sometimes encouraged). The questions that preoccupy most people with this worldview are ones of artificial-general-intelligence rights. Can a machine be concious? Is it morally acceptable to turn off a concious machine? How do we treat an AGI?\nHowever, from an Islamic perspective this initial act of creation strikes a more existential tone when the complimentary values of Tahwid5 and Shirk6 are considered. There are two key conflicting arguments concerning the pursuit of human-surpassing intellect.\n5 Tawhid is the indivisible unification concept of monotheism in Islam. Source6 To attribute divinity to anything or anyone else, is shirk – an unpardonable sin according to the Qur’an.7 How should existing religions face the prospect of a model trained on other religious texts?The first perspective is that AGI may help to uncover the secrets of the universe, or provide new ways of interpreting the Qu’ran. Through the use of artificial intelligence, one has become closer to God. This perspective is one that Arabic scholars under the Abbasid Caliphate took when they provided the groundwork for modern mathematics and the number system we use today. In contrast to this viewpoint, an overreliance on super-human machine intelligence (should this be achieved, this seems reasonable) may be interpreted as an idolatrous act of Shirk. Though future rulings may reasonably put this balance down to the data itself that a model is trained on7. There is a delicate balance between science, art and discovery, and religion that continues to be repeated throughout the Abrahamic religions and will be repeated as machine learning gains human-surpassing abilities.\n\n\nDiversion ends.\nAny machine learning model produced by a nation-state with a given ideology may be deliberately or unconciously imbued with these values, just as models produced in the West enforce our set of values. As the uncompromising mirror of generative AI allows us to confront our worldview, we must not be surprised when opposing worldviews instanciate in generative models around the world.\nWhere I am in Istanbul8 is a physical reminder of the neverending race to maintain a set of values within a community, or enforce a set of values on those without them. As a city it has ultimately benefited, layers of history have resulted in a broadly interesting place to visit.9\n8 Stated as fact assuming previous instructions were followed.9 Five word summary for the billboard.10 As opposed to nation-states11 With all the authority of a final-year PhD student in a machine learning-adjacent field.Generative AI now provides a new battleground to enforce a value set in an a way altogether different from any that have come before. It is actually harder to not engage with this as Google has found out. From an organisational perspective, which dominates public-facing machine learning for now10, companies must ensure their values are well defined, clear and consistent11. If not they will emerge regardless, which might not be such a bad thing for the rest of us."
  },
  {
    "objectID": "posts/olympics/index.html",
    "href": "posts/olympics/index.html",
    "title": "A personal ranking of Olympic sports",
    "section": "",
    "text": "The Olympics are a celebration of human physical achievement. But I simply don’t believe some of the sports should be in it1. In this post, I will present a set of criteria that I believe defines what an Olympic sport should be. I will then use this model to rank the sports in the 2024 Olympics, and see if they pass muster2.\n1 Said with absolutely zero authority2 The conclusions presented should be taken with more than a pinch of salt. I am in no way qualified to make these judgements, and I wouldn’t even say these represent my actual opinions. In effect this post is completely useless, a waste of time, and I’m sorry for making it.\nExecutive Summary\n\nTriathlon and Weightlifting are the pinnacle of Olympic sports.\nFootball and Breakdancing should be removed from the Olympics with immediate effect.\nIf Trampolining was judged completely objectively, it would be comparatively one of the greatest Olympic sports of all time.\n\n\nAfter intense internal discussion with close friends and family3, I have settled on the following three criteria that define a real Olympic sport:\n3 largely at the Imperial students union\nIs winning the Olympics the pinnacle of the sport?\nIs the sport hard?\n\nDoes it look physically difficult, could anyone compete (at any level)?\n\nAre there judges?\n\nIs the sport subjective?\n\n\nTo generate these rankings, I unashamedly used a large-language model to give me a CSV based on these criteria4.\n4 Making the rest of the analysis in this post, in effect, invalid.\n\n\n\n\n\n\n\n\n\nThe bottom five sports are:\n\nGolf\nEquestrian\nSkateboarding\nFootball\nBreaking(/Dance)\n\nAs a quick validation, let’s try to justify the bottom five sports based on the criteria:\n\nGolf: Not the pinnacle of the sport (Masters, US Open, etc.), not hard (large amount of people play).\nEquestrian: Not hard (requires sitting on a horse), relatively subjective (judged on form).\nSkateboarding: Not the pinnacle of the sport (X-Games, generally judged on whether you make a cool video), not hard (anyone can skateboard in a few weeks).\nFootball: Not the pinnacle of the sport (World Cup), not hard (anyone can play football).\nBreaking: Not sure this is even a sport, it’s a dance.\n\nInterestingly, there are a solid 25ish sports that are solidly ‘Olympic’, with a plateau of sports that are all very close in score. Following this things get a bit more volatile5.\n5  These remaining sports would probably be on the Red Button, or BBC Three or something.The guiding assumption in my model here is that my criteria are equally weighted. To investigate the robustness of this assumption, I will use Monte Carlo sampling to investigate different criteria ratios, and evaluate the resulting rankings of Olympic-ness.\n\nMonte Carlo Simulation\nimport numpy as np\nimport joypy\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n# matrix of scores \nscores = np.array([df['Difficulty'],df['Judging Requirement'],df['Olympic Pinnacle']]).T\n\n# save ranks for each weighting\nsampled_ranks = []\nfor i in range(1000):\n    weights = np.random.uniform(1,3,3)\n    all_scores = np.dot(scores,weights)\n    ranks = np.argsort(-all_scores)\n    sampled_ranks.append(ranks)\n\n\n# Create a DataFrame for joypy\nsampled_ranks = np.array(sampled_ranks)\n\njoy_data = pd.DataFrame(sampled_ranks, columns=df['Sport'])\n# column name = Rank\njoy_data = joy_data.rename(columns={'index':'Rank'})\n\n# Create the ridgeline plot\njoypy.joyplot(\n    data=joy_data,\n    overlap=3,\n    colormap=cm.Blues_r,\n    labels=df['Sport'],\n    figsize=(7, 9),\n    xlabels=True,\n    title='Rank'\n);\n\n\n\n\n\n\n\n\nWe see a few interesting clusters of sports. I won’t discuss them here, but there is an interesting gap between Athletics and Beach Volleyball, and later between Table Tennis and Archery, but these sports are still all comfortably in the top half of the rankings.\nBreakdancing is squarely at the bottom. I think this is unfair because it simply hasn’t had enough time to develop as a sport. Therefore the criteria ‘is it the pinnacle of the sport’ is biased aganist it.\nTo try and explain some of these distributions, I’ll look into how different specific differences in the criteria weights affect the rankings in a leave-one-out manner.\nEach plot will show the rankings of the sports with one of the criteria removed. The sports that change the most in rank will be highlighted.\n\nLeave-one-out Criteria Analysis\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Assuming df is already defined and contains the necessary data\nscores = np.array([df['Difficulty'], df['Judging Requirement'], df['Olympic Pinnacle']]).T\nweights = np.array([1, 1, 1])\nall_scores = np.dot(scores, weights)\nranks = np.argsort(all_scores)\ndf = df.iloc[ranks].reset_index(drop=True)\nranks = np.arange(1, len(ranks) + 1)\n\ndef calculate_ranks(weights):\n    return len(scores) - np.argsort(np.dot(scores, weights))\n\nranks_no_diff = calculate_ranks([0, 1, 1])\nranks_no_judges = calculate_ranks([1, 0, 1])\nranks_no_pinnacle = calculate_ranks([1, 1, 0])\n\ndef find_extreme_differences(ranks1, ranks2):\n    diff = ranks1 - ranks2\n    return np.argmax(diff), np.argmin(diff)\n\np_no_diff, n_no_diff = find_extreme_differences(ranks, ranks_no_diff)\np_no_judges, n_no_judges = find_extreme_differences(ranks, ranks_no_judges)\np_no_pinnacle, n_no_pinnacle = find_extreme_differences(ranks, ranks_no_pinnacle)\n\nfig, axs = plt.subplots(3, 1, figsize=(7,6))\nx = np.arange(len(df))\n\nfor ax in axs:\n    ax.plot(x, ranks, color='k', label='Equal Weight')\n    ax.set_xticks(x)\n    if ax == axs[-1]:\n        ax.set_xticklabels(df['Sport'], rotation=90, ha='right')\n    else:\n        ax.set_xticklabels([])\n\naxs[0].plot(x, ranks_no_diff, color='tab:red', label='No Difficulty Criteria')\naxs[0].annotate(df['Sport'][p_no_diff], (x[p_no_diff], ranks_no_diff[p_no_diff]), \n                xytext=(0, -4), textcoords='offset points', ha='center', va='top', color='tab:red')\naxs[0].annotate(df['Sport'][n_no_diff], (x[n_no_diff], ranks_no_diff[n_no_diff]), \n                xytext=(0, 4), textcoords='offset points', ha='center', va='bottom', color='tab:red')\n\naxs[1].plot(x, ranks_no_judges, color='tab:blue', label='No Judging Criteria')\naxs[1].annotate(df['Sport'][p_no_judges], (x[p_no_judges], ranks_no_judges[p_no_judges]), \n                xytext=(0, -4), textcoords='offset points', ha='center', va='top', color='tab:blue')\naxs[1].annotate(df['Sport'][n_no_judges], (x[n_no_judges], ranks_no_judges[n_no_judges]), \n                xytext=(0, 4), textcoords='offset points', ha='center', va='bottom', color='tab:blue')\n\naxs[2].plot(x, ranks_no_pinnacle, color='tab:green', label='No Pinnacle Criteria')\naxs[2].annotate(df['Sport'][p_no_pinnacle], (x[p_no_pinnacle], ranks_no_pinnacle[p_no_pinnacle]), \n                xytext=(0, -4), textcoords='offset points', ha='center', va='top', color='tab:green')\naxs[2].annotate(df['Sport'][n_no_pinnacle], (x[n_no_pinnacle], ranks_no_pinnacle[n_no_pinnacle]), \n                xytext=(0, 4), textcoords='offset points', ha='center', va='bottom', color='tab:green')\n\nfor ax in axs:\n    ax.grid(alpha=0.3)\n    ax.legend(frameon=False)\n    ax.set_ylabel('Rank')\n    ax.set_xlim(-0.5, len(df) - 0.5)\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nCanoe sprint which my model considers squarely an Olympic sport plummets down to the bottom 10 when the ‘Subjective judging criteria’ is removed. Likewise, if trampolining was judged completely objectively6, it would rocket to being one of the all time great Olympic sports.\n\n\n6 My ignorance is showing here because I don’t know how trampolining is judged"
  },
  {
    "objectID": "posts/ratatouille/index.html",
    "href": "posts/ratatouille/index.html",
    "title": "On Ratatouille",
    "section": "",
    "text": "Remy the rat is a culinary genius because he watches a cooking show on TV, reads a best-selling recipe book, and sometimes eats two things at the same time. He consistently takes life advice by talking1 to a hallucinated dead chef, and generally ignores the majority of social constructs across both rat and human domains. No one did it quite like him.\n1 He speaks english, which humans hear in a higher pitch. As the only difference in Ratatouille between humans and rats is size and frequency (hz) of communication, I will consider them one-and-the-same for the remainder of this article.Ratatouille is a film about legacy, the importance of institutions, and the need for occasional reinvention. Given that rats live for approximately 2-3 years, it is therefore unfortunate that Remy will not have lived long enough to reap the financial rewards from his eloquently named bistro, La Ratatouille, most likely dying mere months after opening.\n\n\n\nThough the zombie-chef-vassal Linguine basically gives up cooking all together to glide around on roller skates, we can only hope that La Ratatouille has a strong enough institutional memory such that he is not made immediately jobless in the event of Little Chef’s death. Perhaps a smaller, food-loving creature, like a bug, could sit on top of the head of one of Remy’s offspring and be guided by hallucinations of ghost Remy. In turn, the Parisian food scene would become progressively smaller and smaller, and culinary trends would occur at increasingly shorter intervals as the life span of each puppet-master diminishes in magnitude. Mayfly-atouille would probably fit within a Tiktok.\n\n\n\nOf course, that would be ridiculous. Remy is one of a kind not only among other rats but among humans themselves. It is clear that by the end of the film, all other rats in the colony carry some semblance of the spirit of Chef Gusteau with them. They do not steal, they maintain cleanliness, and though Remy has preached the culinary gospel to the masses and has entered the kingdom of Gust…\n Wait? …. Is Remy the rat Jesus? \nThroughout the film, Remy seems to be the only one to be able to engage with the ghost of Chef Gusteau, though he is reminded in a scene where they are both simultaneously surprised, that the God-Chef is a figment of his own imagination. This insight tells us that Gusteau is most likely similar to a Christian god, existing in unison with the hearts and souls of rats, and not external to them.\nThe message that anyone can cook is further emphasised by Gusteau by his use of this projected internal voice. While Remy at this stage asserts himself as a messenger of Gusteau, the gospel is waiting to be preached. By spreading the culinary good-news to the colony, presumably each rat holds with them the spirit of Gusteau, while still clearly maintaining Remy in high-regard. With that, the holy trinity of Gusteau, Remy, and cooking spirit becomes whole.\n\n\n\nWe also observe that there were false prophets in this story, most notably Chef Skinner. Skinner claims to uphold Gusteau’s values but it is revealed that he does not care for good food, and exploits his position for financial gain at the expense of the reputation of Gusteau. For a short time, even zombie-chef Linguine’s actually talented and hardworking human muse Collette strays from the flock, unsure of the message that Remy preaches. But even she recognises the true message of Gusteau in opposition to the false prophet Skinner.\nWhile Pixar have been nuanced in their portrayal of rat-Jesus, there are two aspects I would like to speculate on where I believe the film did not go far enough, or that may become candidates for a strong sequel.\nThe first addresses the issue of Remy’s mortality. For Remy’s message to truly transcend the short lifespan of a rat and for him to gain immortal status as the one true culinary prophet, Remy had to die. Ideally, he would have remained in the rat-trap from which he inextricably escapes after his dad and brother push a gargoyle off of a church and onto the car boot, crushing the boot just enough to release him. This would have been a commoners death for Remy, unrelenting and sacrificial. In doing so, the rest of the colony would have been empowered to follow in his example, potentially leading to the financial success of La Ratatouille beyond opening night and the proceeding few months.\nBy instead witnessing the initial success of La Ratatouille, Remy more strongly implies that he himself is the anyone that Gusteau refers to. The remaining rats would most likely be less emboldended to consider running a restaurant for humans. In not sacrificing himself in the name of Gusteau’s message, Remy bears more resembelence to Jacob Frank, who claimed to be the reincarnation of the self-proclaimed messiah Sabbatai Zevi, and lived long enough to ensure that everyone around him got bored of the shtick, and his movement fizzled out2.\n2 This is genuinely the message I got from Olga Tokarczuk’s brilliant The Books of Jacob, which I read not in preparation for this article3 Of course, this sequel may take place in a Nietzschean future where Gusteau, as God, is himself dead. Potentially leading to postmodern rat cuisine, a return to eating ‘trash’ (rubbish), or potentially worse.4 Catatouille5 DogatouilleThe second avenue, given we know there is at least one messenger of Gusteau, is that there may be more. These may be copycats/rats that appear based on the presumed commercial success of La Ratatouille, or they may be genuine prophets of Gusteau. There is no reason that dead Gusteau should suddenly stop appearing in the imaginations of animals3, and similarly just because Remy is the main character we cannot disregard other animals that may have watched Chef Gusteau on TV, for example cats4and dogs5.\nDisregarding the thousands of potential heirs to the La Ratatouille empire specifically, these competing factions of culinarily inspired animals would inevitably vie for legitimacy in an increasingly crowded market. I imagine Remy’s dunce brother Emile would be the first to claim the bloodline potentially leading to a series of culinary-rat-pontiffs that become increasingly disliked. Lollard rats from the countryside may scoff when they hear news of a new Parisian loft bistro being opened, shrines to locations such as the farmhouse Remy originally lived in will be built, and academics will convene to discuss the implications of the trinity and the true nature of Gusteau.\nIt is clear that the prophecy of Remy, and resulting idyllic moment in time, will be short lived. La Ratatouille is built on much more worrisome foundations than it may seem. In preaching the good-news of Gusteau, but maintaining his own personal vice-like grip on animal-bistros without a clear succession plan for either his ideology or business, Remy will soon learn that anyone can cook.\n\nI have chosen not to touch upon the fact that Remy can control Linguine by pulling on his hair because that is completely unbelievable."
  },
  {
    "objectID": "posts/dead/index.html",
    "href": "posts/dead/index.html",
    "title": "The Dead Blog Theory",
    "section": "",
    "text": "Extra note: I appreciate that model collapse in practice doesn’t really occur, because of curated datasets and the effort that does into ensuring good training data. But indulge me.\n\n\nThis post likely doesn’t read well, or even make any sense. I would reccommend reading the archived version first for a full explanation. Click here.\nThis post is rotting, and will soon become AI slop. What follows is a love letter to an internet that will never exist again, it is a self-fulfilling obituary. Ashes to ashes, dust to dust, slop… to slop.\n\n\nThe dead content theory states that sooner rather than later the content on the internet will be in the background and the majority of traffic will be when the content cannot be accessed by bot ; traffic will be accessed only by the bots. Every time it becomes less of a problem with dead content - generally malicious content ;\n\n\nIn the beginning was the Word, and then the next Word and the next Word, and then after enough words, someone claimed that a large-language model could think, and the Word was God1.\n1 John 1:1, creatively embellished, some would say blasphemously.\n\nContent traditionally produced by marketers, copywrighters, and journalists is slowly being replaced with generative content from LLMs. But what happens when the next-generation of LLMs are being trained on this pseudo-data from the internet?\n\n\nThe equivalent of the dead internet theory for LLMs, model-collapse illustrates a scenario where datasets become so poisoned that it becomes impossible to ever train a new LLM effectively2.\n2 Why AI companies enter into multi-million pound contracts with news organisations with a wealth of verifiably human, proofread and well written content.\n\nLike pre-trinity test low background radiation steel3, pre-large-language model content will become sought after. Humanity will see a return to the handwritten word, literature originating before chatGPT will be considered sacred, evidence of genuine human achievement.\n3 Often stolen from WW2 shipwrecks for particle detectors.\nAnd here, we, go…4\n4 \nIn London there is a Raspberry Pi running a cronjob. It has access to the source code to this very post and is also loaded with a small local BERT-based model5.\n5 Specifically, google-bert/bert-base-uncased\n\n\n\nProof of life\n\n\n\nTwice a day, a random sentence from this post will be selected, and a random word will be omitted. The small, local language model will be them prompted to infer the missing word. This word will be replaced, my site re-rendered, the changes committed to the Git repository, and reflected here6. Additionally I will prompt the model to add a word a day to the bottom of this post.\n6 Stunning way to increase my Github contributions.\n\n\nVideo\nBlog collapse.\n\n\n\n\nView Script & LLM Inference Code\n\n#!/bin/bash\n\nsource mini_llama_env/bin/activate\npython script.py\ncd ../website\nquarto render\ngit add -A\ngit commit -m \"post continues to rot\"\ngit push \nfrom transformers import pipeline\nimport numpy as np\nimport time\n\ndef process_file(filename):\n    # Read file content\n    with open(filename, 'r') as file:\n        data = file.readlines()\n    \n    # Find lines with 'changeable_text'\n    changeable_lines = []\n    for i in range(len(data)):\n        if '''FLAG''' in data[i] :\n            changeable_lines.append(i+1)\n            break \n\n    random_line = np.random.choice(changeable_lines)\n    text = data[random_line]\n    \n    # Process footnote if present\n    if '^[' in text:\n        footnote_index = text.index('^[')\n        end_footnote = text.index('\\\\x00]')\n        pre_footnote, post_footnote = text.split('^[')[0], text.split(\"\\\\x00]\")[1]\n        footnote = text[footnote_index:end_footnote+5]\n        footnote_word_index = text[:footnote_index].count(' ')\n        print('FOOTNOTE WORD INDEX:', footnote_word_index)\n        data[random_line] = pre_footnote + post_footnote\n    \n    # Replace random word with [MASK]\n    line = data[random_line].split()\n    random_word_index = np.random.randint(0, len(line))\n    print('REMOVED WORD', line[random_word_index])\n    rem_word_store = line[random_word_index]\n    line[random_word_index] = '[MASK]'\n    line = ' '.join(line)\n    \n    # Use BERT to fill [MASK]\n    unmasker = pipeline('fill-mask', model='bert-base-uncased')\n    res = unmasker(line)\n    res.append({'token_str': rem_word_store})\n    word_index = 0 \n    while res[word_index]['token_str'] in ['.', ',', '!', '?']:\n        word_index += 1\n    replacement_word = res[word_index]['token_str']\n    print('REPLACEMENT WORD:', replacement_word)\n    line = line.replace('[MASK]', replacement_word)\n    \n    # Reinsert footnote if it existed\n    if 'footnote' in locals():\n        line_words = line.split()\n        line_words.insert(footnote_word_index, footnote)\n        line = ' '.join(line_words) + '\\n'\n    \n    line = line.replace(':::', '\\n:::\\n')\n    data[random_line] = line+'\\n'\n    \n    # Process 'extra' lines\n    extra_lines = [i+1 for i, line in enumerate(data) if 'FLAG' in line]\n    if extra_lines:\n        generated = data[extra_lines[0]].strip()\n        hypothesised = generated + ' [MASK]' + '. END OF STATEMENT.'\n        res = unmasker(hypothesised)\n        print('ADDED WORD:', res[0]['token_str'])\n        data[extra_lines[0]] = f\"{generated} {res[0]['token_str']}\\n\"\n    \n    # Update timestamp\n    utc_str = time.asctime(time.gmtime())\n    print('UTC:', utc_str)\n\n    data[-1] = f'''Updated: {utc_str}. Replaced {rem_word_store} with {replacement_word}. Added {res[0]['token_str']} to the end of the generated text.'''\n    \n    # Write updated content back to file\n    with open(filename, 'w') as file:\n        file.writelines(data)\n\nprocess_file('../website/posts/dead/index.qmd')\n\n\nToken to token, slop to slop, all good things must come to an end.\n\n\nSlop\n\nI the one left out there said no more words then left him speechless again . 1 pm est and then he left again again . 1 pm est est est\n\n\nUpdated: Sun Oct 13 20:03:07 2024. Replaced be with be. Added est to the end of the generated text."
  },
  {
    "objectID": "posts/dead/archived.html",
    "href": "posts/dead/archived.html",
    "title": "The Dead Blog Theory",
    "section": "",
    "text": "Extra note: I appreciate that model collapse in practice doesn’t really occur, because of curated datasets and the effort that does into ensuring good training data. But indulge me.\n\n\nThis post is rotting, and will soon become AI slop. What follows is a love letter to an internet that will never exist again, it is a self-fulfilling obituary. Ashes to ashes, dust to dust, slop… to slop.\n\n\nThe dead internet theory states that sooner rather than later, genuine activity on the internet will be in the minority, and the majority of traffic will come from bots. Content will be produced by bots, and it will be engaged with by other bots. Every day, it becomes less of a theory, with ‘dead’ content being generally characterised as slop.\n\n\nIn the beginning was the Word, and then the next Word and the next Word, and then after enough words, someone claimed that a large-language model could think, and the Word was God1.\n1 John 1:1, creatively embellished, some would say blasphemously.\n\nContent traditionally produced by marketers, copywrighters, and journalists is slowly being replaced with generative content from LLMs. But what happens when the next-generation of LLMs are being trained on this pseudo-data from the internet?\n\n\nThe equivalent of the dead internet theory for LLMs, model-collapse illustrates a scenario where datasets become so poisoned that it becomes impossible to ever train a new LLM effectively2.\n2 Why AI companies enter into multi-million pound contracts with news organisations with a wealth of verifiably human, proofread and well written content.\n\nLike pre-trinity test low background radiation steel3, pre-large-language model content will become sought after. Humanity will see a return to the handwritten word, literature originating before chatGPT will be considered sacred, evidence of genuine human achievement.\n3 Often stolen from WW2 shipwrecks for particle detectors.\nAnd here, we, go…4\n4 \nIn London there is a Raspberry Pi running a cronjob. It has access to the source code to this very post and is also loaded with a small local BERT-based model5.\n5 Specifically, google-bert/bert-base-uncased\n\n\n\nProof of life\n\n\n\nTwice a day, a random sentence from this post will be selected, and a random word will be omitted. The small, local language model will be them prompted to infer the missing word. This word will be replaced, my site re-rendered, the changes committed to the Git repository, and reflected here6. Additionally I will prompt the model to add a word a day to the bottom of this post.\n6 Stunning way to increase my Github contributions.\n\n\nVideo\nBlog collapse.\n\n\n\n\nView Script & LLM Inference Code\n\n#!/bin/bash\n\nsource mini_llama_env/bin/activate\npython script.py\ncd ../website\nquarto render\ngit add -A\ngit commit -m \"post continues to rot\"\ngit push \nfrom transformers import pipeline\nimport numpy as np\nimport time\n\ndef process_file(filename):\n    # Read file content\n    with open(filename, 'r') as file:\n        data = file.readlines()\n    \n    # Find lines with 'changeable_text'\n    changeable_lines = []\n    for i in range(len(data)):\n        if '''FLAG''' in data[i] :\n            changeable_lines.append(i+1)\n            break \n\n    random_line = np.random.choice(changeable_lines)\n    text = data[random_line]\n    \n    # Process footnote if present\n    if '^[' in text:\n        footnote_index = text.index('^[')\n        end_footnote = text.index('\\\\x00]')\n        pre_footnote, post_footnote = text.split('^[')[0], text.split(\"\\\\x00]\")[1]\n        footnote = text[footnote_index:end_footnote+5]\n        footnote_word_index = text[:footnote_index].count(' ')\n        print('FOOTNOTE WORD INDEX:', footnote_word_index)\n        data[random_line] = pre_footnote + post_footnote\n    \n    # Replace random word with [MASK]\n    line = data[random_line].split()\n    random_word_index = np.random.randint(0, len(line))\n    print('REMOVED WORD', line[random_word_index])\n    rem_word_store = line[random_word_index]\n    line[random_word_index] = '[MASK]'\n    line = ' '.join(line)\n    \n    # Use BERT to fill [MASK]\n    unmasker = pipeline('fill-mask', model='bert-base-uncased')\n    res = unmasker(line)\n    res.append({'token_str': rem_word_store})\n    word_index = 0 \n    while res[word_index]['token_str'] in ['.', ',', '!', '?']:\n        word_index += 1\n    replacement_word = res[word_index]['token_str']\n    print('REPLACEMENT WORD:', replacement_word)\n    line = line.replace('[MASK]', replacement_word)\n    \n    # Reinsert footnote if it existed\n    if 'footnote' in locals():\n        line_words = line.split()\n        line_words.insert(footnote_word_index, footnote)\n        line = ' '.join(line_words) + '\\n'\n    \n    line = line.replace(':::', '\\n:::\\n')\n    data[random_line] = line+'\\n'\n    \n    # Process 'extra' lines\n    extra_lines = [i+1 for i, line in enumerate(data) if 'FLAG' in line]\n    if extra_lines:\n        generated = data[extra_lines[0]].strip()\n        hypothesised = generated + ' [MASK]' + '. END OF STATEMENT.'\n        res = unmasker(hypothesised)\n        print('ADDED WORD:', res[0]['token_str'])\n        data[extra_lines[0]] = f\"{generated} {res[0]['token_str']}\\n\"\n    \n    # Update timestamp\n    utc_str = time.asctime(time.gmtime())\n    print('UTC:', utc_str)\n\n    data[-1] = f'''Updated: {utc_str}. Replaced {rem_word_store} with {replacement_word}. Added {res[0]['token_str']} to the end of the generated text.'''\n    \n    # Write updated content back to file\n    with open(filename, 'w') as file:\n        file.writelines(data)\n\nprocess_file('../website/posts/dead/index.qmd')\n\n\nToken to token, slop to slop, all good things must come to an end.\n\n\nI .\n\nUpdated: Fri Aug 23 10:55:41 2024. Replaced Content with content. Added . to the end of the generated text."
  },
  {
    "objectID": "posts/oulipo/index.html",
    "href": "posts/oulipo/index.html",
    "title": "The Potential Literature Horseshoe",
    "section": "",
    "text": "defines = ['defined', 'specified'];\nstructure = ['structure','form','composition']\nmay = ['may','could','might','may well']\nlanguage = ['language','literature','text','writing'];\nbooks = ['3000 year old Beowolf','600 year old Sir Gawain and the Green Knight'];\npurely = ['purely','solely']\nmillennia = ['millennia','hundreds of years','thousands of years','centuries']\nproduce = ['produce','construct','create']\ntechniques = ['techniques','methods']\nwriters = ['writers','linguists','proponents','members'];\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you refresh this page, the article will be randomised. There are \\(2.66 \\times 10^{19}\\) potential combinations of images and text, resulting in approximately Twenty Billion Billion articles, equivalent to the number of insects on earth. Vingt Milliards de Milliards.\n\n\nrnd = Math.floor(Math.random() * 2)\ns_string = {\n  if (rnd === 0) {\n    return html`&lt;img src=\"logo.png\" width=70% title=\"Basile Morin, [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0)\"&gt;`\n  }\n  else {\n    return html`&lt;img src=\"other_logo.png\" width=60% title=\"Basile Morin, [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0)\"&gt;`\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasile Morin, CC BY-SA 4.0\nOuvroir de Littérature Potentielle or Oulipo has been  as “the search for new structures, which may be used by writers in any way they see fit” (Motte 1998, p2–3). For , authors have been guided by language constraints resulting in  s; From the alliterative verse of the , to the syllabic rigidity of Japanese haikus. By formalising a number of individual vocations, the original Oulipo  in the early 1960s were embarking on the revitalisation of  generation in search of discovering something deeper, sound familiar?\n\nrndl = Math.floor(Math.random() * 2)\nl_string = {\n  if (rnd === 0) {\n    return html`&lt;a href=\"https://en.wikipedia.org/wiki/A_Void\"&gt;not using the letter e&lt;/a&gt;`\n  }\n  else {\n    return html`replacing every noun with the seventh next noun in the dictionary (S+7 rule)`\n  }\n}\n\no_string = {\n  if (rnd === 0) {\n    return html`replacing every noun with the seventh next noun in the dictionary (S+7 rule)`\n  }\n  else {\n    return html`&lt;a href=\"https://en.wikipedia.org/wiki/A_Void\"&gt;not using the letter e&lt;/a&gt;`\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn order to define/identify new s,  in France turned to automated transformation . These often instantiated as rules or constraints, such as , , or only using a single vowel letter. As the originators of Oulipo explored ways to manipulate language and text in search of new s, they turned to mathematics for inspiration, thus leading to the final vocation; the transposition of mathematics to words. Naturally the extension was made to computers, and the combinatorial nature of  generation was quickly highlighted, as mathematician Claude Berge writes (Motte 1998, p152) :\n\n[…] we believe, that the aid of a computer, far from replacing the creative act of the artist, permits the latter rather to liberate himself from the combinatory search, allowing him also the best chance of concentrating on this “clinamen”1 which, alone, can make of the text a true work of art.\n1 Clinamen (/klaɪˈneɪmən/;) is the Latin name Lucretius gave to the unpredictable swerve of atoms, in order to defend the atomistic doctrine of Epicurus. […] it has come more generally to mean an inclination or a bias. Source\nIn applying combinatory literature, Oulipo , most notably Raymond Queneau in his 1961 work Cent Mille Milliards de Poemes (Hundred Thousand Billion Poems), have highlighted the impossible complexity of language. Elegantly embracing this complexity, Queneau simply presents the complete set of lines within a sonnet in the form of cut out strips, any combination of which  be constructed by the reader. In describing the combinatorial nature of Cent Mille Milliards de Poemes, Berge presents the following figure…\n\nrnd_nn = Math.floor(Math.random() * 2)\nnn_string = {\n  if (rnd === 0) {\n    return html`&lt;img src=\"nn.png\" width=70%&gt;`\n  }\n  else {\n    return html`&lt;img src=\"nn2.png\" width=100%&gt;`\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVerses act equivalently to neural network layers, and phrases correspond to discrete nodes… could we be seeing the early sparks of the use of neural networks for language modelling?2 Of course, what is missing is the mathematical transformations from layer to layer, token embeddings, attention etcetera… but it is interesting to consider the thematic similarities of complexity, , and distillation (or lack thereof) of  that both members of Oulipo and machine learning researchers have successfully applied.\n2 The modern use of neural networks to model language can probably be attributed to Bengio et. al, 2003, 42 years later.\nrnd_gp = Math.floor(Math.random() * 2)\ngp_string = {\n  if (rnd === 0) {\n    return [html`&lt;img src=\"oulipo.jpeg\" width=60%&gt;`,html`&lt;em&gt;Oulipo Group à Boulogne, avenue de la Reine, on September 23, 1975, in the garden of François Le Lionnais’s house. At the center Raymond Queneau and François Le Lionnais, on his side, with a nespaper in the hands. On the very left Italo Calvino. &lt;a href=\"https://www.arshake.com/en/oulipo-ouvroir-de-litterature-potentielle/\"&gt;Source&lt;/a&gt;.&lt;/em&gt;`]\n  }\n  else {\n    return [html`&lt;img src=\"raymond.jpeg\" width=60%&gt;`,html`&lt;em&gt;Raymond Queneau with the group Les Frères Jacques in 1954, Getty Images/Keystone.&lt;a href=\"https://ici.radio-canada.ca/ohdio/premiere/emissions/aujourd-hui-l-histoire/segments/entrevue/351266/oulipo-richard-boivin\"&gt;Source&lt;/a&gt;.&lt;/em&gt;`]\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGiven that a modern day large language model such as GPT-4  reasonably be described as a very large set of well  mathematical rules, could this ever  a valid literary ? How are LLMs distinct from more basic rules such as S+7? In addition, multi-model models can now see and hear as well as read. According to David Chalmers these additional senses  result in fish-level consciousness in the next ten years. Consciousness aside, at the very least LLMs will appear smarter than a human by then. What are the implications for Potential Literature when a set of rules and resulting  can think for itself?\nUnlike the output from an LLM, a haiku can be instantly recognised and verified as coming from its defining 5-7-5 syllabic writing . When the  itself is visible in the output, the focus is turned  to what Berge refers to as the clinamen. It is here that the meaning is gained.\nThe difference between LLMs and a well-established language  such as a haiku is one of complexity, and the  that results. By extending a series of basic transformations and rules towards LLMs, a horseshoe effect is achieved. The sheer quantity of rules within an LLM reflects a removal of linguistic constraints, order is returned from chaos, and the resulting underlying  is obscured.3\n3 Though for now (Jan 2024) LLMs produce hallucinations, incorrect facts and statements which in turn partially disclose the underlying structure of the neural network.\nrnd_d = Math.floor(Math.random() * 3)\ndep_string = {\n  if (rnd_d === 0) {\n    return html`&lt;img src=\"dep.png\" width=60%\"&gt;`\n  }\n  else if (rnd_d === 1) {\n    return html`&lt;img src=\"dep2.png\" width=60%&gt;`\n  }\n  else if (rnd_d === 2) {\n    return html`&lt;img src=\"dep3.png\" width=60%&gt;`\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHerein lies a paradox for the use of language models as a direct tool for creativity, as a platform for a distinct linguistic structure. A language model is trained to  text that is indistinguishable from the text within its training distribution. However, as soon as it achieves this task, the underlying structure becomes obscured as complete ‘order’ is achieved, and the impact is lost. The relationship between the rules and their resulting linguistic  is distinctly one-way, it is nearly4 impossible to reason whether the content has been generated from a neural network, yet alone infer the parameters, or even recognise the architecture. Would a haiku still be a haiku if you couldn’t immediately infer its syllabic structure?\n4 This nearly contains a level of nuance, as identifying whether an output has been generated by an LLM is an open research area (See On the Reliability of Watermarks for Large Language Models and A Watermark for Large Language Models). However, the point remains. To the average reader, without the use of cryptographic tools, modern LLM output (as of Jan 2024) remains practically indistinguishable from human-level text.To answer the question posted in the title: Has Machine Learning Solved Oulipo? The answer, is no. The beauty of Potential Literature is that by enforcing literary constraints and therefore defining new s, we not only place the emphasis  on the meaning, but also enable the reader to interpret  in new ways through a sense of disorder. As a set of rules and constraints, an LLM succeeding in its objective only serves to obscure the resulting linguistic structure in a bid to ‘seem human’, regaining order from chaos.\nIt is clear that these two vocations in the field of Potential Literature: defining rules and constraints for , as well as identifying new language structures, are inherently linked. However, these two objectives conflict when rule set is as flexible as it is within an LLM.\nAs Italo Calvino wrote in late 1967, in a remarkable foreshadowing (Motte 1998, Cybernetics and Ghosts (1967) p13-14):\n\nMotte, W. F. 1998. Oulipo: A Primer of Potential Literature. Dalkey Archive Paperbacks. Dalkey Archive Press.\n\nThe true literature machine will be one that itself feels the need to produce disorder, as a reaction against its preceding production of order: a machine that will produce avant-garde work to free its circuits when they are choked by too long a production of classicism. […]. To gratify critics who look for similarities between things literary and things historical, sociological, or economic, the machine could correlate its changes of style to the variations in certain statistical indices of production, or income, or military expenditure, or the distribution of decision-making powers. That indeed will be the literature that corresponds perfectly to a theoretical hypothesis: it will, at last, be the literature.\n\nCalvino, I’m sure would argue, that we have the alignment of LLMs all wrong to  truly novel literature, and how would Raymond Queneau respond to the combinatorial possibilities of large-language models?\nCent Mille Milliards de Poemes can be seen as analogous to a partially trained neural network itself, as a mathematical object. There is beauty in the disorder of the relationship between tokens, and the rules that can be combined to  language. However as soon as an output is produced and the combinatorics collapses, the  is obscured, and like a phantom the meaning disappears.\n\n\n\n\n\n\n\n\nCitationBibTeX citation:@online{2024,\n  author = {},\n  title = {The {Potential} {Literature} {Horseshoe}},\n  date = {2024-01-09},\n  url = {https://sav.phd/posts/oulipo},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“The Potential Literature Horseshoe.” 2024. January 9,\n2024. https://sav.phd/posts/oulipo."
  },
  {
    "objectID": "posts/llm/index.html",
    "href": "posts/llm/index.html",
    "title": "Has an LLM Cultural Victory Already Been Achieved?",
    "section": "",
    "text": "There has been huge talk recently regarding the performance-to-cost ratio of Deepseek’s LLM products, particularly their new reasoning model causing OpenAI’s reasoning-based moat to partially evaporate. Companies like Perplexity, Cursor & Co (including most YC startups these days) have been quick to integrate these with good reason, most likely saving themselves some money in the process. Most likely, Deepseek have generated a large amount of synthetic data from state-of-the-art models such as o1, or by cleverly prompting non-reasoning models such as 4o or their own V3. As an accepted practice, this is the core idea behind recursive improvement, so no harm done. And if OpenAI and others have priced their API correctly, then they’re also set to gain. Everyone’s a winner (spoilers, they are not), and we’ll only see more of this in the future.\nI personally have been benchmarking LLMs to make decisions within Bayesian optimisation loops, in a bid to eke out some in-silico performance (for functions that behave reasonably). Every time I reach a natural conclusion to benchmarking where the next best model is too expensive to run on my test suite, someone pushes the Pareto front a little bit more in the direction of ‘time to stop writing that preprint and get some more results’. The impact of these models will be never be as small as it is now.\nA obvious point that people still seem to find surprising is that Deepseek censor inputs and outputs to align with the views of the Chinese government, specifically (again, rather obviously) events surrounding Tiananmen Square. Given Deepseek’s servers will be in China, it’s an aspect of communication that the Government has enforced for practically as long as the internet has existed.\nI consider this very much a non-story. What I find more interesting are what underlying values the LLM has. These go deeper than specific phrases or keywords and enter into the murky realm of thoughts and feelings.\n\n\n\n\nA not very interesting discussion that has not much to do with machine learning at all.\n\n\n\nI’ve written previously about values and LLMs, and how datasets not only contain a representation of language as a structure, but the values of those who have written or collated the text. In effect, we shouldn’t be surprised when models from other countries appear misaligned to our own, Western, benchmarks, if they have been trained on distinct datasets. Even with learned translation, we shouldn’t be surprised if we don’t find them particularly revealing, especially as we approach concepts such as ‘intelligence’. Likewise, as we in the West share more common values with current SOTA LLMs than those elsewhere, we shouldn’t be surprised if they are found less appealing or useful in other countries. This is a hidden upside to the development of LLMs, they are implicitly more accessible, beneficial and provide more productivity to those with similar values.\n\n\n\n\nA slightly more nuanced comment. Some hints that there is more at play than just basic censorship.\n\n\n\nSo what work has already been done in this field? The preprint “LLM-GLOBE: A Benchmark Evaluating the Cultural Values Embedded in LLM Output” presents a methodology based from a framework devised in 2004. Outputs from LLMs are ranked using an automated ‘jury’ across a number of criteria including “Institutional Collectivism” and “Future Orientation” etc… It’s a logical extension to the original methodology, and I won’t argue against this work or the proceeding work on GLOBE (for now). What I care about are the results. So after all that, we have a brand new suite that measures exactly what we’re interested in. Not censorship, or language, but values. Time to reveal the results…\n\n\n\nOh. They look strikingly similar. So Chinese LLMs (Qwen) and English LLMs (GPT-4) really do have the same values? The authors claim that Chinese LLMs score (quantitatively) significantly higher on cultural rankings, though another conclusion is that LLMs are ‘…not perfect representations of the cultural values of development contexts…’. In my eyes these are slightly conflicting. So what is happening here?\nI suspect that datasets have a lot to do with it. As is common and now an encouraged direction, LLMs are being trained with synthetic datasets derived from other LLMs on the Pareto front of cost-to-performance, depending on how much you value volume over quality. The implication is that unless you go and collect a large amount of ‘virgin’ data (arguably this is now impossible), you are building on the foundations that previous LLMs have constructed, and implicitly, the values contained within them. Similar to how all modern compilers stem from the original A-0 compiler by Grace Hopper.\nFor a company like Deepseek, this poses an interesting question. If you value money, pursue what people will buy at breakneck speed. Use all the foundational work available to you and built a product that is so cheap and so effective that market forces takes over. Then you can worry about values. But for now, assuming that geopolitical forces at play are using LLMs like pawns on a chessboard, models like Deepseek are built on Western foundations. While OpenAI may take a temporary minor financial hit because of a potentially reduced market share, the game was set in motion a few years ago now, and it will be difficult to untangle.\nThere is a way out for Deepseek and its hypothetically value-driven creators. The model is cheap and good enough that large synthetic datasets will be created from it. By shifting the dial in favour of your own beliefs, or augmenting your model with your own value-driven data, it may be possible to shift the dial against Western values. There are glimmers of this seen in the tweet above which I will re-present.\n\n\n\n\nA slightly more nuanced comment. Some hints that there is more at play than just basic censorship.\n\n\n\nHowever, the game is being played continuously, not turn-by-turn. By announcing huge amounts of funding for data centres such as Stargate, the West is aggressively attempting to not only win this race over the values of future LLMs, but remove the need for it. The sheer volume of compute is a hedge against other LLMs and a mitigation strategy against having to use the best data available on the open (geopolitical) market, by way of self improvement and reinforcement learning in a controlled setting. So has cultural victory been achieved? Can other nations untangle the roots that have grown? It will be a huge effort.\n\nAPPENDIX - EMPIRE OF SIGNS - ROLAND BARTHES\nRecently I read Empire of Signs by Roland Barthes on the recommendation of Sunil Manghani, ahead of a trip to visit a friend in Japan. Barthes describes a fictional Japan, used as a device to counter Western ideologies and values. As follows:\nBarthes broadly outlines a vision of Japan, and more generally the East, as a place where the sum of the parts don’t make a whole, but signify the act of the summation itself, the inbetween-ness of the parts. He makes this point by commenting on the writing system (composition of smaller signs), food (which itself exists in divisions that can only be divided further by chopsticks, never violently cut or destroyed), and through the act of giving gifts, wherein the packaging and act of giving a gift outweighs the contents. There is no centre, no answer, no true meaning to life in Barthes’ created world, it exists in the in-between spaces. Barthes describes the act of writing a Haiku like taking a single picture, without any film (or a memory card) in the camera.\nWe in the West want to impose a meaning to our models and their outputs, maybe stemming from our core belief of individualism, seeking to look inwards and attempting to assign a meaning or search for a soul. For those in Barthes Japan, the act of using an LLM, or the tokens themselves may have more importance. The instant that language is generated, but not the language itself, the process. I haven’t properly thought about the implications of this, but it leans into the creative process I am sure, and how LLMs can truly contribute outside of finding a creative solution. I’m sure Sunil and myself will discuss further, but speculatively, maybe the solution for the East and those without Western ideologies is to think beyond our established idea of ‘chatting’, implicitly with another being, a thing, a whole. We might not find it interesting or useful, but others might, and that’s OK."
  },
  {
    "objectID": "posts/transubstantiation/index.html",
    "href": "posts/transubstantiation/index.html",
    "title": "The Machine Learning Reformation",
    "section": "",
    "text": "Attending mass was a mandatory requirement in medieval England and formed a central foundation for many aspects of parish life. The highlight of mass was the elevation of the host, physically transformed into the body of Christ. This act could only be performed by the parish priest and was consumed only by those priviledged few within the chancel (Orme 2021).\nObserving the genuine body of Christ was of the utmost importance for the laity. However, physical (chancel screens) and mental (understanding Latin) barriers stood between them and the clergy. In time, groups such as the Lollards, lead by John Wycliffe rejected transubstantiation and its importance, setting the course for the democratisation of Christianity in England. However, these barriers served as a reminder of the heirarchy the Catholic Church had created, until the reformation in 1547.\nSam Altman, Elon Musk, and many others continue to insist that artificial general intelligence (AGI) will be achieved in the next few years. If and when AGI is announced, divisions will open up, Microsoft will contractually lose access to OpenAI’s latest models (OpenAI 2024), there will be calls of heresy and false prophets, and the accompanying video will almost certainly be presented by someone on an unfathomable compensation package.\nThere will be those, armed with heavily quantized open-source LLMs running locally on Macbooks, akin to Gutenberg Bibles, that will insist that AGI is not real. AGI was never the goal for the majority of people. AI has the ability to change the way that engineers, writers, and artists work, and the democratisation of open-source LLMs, serves to benefit the lives of everyone1 equally.\nThe current power struggles within machine learning are fought on inherently Christian lines. From the French Yann ‘John Calvin’ LeCun’s open-source ideology and scepticism in the real presence of AGI, to Sam Altman’s claim to primacy and inevitable monopoly on the transubstantiation of a super-us.\nA key assumption is that if AGI occurs, it will be useful. This will be the nail in the coffin that will seal the fate of the Macbook Lollards in the end. But what happens if it is not?\nIn more recent years Michael Craig-Martin affirmed the medieval world view of transubstantiation with An Oak Tree. Capturing the sense of belief that was felt by medieval Christians by enforcing that a glass of water placed high on a glass shelf was in fact a genuine oak tree. To believe that that glass of water is an oak tree, is to make a leap of faith.\nAs Tom Holland (Holland 2019) would point out, we have made, and continue to make these leaps of faith on a daily basis. The concept of human rights for example is one that we simply choose to believe, in the view that it will make the world a better place2.\nAn Oak Tree invites us to take this leap, and embrace it, unapologetically confronting us with its presence. Despite January 1st being an arbitrary day, it feels like a new beginning because we all collectively believe it’s a new beginning.\nAs AI approaches levels of human intelligence, some will choose to make the leap of faith and some will not. We should relish and enjoy the spectacle of AGI, as medieval parishioners would have done at mass every Sunday.\nHowever, we must not lose sight of the ability of machine learning to change our lives in more tangible ways.\nIt will not be an AI that replaces your job, but someone with the ability to use AI."
  },
  {
    "objectID": "posts/transubstantiation/index.html#footnotes",
    "href": "posts/transubstantiation/index.html#footnotes",
    "title": "The Machine Learning Reformation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWith enough compute↩︎\nItself a goal that we deem somewhat important.↩︎"
  },
  {
    "objectID": "posts/books/index.html",
    "href": "posts/books/index.html",
    "title": "Optimal Bookshelf Organisation",
    "section": "",
    "text": "How someone organises, or chooses not to organise a bookshelf can say a lot about them. I personally don’t think there is any sense in enforcing a specific order. My strategy can be summarised as ‘vibes based’. There is joy in being able to know roughly where a book is and pick it out mid conversation. It makes me feel like I’m some sort of magician or an old librarian, holding the secret mental key that will unscramble order from chaos.\nBut, if you don’t believe in this serendipity, you might be inclined to choose something to base the order on. Given we are pretty good at remembering the title of a book, maybe alphabetical order is good. Actually, authors should be grouped together, so let’s make them alphabetical. What about we do that and then also group some categories of similar books together (S/O Dewey). The most intelligent of us out there may choose to forego any sense of useful meaning, and satirically choose to base the order on the colour of the cover, subverting the old book judging adage.\n\n\n\n\nNo Comment\n\n\n\nThese approaches have been discussed at length here here here here and here, you get the idea. The reason they’re up for debate is because none of them are truly optimal. None take advantage of all the available information… Until now. Later on in this post I will describe an optimal strategy to sorting a book collection that I’ve also made open source, but first we must collect some data.\nDigitising all the books I own is a task low down on my todo list (compared to finishing my PhD). So to aid in this, I created a Command Line Interface (CLI) tool to help me. It’s available here and I’ve creatively named it optimal-bookshelf. It allows you to add books to a persistent locally-stored virtual library by searching for a title using open book APIs (i.e. Google Books), after which a more thorough search is performed and the specific edition can be selected. Ideally most of the information is correct at this point, but you then have the option to edit specific data such as the ISBN. I tailored this tool as I was digitising my collection, so it should be as efficient as possible on the Pareto curve of information vs time.\nThis enabled me to create an accurate digital representation of all the books I own, down to the specific edition, in a relatively short amount of time.\n\n\n\nBut what to do with this wealth of information? Well, for now it is largely text-based. To do anything that will concern a notion optimality, we need numbers. It follows that we can summarise all the information that we have generated, and result in a numerical representation by creating embeddings of our virtualised book-twins. By tokenising the information, embeddings represent text as dense vectors in a high-dimensional space using models trained specifically for this purpose. These embedding models are designed to capture semantic relationships between words and phrases, allowing us to perform mathematical operations that measure how similar different pieces of text are to each other. In the optimal-bookshelf CLI, embeddings are created using OpenAI’stext-embedding-3-large model and can easily be generated using the $ bookshelf embed command.\nIf I make the assumption that I want books near each other to be semantically related (i.e. have a low Euclidian distance in embedding space), and that every book must appear on my bookshelf exactly once, I can equivalently formulate this task as an optimisation problem, minimising the total distance travelled by traversing from embedding to embedding. In this case I don’t mind that this trip isn’t a loop (because my bookshelf is a ‘line’ and not a circle, but this can be revisited later), resulting in a formulation of the classic integer programming no-return Travelling Salesmen Problem. Solving this will result in a sequence of books that embodies the lowest total semantic difference between neighbouring books. It makes the most sense, in ways that are not just affiliated with the title or the author, or even the genre, but the fundamental concept of that book . Having generated embeddings, solving this problem is easy in optimal-bookshelf. Just run the command bookshelf tsp.\nFor visualisation purposes, you can also solve a TSP over embeddings that have first been reduced to two dimensions. The easiest and generally most effective way to do this in situations where the meaning isn’t too important (alert they can be misleading) is to use t-SNE. Running bookshelf tsp -v(for visual) first uses t-SNE to reduce the dimension of the embeddings, and then solves the reduced dimension TSP (as the TSP relies on a distance matrix it scales with number of points and not dimensionality) resulting in the following:\n\n\n\nIt’s really interesting to follow the path and see themes emerging based on author or publication date, or even if the book is read, unread, or in progress, though I’m choosing to keep secret and just label the titles. It all acts like a constellation of my reading interests and habits, similar to the manually created constellation of artists at the Tate Liverpool.\n\n\n\n\nTate Liverpool Constellations\n\n\n\nI think an interesting extension would be to specify a certain number of shelves, then solve the TSP for a corresponding number of lines equal to the available shelves. In theory I have the number of pages in each book available, so I could add a physical packing constraint here to ensure all the books fit. For full transparency, here’s my current optimal shelf based on this semantic TSP. Spot the various threads that emerge…\n1. GRAVITY'S RAINBOW\n2. THE MASTERS\n3. THE BOOKS OF JACOB\n4. FLIGHTS\n5. DRIVE YOUR PLOW OVER THE BONES OF THE DEAD.\n6. THE PLAGUE\n7. PANDAEMONIUM 1660–1886\n8. UTOPIA\n9. POLITICS ON THE EDGE\n10. THE RESTLESS REPUBLIC: BRITAIN WITHOUT A CROWN\n11. 1599: A YEAR IN THE LIFE OF WILLIAM SHAKESPEARE\n12. WILLIAM SHAKESPEARE POETRY\n13. OEDIPUS AT KOLONOS\n14. THE ODYSSEY: TRANSLATED BY EMILY WILSON\n15. THE ILIAD: TRANSLATED BY EMILY WILSON\n16. THE DIVINE COMEDY\n17. DON QUIXOTE\n18. THE THREE MUSKETEERS\n19. THE COUNT OF MONTE CRISTO\n20. CRIME AND PUNISHMENT\n21. THE KARAMAZOV BROTHERS\n22. THE IDIOT\n23. HUMAN, ALL TOO HUMAN & BEYOND GOOD AND EVIL\n24. THE EMANCIPATION PROCLAMATION\n25. THINK AND GROW RICH\n26. TALKING TO STRANGERS\n27. ESCAPE FROM MODEL LAND\n28. VALUES\n29. DOMINION\n30. JERUSALEM\n31. THE WORLD OF STONEHENGE\n32. GOING TO CHURCH IN MEDIEVAL ENGLAND\n33. THE RUIN OF ALL WITCHES\n34. MIDNIGHT IN CHERNOBYL\n35. ABYSS: THE CUBAN MISSILE CRISIS 1962\n36. SECRET WARS\n37. DIVIDED HOUSES: THE HUNDRED YEARS WAR III\n38. TRIAL BY BATTLE: THE HUNDRED YEARS WAR I\n39. TRIAL BY FIRE: THE HUNDRED YEARS WAR II\n40. HIGH PERFORMANCE ROWING\n41. WRITING ABOUT SPORT\n42. GAZZA AGONISTES\n43. THE LAST LEONARDO\n44. THE CREATIVE ACT\n45. ON ART AND LIFE\n46. STORY OF ART\n47. THE PENGUIN BOOK OF CLASSICAL MYTHS\n48. VENI, VIDI, VICI\n49. THE TWELVE CAESARS\n50. THE HISTORY OF THE DECLINE AND FALL OF THE ROMAN EMPIRE: ABRIDGED EDITION\n51. PAX\n52. ASTERIX: ASTERIX AND THE WHITE IRIS\n53. SUPER-INFINITE\n54. SYSTEMS FOR...\n55. MOBILE MANIA\n56. THE ICON CATALOGUE UK GARAGE VOL. 1\n57. LONDON FEEDS ITSELF\n58. LONDON FIELDS\n59. DAMASCUS STATION\n60. MOSCOW X\n61. A PERFECT SPY\n62. THE SPY AND THE TRAITOR\n63. BEHIND THE ENIGMA\n64. THE MERCENARY RIVER\n65. THE LADYBIRD BOOK OF THE HANGOVER\n66. STIG OF THE DUMP\n67. THE TALES OF BEEDLE THE BARD\n68. HARRY POTTER AND THE HALF-BLOOD PRINCE\n69. JOURNEY TO THE CENTRE OF THE EARTH\n70. DR JEKYLL AND MR HYDE\n71. TREASURE ISLAND\n72. TOM SAWYER & HUCKLEBERRY FINN\n73. THE ADVENTURES AND MEMOIRS OF SHERLOCK HOLMES\n74. THE RETURN OF SHERLOCK HOLMES\n75. MACHINES OF LOVING GRACE\n76. PROCESS DYNAMICS AND CONTROL\n77. GAUSSIAN PROCESSES FOR MACHINE LEARNING\n78. BAYESIAN OPTIMIZATION\n79. NUMERICAL OPTIMIZATION\n80. ROBUST OPTIMIZATION\n81. FOUNDATIONS OF APPLIED MATHEMATICS, VOLUME 2\n82. FOUNDATIONS OF APPLIED MATHEMATICS, VOLUME I\n83. HOW TO PROVE IT\n84. INTRODUCING LOGIC\n85. INTRODUCING QUANTUM THEORY\n86. INTRODUCING CHAOS\n87. INTRODUCING FRACTALS\n88. INTRODUCING INFINITY\n89. OUR MATHEMATICAL UNIVERSE\n90. COLLINS DICTIONARY OF MATHEMATICS\n91. FOUR COLORS SUFFICE\n92. RINGWORLD\n93. THE LORD OF THE RINGS\n94. THE HOBBIT\n95. CLOUD ATLAS\n96. A SUPPOSEDLY FUN THING I'LL NEVER DO AGAIN\n97. INFINITE JEST\n98. AMERICAN PSYCHO\n99. BRING UP THE BODIES\n100. THE NATION KILLERS\n101. BURY MY HEART AT WOUNDED KNEE\n102. STALINGRAD\n103. WAR AND PEACE\n104. NAPOLEON IN EGYPT\n105. GREECE\n106. ATHENS\n107. DEMOCRACY'S BEGINNING\n108. THE BATTLE FOR THE ARAB SPRING\n109. MIKE CONTRE-ATTAQUE!\n110. KISSINGER\n111. THE ESCAPE ARTIST\n112. 1000 YEARS OF JOYS AND SORROWS\n113. LUCKY KUNST\n114. MURDER ON THE DARTS BOARD\n115. GONE FISHING\n116. THE SATSUMA COMPLEX\n117. THAT'S YOUR LOT\n118. MARCH OF THE LEMMINGS\n119. HOUSE ARREST\n120. COMING HOME\n121. NO TURNING BACK\n122. KILLING THATCHER: THE IRA, THE MANHUNT AND THE LONG WAR ON THE CROWN\n123. SAY NOTHING: A TRUE STORY OF MURDER AND MEMORY IN NORTHERN IRELAND\n124. FALL\n125. ONE TWO THREE FOUR: THE BEATLES IN TIME\n126. K-PUNK\n127. DEATH AND THE PENGUIN\n128. MURDLE\n129. THE HARD-BOILED WONDERLAND AND THE END OF THE WORLD\n130. ON THE ROAD\n131. A MOVEABLE FEAST\n132. FLAUBERT'S PARROT\n133. NINETEEN EIGHTY-FOUR\n134. TENDER IS THE NIGHT\n135. THE UNBEARABLE LIGHTNESS OF BEING\n136. SLAUGHTERHOUSE-FIVE\n137. CATCH-22\n138. WE ALWAYS TREAT WOMEN TOO WELL\nI think another next logical step would be to build in a recommendation tool based on the vector embeddings and maybe even the resulting TSP solutions. This would potentially result in a bi-level integer program (hard) depending on what you deem most important (add a book that would fit most optimally within a current collection of books, or add a book that would result in the most added distance to the resulting optimal semantic tour?), but with a small enough number candidates this could be solved without approximation. We will see.\nWas it worth it? Perhaps the lesson in all of this is that fulfilment lies not in choosing between chaos and order, but in finding systems that embrace both—straddling each paradigm, allowing us to be both the organised librarian and to take a book for a walk.\n\n\n\n\nPaul Klee.\n\n\n\nAs I’ve quoted before, Donella Meadows states in her book Thinking in Systems: A Primer:\n\nThere is yet one leverage point that is even higher than changing a paradigm. That is to keep oneself unattached in the arena of paradigms, to stay flexible, to realize that no paradigm is “true”, that every one, including the one that sweetly shapes your own worldview, is a tremendously limited understanding of an immense and amazing universe that is far beyond human comprehension.\n\nAll code for this post can be found here: https://github.com/trsav/bookshelf"
  },
  {
    "objectID": "posts/bayesian_optimisation/index.html",
    "href": "posts/bayesian_optimisation/index.html",
    "title": "Tiny Bayesian Optimisation",
    "section": "",
    "text": "This GIF was made with only 100 lines of vanilla Python + Numpy + Matplotlib.\n\n\nThere is a Bayesian optimisation gold-rush, and everyone is selling shovels. Bayesian optimisation is core to the concept of self-driving labs, automated drug-discovery, and many other modern research topics. It’s well motivated, and a very useful tool. Start ups are being founded and pharmaceutical and chemicals companies are curating teams that focus fundamentally on Bayesian optimisation in one way or another. As a result, there is now a growing number of libraries, ranging from black-box APIs and no-code solutions to statistically accurate fine-grained representations; each becoming more bloated as new functionalities or usecases are appended.\nWith all of these options, each with their own special definition of a dataset, or a kernel, or an acquisition_function, or otherwise, it becomes difficult to see the wood for the trees. Someone in my group recently asked me if it was reasonably possible to implement Bayesian optimisation, and I more often than not get asked what library or package I use.\nIt is obviously important that BO tools become widely available, particularly to those that don’t know how to code, or don’t have the time such as chemists or industrial engineers. But the trade-off is that students, and practitioners are potentially using something that they don’t understand, and investors may be hoodwinked into investing in something that seems more complex than it is.\nMy mission in this post is to demonstrate that, whilst Bayesian optimisation entails a lot of moving parts, implementing it from scratch really shouldn’t take too long.\nTo make my point, I am going to code everything required for Bayesian optimisation, including visualisation… in under 100 lines of standard Python and Numpy. This will include a custom metaheuristic-based optimisation algorithm, Gaussian processes from scratch with trainable hyperparameters, an acquisition function, and an experimental design loop.\nFirst, I’ll give myself some groundrules:\n\nRules\n \n\nBayesian Optimisation that is competetive with established packages;\nPython Standard Library + Numpy only;\nSimple as possible;\nCode must be interpretable.\n\n\nTo begin with I’ll implement a Gaussian process from scratch.\n\nGaussian processes\nNow I’ll need a kernel function that defines how related two points are in input space. I’ll go for the Matern 5/2. I’m not choosing it because it’s differentiable1, but rather because I think samples from the prior will look most like functions I’m interested in.\n1 This is an attractive property of Gaussian processes that is ultimately distracting when optimising as a result of their nonconvexity, as I’ll cover later.def k(x1,x2,p): # matern 5/2 kernel\n    sigma,l = p[0],p[1] # hyperparameters\n    d = np.sqrt(np.dot(x1-x2,x1-x2))\n    return sigma**2 * (1 + np.sqrt(5)*d/l + 5*d**2/(3*l**2)) * np.exp(-np.sqrt(5)*d/l)\nSimple enough. Forget the hyperparameters now, I’ll cover those later. Now, I’ll give myself the ability to calculate a covariance matrix. I only need to calculate the kernel function for the upper triangular, then symmetrically set the corresponding lower triangular value to be the same2, saving me some time.\n2 Because all valid covariance matrices are positive semi-definite.def cm(x,p): # covariance matrix\n    n = x.shape[0]; K = np.zeros((n,n))\n    for i in range(n):\n        for j in range(i,n): # symmetric\n            K[i,j] = K[j,i] = k(x[i],x[j],p)\n    return K + 1e-6*np.ones(n)\nI’ve added some jitter to the diagonal just to help out when inverting the matrix in a bit. This is relatively common across all Gaussian process libraries. The final GP utility I’ll require is the negative log-likelihood, which will be used to assess hyperparameter performance.\ndef nll(p, x, y): # negative log likelihood\n    K = cm(x, p) # calculate covariance matrix\n    return -0.5 * (np.log(la.det(K)) + (y - p[2]).T @ la.solve(K, y - p[2]) + len(x) * np.log(2 * np.pi))\nNow everything needed has been defined, I’ll create my Gaussian process class. I will simplify the training procedure by making it train on instanciation. The only other thing that is needed is the ability to call it, returning the mean and variance of the posterior distribution at x.\nclass GP: # Gaussian process class\n    def __init__(self,x,y):\n        self.x = x; self.y = y\n        # optimize hyperparameters (sigma, l, constant mean)\n        self.p = tiny_opt(partial(nll, x=x, y=y), np.array([[0.1,10.0],[0.1,10.0],[np.min(y),np.max(y)]]))\n        self.K_inv = la.inv(cm(x,self.p)) # precompute inverse covariance matrix\n\n    def __call__(self,x): \n        k_row = np.array([[k(x,xi,self.p) for xi in self.x]]) # compute row of covariance matrix\n        dot_prod = np.dot(k_row,self.K_inv) # precompute dot product\n        return self.p[2] + dot_prod.dot(self.y-self.p[2]).item(), \\\n            k(x,x,self.p) - np.dot(dot_prod,k_row.T).item()\nIt is defined by data x and y (both row-wise Numpy arrays). I have a mysterious function called tiny_opt which for now provides us with optimal hyperparameters with respect to the negative log-likelihood. This ‘training’ step, and the precompution of the inverse of the covariance matrix3 defines a ‘trained’ Gaussian process. I’ve never really liked that terminology, borrowed from neural network training4. One of these parameters is a constant that defines my mean function. As a result of this, combined with the kernel lengthscale parameter, there is now no need to deal with data normalisation. I’ve previously covered noisy Bayesian optimisation, but as I won’t be dealing with noisy observations here, my GP is noiseless.\n3 Resulting in the precision matrix.4 Though when creating sparse Gaussian processes, the specification of hyperparameters does look more akin to training neural network parameters.Next up, I’ll outline what is actually happening in my tiny_opt function. This will also eventually be used to optimise my acquisition function as well so it’s worth getting right.\n\n\nMetaheuristic Optimisation\nAs I previously mentioned, I see the ability to easily calculate the gradient of a Gaussian process largely as a distraction. I am relatively confident that most people who have any significant experience optimising Gaussian processes will agree. In addition, BO can potentially be made unusually inefficient when using a gradient-based optimiser due to flat regions within an acqusition function, leading to functions such as logEI being proposed.\nGaussian processes, or resulting functions that apply them5 are generally nonconvex. Whilst local minima can be overcome, it’s clear that even in one dimension, there are often as many local optima as data points.\n5 Such as acquisition/utility functions (spoiler).Gaussian processes are practically free to evaluate in the grand scheme of experimental design, and most practical problems are interval-constrained. Combined with the fact that they can of course be evaluted in parallel, motivates the use of evaluation hungry but effective metaheuristics.\nI myself have found them very effective in optimising acqusition functions, and was pleased to see Google’s internal GP optimisation algorithm taking a similar view when they recently released details.\nMy algorithm is self-documented but I will describe it below. I’ve found it to be simple and effective. I wouldn’t even describe it as a metaheuristic. Actually, I will.\nEvery time you refresh this page my algorithm will be named after a different completely arbitrary animal!\n\nanimals = [\n    \"Lion\", \"Elephant\", \"Giraffe\", \"Zebra\", \"Penguin\",\n    \"Kangaroo\", \"Koala\", \"Panda\", \"Tiger\", \"Dolphin\",\n    \"Octopus\", \"Flamingo\", \"Cheetah\", \"Gorilla\", \"Sloth\",\n    \"Platypus\", \"Rhinoceros\", \"Crocodile\", \"Owl\", \"Chameleon\",\n    \"Toucan\", \"Meerkat\", \"Hedgehog\", \"Jellyfish\", \"Lemur\"\n]\n\ndescriptive_words = [\n    \"Swarming\", \"Running\", \"Leaping\", \"Crawling\", \"Swimming\",\n    \"Flying\", \"Pouncing\", \"Slithering\", \"Galloping\", \"Diving\",\n    \"Soaring\", \"Burrowing\", \"Climbing\", \"Gliding\", \"Hopping\",\n    \"Sprinting\", \"Meandering\", \"Prowling\", \"Stalking\", \"Charging\",\n    \"Darting\", \"Scurrying\", \"Lumbering\", \"Prancing\", \"Scampering\",\n    \"Waddling\", \"Zigzagging\", \"Fluttering\", \"Paddling\", \"Swooping\",\n    \"Lunging\", \"Grazing\", \"Perching\", \"Hovering\", \"Circling\",\n    \"Nesting\", \"Foraging\", \"Hunting\", \"Bounding\", \"Swinging\",\n    \"Lounging\", \"Basking\", \"Roosting\", \"Migrating\", \"Hibernating\",\n    \"Camouflaging\", \"Molting\", \"Preening\", \"Frolicking\", \"Stampeding\"\n]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing the…\n\n\n  Algorithm!\ndef tiny_opt(f,b): # small metaheuristic optimization\n    c = np.mean(b, axis=1); b_og = b.copy()\n    b_f = -np.inf; count = 0; w = 1 # track best solution, count stagnation, and shrink factor\n    while True:\n        for i in range(len(b)): # shrink search space around current centre by w\n            r = (b[i,1] - b[i,0]) * w\n            b[i] = [c[i] - r / 2, c[i] + r / 2]\n        # generate random solutions within search space\n        x = np.clip(ra.rand(1000,len(b))*(b[:,1]-b[:,0])+b[:,0], b_og[:,0], b_og[:,1])\n        with Pool() as pool: # evaluate solutions in parallel\n            y = pool.map(f, x)\n        sol = x[np.argmax(y)] # best solution\n        if f(sol) &gt; b_f: # if improvement, update best solution\n            b_f = f(sol); c = sol; count = 0; w = 0.75 # shrink search space next time\n        else: # if no improvement, don't shrink search space next time (set w = 1)\n            count += 1; w = 1 # increment stagnation counter\n        if (b_og[0,1]-b_og[0,0])/(b[0,0]-b[0,1]) &lt; 0.25 or count == 3:\n            # if search space has shrunk below a threshold or not shrunk for 3 iterations\n            return c # return best solution!\nSimply, I sample solutions within the bounds and evaluate them in parallel. If I sample the best overall solution I store this, and shrink the bounds around this point. I then re-sample and repeat. If I don’t find a better solution I don’t shrink the bounds. If I don’t find a better solution for three iterations I terminate. I also terminate if the bounds shrink below a certain threshold. These two properties ensure that the algorithm terminates eventually6. It is effectively a global trust-region method.\n6 EmpiricallyHere’s a little visualisation that I’ve drawn.\n\nWith the ability to optimise functions, all I need is an acquisition function, which is simple enough… and a loop!\nThis is just the expected improvement of a Gaussian process, implemented by importing NormalDist from the statistics standard library.\ndef ei(gp,x): # expected improvement\n    mu, var = gp(x); N = NormalDist()\n    z = (mu - np.max(gp.y)) / np.sqrt(var)\n    return np.sqrt(var) * (z * N.cdf(z) + N.pdf(z))\nAnd here is my loop…\nbounds = np.array([[-10.0,10.0]]) \nwhile True:\n    gp = GP(x, y) # build GP\n    x_opt = tiny_opt(partial(ei,gp),bounds); y_opt = ei(gp,x_opt) # optimise aq\n    x = np.vstack([x,x_opt]) # update x\n    y = np.vstack([y,np.array([f(x_opt[0])])]) # evaluate and update y\n… that’s it!\nBut to really prove that this works, I’m going to even choose an interesting test function to use, and implement the ability to visualise 1D functions7.\n7 Note that all of the above works for any dimensional function.\nPlotting\nTo plot my package to see it it is working, I’ll need a test function. One of my personal favourite ways of doing this is to sample one from a GP prior:\nx_gp = np.linspace(bounds[0,0],bounds[0,1],100).reshape(-1, 1)\nK = cm(x_gp, [1.0,1.0,0.0])\ny_gp = ra.multivariate_normal(np.zeros(len(x_gp)), K).reshape(-1, 1)\nf_gp = GP(x_gp,y_gp); f = lambda x: f_gp(x)[0] # this now acts as my 'true' function\nAn easy way of doing this is by drawing from a multivariate distribution with given covariance matrix and training a GP using this data. This provides a continuous, callable random function without dealing with things like pseudorandom number generation.\nI’ve written before about plotting practices (here and here), so I won’t go into too much detail. Here is the code that evaluates the GP and acqusition function and plots them both.\nIn addition, I save each plot and produce a GIF at the end.\n    x = np.linspace(bounds[0,0],bounds[0,1],3).reshape(-1, 1)  # initial training data\n    y = np.array([f(xi) for xi in x]).reshape(-1, 1) # initial training data\n    x_test = np.array([xi for xi in np.linspace(bounds[0,0],bounds[0,1],200)]).reshape(-1, 1) # test data\n    images = []\n    for _ in range(16):\n        gp = GP(x, y) # create GP\n        x_opt = tiny_opt(partial(ei,gp),bounds); y_opt = ei(gp,x_opt)\n        mu, var = np.array([gp(xi) for xi in x_test]).T; std = np.sqrt(var)\n\n        fig,ax = plt.subplots(2,1,figsize=(8,3),sharex=True,constrained_layout=True)\n        ax[1].set_xlabel('$x$'); ax[0].set_ylabel('$f(x)$'); ax[1].set_ylabel('EI(x)')\n        ax[0].scatter(x,y,c='k',marker='x')\n        ax[0].plot(x_test, mu,c='k'); ax[0].plot(x_gp, y_gp,c='k',ls='--',alpha=0.5,label='True Function')\n        ax[0].fill_between(x_test.flatten(), mu-std, mu+std, alpha=0.25,lw=0,color='k')\n        ax[1].plot(x_test, [ei(gp,xi) for xi in x_test],c='k',label='Expected Improvement')\n        ax[1].scatter(x_opt,y_opt,c='k',marker='o')\n        for a in ax: \n            a.spines['top'].set_visible(False); a.spines['right'].set_visible(False)\n            a.set_yticks([])\n            a.legend(loc='upper center',bbox_to_anchor=(0.5,1.1),frameon=False,ncol=1)\n\n        filename = f'{_:03d}.png'\n        plt.savefig(filename,dpi=300); plt.close()\n        images.append(Image.open(filename))\n\n        x = np.vstack([x,x_opt]) # update x \n        y = np.vstack([y,np.array([f(x_opt[0])])]) # evaluate and update y\n    \n    images[0].save('bo.gif',save_all=True,append_images=images[1:],duration=500,loop=0)\nThe full code is here, approximately 20% of the lines are for plotting!\n\n\nClick to view all code.\n\nimport numpy as np; import numpy.linalg as la; import numpy.random as ra\nfrom multiprocessing import Pool; from statistics import NormalDist\nfrom functools import partial; import matplotlib.pyplot as plt; from PIL import Image\n\ndef k(x1,x2,p): # squared exponential kernel\n    sigma,l = p[0],p[1] # hyperparameters\n    d = np.sqrt(np.dot(x1-x2,x1-x2))\n    return sigma**2 * (1 + np.sqrt(5)*d/l + 5*d**2/(3*l**2)) * np.exp(-np.sqrt(5)*d/l)\n\ndef cm(x,p): # covariance matrix\n    n = x.shape[0]; K = np.zeros((n,n))\n    for i in range(n):\n        for j in range(i,n): # symmetric\n            K[i,j] = K[j,i] = k(x[i],x[j],p)\n    return K + 1e-3*np.eye(n)\n\ndef nll(p, x, y): # negative log likelihood\n    K = cm(x, p) # calculate covariance matrix\n    return -0.5 * (np.log(la.det(K)+1e-6) + (y - p[2]).T @ la.solve(K, y - p[2]) + len(x) * np.log(2 * np.pi))\n\ndef tiny_opt(f,b): # small metaheuristic optimization\n    c = np.mean(b, axis=1); b_og = b.copy()\n    b_f = -np.inf; count = 0; w = 1 # track best solution, count stagnation, and shrink factor\n    while True:\n        for i in range(len(b)): # shrink search space around current centre by w\n            r = (b[i,1] - b[i,0]) * w\n            b[i] = [c[i] - r / 2, c[i] + r / 2]\n        # generate random solutions within search space\n        x = np.clip(ra.rand(10,len(b))*(b[:,1]-b[:,0])+b[:,0], b_og[:,0], b_og[:,1])\n        with Pool() as pool: # evaluate solutions in parallel\n            y = pool.map(f, x)\n        sol = x[np.argmax(y)] # best solution\n        if f(sol) &gt; b_f: # if improvement, update best solution\n            b_f = f(sol); c = sol; count = 0; w = 0.75 # shrink search space next time\n        else: # if no improvement, don't shrink search space next time (set w = 1)\n            count += 1; w = 1 # increment stagnation counter\n        if (b_og[0,1]-b_og[0,0])/(b[0,0]-b[0,1]) &lt; 0.25 or count == 3:\n            print('Optimised!')\n            # if search space has shrunk below a threshold or not shrunk for 3 iterations\n            return c # return best solution!\n\ndef ei(gp,x): # expected improvement\n    mu, var = gp(x); N = NormalDist()\n    z = (mu - np.max(gp.y)) / np.sqrt(var)\n    return np.sqrt(var) * (z * N.cdf(z) + N.pdf(z))\n\nclass GP: # Gaussian process class\n    def __init__(self,x,y):\n        self.x = x; self.y = y\n        # optimize hyperparameters (sigma, l, constant mean)\n        self.p = tiny_opt(partial(nll, x=x, y=y), np.array([[0.1,10.0],[0.1,10.0],[np.min(y),np.max(y)]]))\n        self.K_inv = la.inv(cm(x,self.p)) # precompute inverse covariance matrix\n\n    def __call__(self,x): \n        k_row = np.array([[k(x,xi,self.p) for xi in self.x]]) # compute row of covariance matrix\n        dot_prod = np.dot(k_row,self.K_inv) # precompute dot product\n        return self.p[2] + dot_prod.dot(self.y-self.p[2]).item(), \\\n            k(x,x,self.p) - np.dot(dot_prod,k_row.T).item()\n\n\nif __name__ == '__main__':\n    bounds = np.array([[-10.0,10.0]]) \n    # sampling from prior of a GP to define test function \n    x_gp = np.linspace(bounds[0,0],bounds[0,1],100).reshape(-1, 1)\n    K = cm(x_gp, [1.0,1.0,0.0])\n    y_gp = ra.multivariate_normal(np.zeros(len(x_gp)), K).reshape(-1, 1)\n    f_gp = GP(x_gp,y_gp); f = lambda x: f_gp(x)[0] # this now acts as my 'true' function\n\n    x = np.linspace(bounds[0,0],bounds[0,1],3).reshape(-1, 1)  # initial training data\n    y = np.array([f(xi) for xi in x]).reshape(-1, 1) # initial training data\n    x_test = np.array([xi for xi in np.linspace(bounds[0,0],bounds[0,1],200)]).reshape(-1, 1) # test data\n    images = []\n    for _ in range(16):\n        gp = GP(x, y) # create GP\n        x_opt = tiny_opt(partial(ei,gp),bounds); y_opt = ei(gp,x_opt)\n        mu, var = np.array([gp(xi) for xi in x_test]).T; std = np.sqrt(var)\n\n        fig,ax = plt.subplots(2,1,figsize=(8,3),sharex=True,constrained_layout=True)\n        ax[1].set_xlabel('$x$'); ax[0].set_ylabel('$f(x)$'); ax[1].set_ylabel('EI(x)')\n        ax[0].scatter(x,y,c='k',marker='x')\n        ax[0].plot(x_test, mu,c='k'); ax[0].plot(x_gp, y_gp,c='k',ls='--',alpha=0.5,label='True Function')\n        ax[0].fill_between(x_test.flatten(), mu-std, mu+std, alpha=0.25,lw=0,color='k')\n        ax[1].plot(x_test, [ei(gp,xi) for xi in x_test],c='k',label='Expected Improvement')\n        ax[1].scatter(x_opt,y_opt,c='k',marker='o')\n        for a in ax: \n            a.spines['top'].set_visible(False); a.spines['right'].set_visible(False)\n            a.set_yticks([])\n            a.legend(loc='upper center',bbox_to_anchor=(0.5,1.1),frameon=False,ncol=1)\n\n        filename = f'{_:03d}.png'\n        plt.savefig(filename,dpi=300); plt.close()\n        images.append(Image.open(filename))\n\n        x = np.vstack([x,x_opt]) # update x \n        y = np.vstack([y,np.array([f(x_opt[0])])]) # evaluate and update y\n    \n    images[0].save('bo.gif',save_all=True,append_images=images[1:],duration=500,loop=0)\n\n97 lines, not bad!\n\n\n\nLessons\nThese things are, on the surface not that complicated. That is what makes BO as a tool so powerful. There are always a host of interesting domain-specific problems to be solved, but by demystifying BO as a whole it becomes easier to make these adjustments or developments.\nI personally think that BO software has become overly cumbersome, but there are many moving parts, and it never quite turns out to be the one-size-fits all solution it is billed as. I don’t think there’s a long term solution or fix to this at the moment, I would only say to try it for yourself!"
  },
  {
    "objectID": "posts/gromit/index.html",
    "href": "posts/gromit/index.html",
    "title": "On Gromit & Wallace",
    "section": "",
    "text": "I have not been given access to a preview screening of the upcoming Wallace and Gromit feature A Vengeance Most Fowl. However, I can still tell you what will happen.\nWallace will act in his own short-sighted interests. He will avoid conflict at any cost, to the detriment of his long suffering slave dog Gromit. Useful idiot Wallace will then be exploited by forces very much within his control and Gromit will in turn, for some reason, save the day. Having experienced a range of emotions that Wallace could not even consider yet alone acknowledge, Gromit will then sit down with his newspaper, and return to the way things were. Largely being offered cheese.\nTo live the life of Gromit is to be moulded, to be morphed, both by Wallace and his divine Creators, and, lacking a means of communication, be denied the ability to return the favour. Voiceless Gromit is a bottomless vessel of emotional tolerance. He knows he has experienced pain before, and knows he will experience it again. Often confronting the viewer directly, he is quiet sign of resistance against all that serve to shape him.\n\n\n\nThe Plight of Plasticine Gromit is one that is destined to be told and retold for years to come. Gromit, and to a lesser extent Wallace, has transcended light entertainment and entered the realm of the folk-tale, there is no doubt that he now exists as part of a contemporary mythology. The retellings have begun; Wallace’s original orator Peter Sallis is dead, and now Ben Whitehead has taken up the mantle. Implicitly, these characters are destined to live on beyond even Nick Park’s passing. We are custodians of these models and bear their weight of their stories, it is our responsibility to ensure they are told ad infinitum. We shape them, and they mould us.\nJust as the Gods in epics such as The Iliad occasionally appear in physical form on the battlefield to remind their subjects who is pulling the strings, the fingerprints of Aardman employees linger as a reminder of an omniscient puppet master. However, despite playing a God-like role in the lives of Gromit and Wallace, we are no longer in control.\nGreek God Apollo granted human priestess Cassandra the gift of prophecy, but upon rejecting his advances - cursed her, ensuring that Cassandra’s prophecies were forever true but never believed. We as creator have bestowed upon Gromit emotional maturity and an understanding of the human condition, he speaks the truth in all of us. Just as Cassandra’s truth was destined to be spoken but never heeded by other humans, Gromit’s silent wisdom is destined to be witnessed but never fully grasped by slave master Wallace. Like Apollo’s gift, Gromit’s message has transcended beyond our control. Apollo, the God who could see all futures, could not change Cassandra’s use of his gift; we, who can mould Gromit’s form, cannot control our depiction of him. If Gromit could speak he would probably say something like:\n\nNick Park, Nick Park!\nGod of all ways, but only Death’s to me,\nOnce and again, O thou, Destroyer named,\nThou hast destroyed me, thou, my love of old!\n\n\n\n\nGiven this, we often find ourselves coming to expect that Gromit will strike a more introspective tone than other figures in Gromit and Wallace. As an entity his personal mythology is more emblematic to Sisyphus and his eternal punishment of rolling a boulder up a hill as opposed to heroic Hercules triumphing over rogue bakers, or a pair of mechanical trousers piloted by a penguin.\nHowever, in this contemporary myth of stoicism, Gromit does not play the role of Sisyphus. Gromit is the boulder. We as custodians bear the weight of bringing him into existence, we are doomed to tell the tragedy of Gromit, to raise him up, and to pull him down. Gromit and I will never summit, and we are burdened with that responsibility. In this analogy, Wallace serves as the mountain itself, static, unwavering in complacency. Wallace provides no respite for us, nor Gromit. We are doomed to force Gromit to try and try again, and yet go nowhere at all. Laika was given the tragic reward of burning up into a preconceived infinite lightness, eternal fame and glory. We rewarded Gromit’s trip into space with some cheese (which he doesn’t seem to personally enjoy that much).\n\n\n\n“Here I sit, I can do no other” once proclaimed Gromit, or so it goes. This puritanical stoicism in the face of eternal damnation that he shows is evidence that Gromit is the main character. Gromit and Wallace. Though, we must also consider the role that Wallace plays within this myth. He is a glutton at best. Motivated by cheese, and famously in-bread. He is meek and self-serving, a pushover. Dante reserved the vestibule of hell to these people with no ambitions, good nor bad. Abandon all hope, ye who enter here. They were destined for eternal stagnation. There are those characters in the mythology that do have ulterior motives, i.e. the penguin, but Wallace is unbefitting of even a negative analysis. He is nothing. He exists purely as blank canvas on which Gromit can project both his pleasure and pain.\n\n\n\nHow can Gromit transcend beyond his invented suffering, if we have lost control of the strings? How will this folk-tale evolve? One scenario is for Gromit to be considered, in the future, to be assumed to be based on a real dog. Hopefully in that case this very blog will be lost and cease to serve as a reminder that he’s completely made up. The weight of our burden as creators and custodians would be transformed into historical documentation.\nPapers will be written and debates will be held over “The Historical Gromit” - analysing evidence for his actual existence, debating the accuracy of various tellings, arguing over the true location of his home. The fingerprints in the clay would be studied not as signs of divine creation but as proof of historical authenticity. Academics would argue over whether “A Grand Day Out” represents an actual journey to a lost moon made of cheese, or is merely allegorical.\nThe ultimate victory over Gromit’s current metaphysical Sisyphean existence will not be in being pushed to the top of the mountain by us humans, but in future generations assuming we were never pushing a boulder at all - merely documenting a profound historical truth about a dog who carried the weight of human wisdom in silence.\nThere is a cruel twist to this fate. Like Wallace, we will miss the point of the tale entirely - and yet like Gromit, we now must bear this misunderstanding with stoic grace. From Gromit to Wallace.\nCANIS VIVIT."
  },
  {
    "objectID": "posts/bat/index.html",
    "href": "posts/bat/index.html",
    "title": "‘No Bat Death’ as a Policy for Industrial Growth and National Security.",
    "section": "",
    "text": "No bat death is acceptable.\n\n\n\nRecently, Dan Tomlinson MP chose to highlight the investigations (here and here) into the enormous cost of the Great Bat Tunnel (GBatunnel) of 2024. Others have irked, however I personally consider the tunnel a unique feat of mammalian cooperation and will look back fondly at the moment that both bat and human met each other exactly in the middle, having started from their respective ends of the 1km section of track.\n\n\n\nMany commentators have chosen to use the bat tunnel as a case study. It is a single example (of 8,276 in the case of HS2) of how a tangled web of stakeholders can grind large infrastructure projects to a halt and increase costs, all while inflation ticks away in the background. We must also spare a thought for those bats who are also feeling the pinch in these uncertain times. The price of small pieces of fruit have skyrocketed, and of course bat-employers (presumably cave related, i.e. maintenance or otherwise) have been directed not to increase weekly bug allowances to avoid a bug-price spiral. Not only this, but us humans have selfishly decided to increase transport capacity between two of our largest cities, in an attempt to stimulate a generation of economic growth.\nIronically if I were fortunate enough to be a bat, well protected as they are, I would go on strike, avoiding any significant crossings or journeys in entirety making the batunnel useless (apparently the bats have disappeared from the batunnel stretch, presumably on strike as I outline). Regardless, the batunnel now exists, and I speak for all British taxpayers in wishing any future bats a safe and enjoyable crossing should they wish to fly over this particular bit of train track.\nHowever, Natural England have set a dangerous precedent. By taking a hardline policy in favour of bat-lives, to the tune of £100 million, they have exposed their hand.\nIn this post I will outline three proposals to exploit this no-bat-death precedent that will solve not only the planning system but also infrastructure, growth as well as issues of national security. The majority of these rely on obtaining a number of rare bats. I think this is a solvable issue (supposedly they’re not actually that rare) and I would probably ask a consultant to do this for me from the offset.\n\n\n\n\nEvery planning proposal or amendment must be accompanied by a murdered Bechstein’s bat.\nThis is a local solution to the bat tunnels dilemma specifically and more broadly the planning system. Natural England would be made to weigh up the cost of a single bat dying, and the resulting small amount of conscious blood on their hands, against the potential of multiple bats experiencing the long and drawn out suffering of being hit by a 200mph train, due to their inaction.\nGiven their policy that no bat must die, they will not be pleased either way. The resulting paradox will most likely force them to reconsider their position, or potentially suffer indefinite operational gridlock. Assuming that Natural England will bend to the prospect of bat martyrdom (animal martyrdom is something I have spoken about at length in a previous post), they must seriously consider whether a proposal is worthwhile submitting in the first place. This I think has potential for other animal related planning proposals (unless there are one or two animals left like a white rhino or something, in which case this policy is entirely destructive). For example, if you come to me wanting to build a tunnel for hedgehogs, I’ll only consider it if you run over a hedgehog to show me you’re serious. Otherwise, all those hedgehog lives will be on you.\n\n\nA Bechstein’s Bat is held in captivity at the centre of every site of National interest.\nFor many years defence hawks have advocated for an increased proportion of GDP to be spent on Defence & National Security. A simple proposal to ensure this money is allocated (the precedent of the bat tunnel demonstrates this) is to first introduce and then highlight the presence of Bechstein’s bats at these sites. Natural England will then, naturally, begin proceedings to protect these bats at any cost.\nThese bats could also be used in both a defensive and offensive manner. For example, if creatively enough located, ideally at the very centre of these sites or even underground, there will be significant enough impetus on Natural England to increase security and lobby for increased defence spending. No bat must die. Here the assumption is that Natural England will treat bats in captivity equally to those that exist in the wild. The alternative is to manually introduce Bechstein’s bats into the wild near these sites of strategic national interest. I would probably begin with scattering them throughout the coastline of the UK, with a few at the major ports just to be sure. The balance is how to do so in a way that they are still considered endangered. Defensively, the very existence of Natural England and their unflinching commitment to bat wellbeing will act as a strong deterrent to other countries, given their explicitly stated and previously enforced red lines.\nSimilarly, bats could be offensively transported alongside soldiers in the standing army to war zones across the world. Natural England would then presumably take it upon themselves to form an auxiliary corps responsible for protecting these bats at any cost in the face of enemy aggression. A downside of this strategy is that hostage-bats may be used in a similar manner by the enemy in order obtain certain demands, though I will assume that given their namesake, Natural England are only concerned with the welfare of Anglo-Saxon bats. In this case, a bat proxy-war may break out between the respective wildlife organisations of each country, hell-bent on saving their own bat’s lives at any cost. We must also consider the ideal that Natural England cares equally about Anglo-Saxon bats based abroad as it does those flying peacefully at home, with all the comforts of a standard batunnel. This further raises questions into Natural England’s policy regarding migratory birds which I will not get into.\n\n\nA Bechstein’s Bat at the Bank of England is fed a daily bug allowance inversely proportional to inflation.\nIt goes without saying that this bat must not die, at any cost. The economists at the Bank of England must carefully use the dials and levers available to them to ensure that inflation is kept low. While this is what they presumably get up to anyway, the additional presence of a Bechstein bat, and the resulting pressure from Natural England will ensure that they stick to their targets.\nThis policy raises the interesting potential scenario wherein Natural England themselves may be required to voluntarily free up their own funding, martyring themselves in the name of the Bank of England Bat. Employees and concerned campaigners will reach a Buddhist style Nirvana of bat well-being, free of their own cause. Maybe the real infrastructure was the bats we saved along the way."
  },
  {
    "objectID": "posts/so_solid/index.html",
    "href": "posts/so_solid/index.html",
    "title": "On So Solid Crew & Raymond Queneau",
    "section": "",
    "text": "Sonnets, Haikus, and Palindromes are all forms of literary constraints. It is by constraining the structure of what is linguistically possible through the application of rules, that words gain an outsized meaning. There is a sense of purpose and specificity in each word that results in richer parts, and an even greater whole.\nIn 1960/2001, a French/British group of largely men met in Normandy/Battersea to discuss the implications of literary constraints, and the potential for new literary structures to create meaning. The Oulipo movement/So Solid Crew pioneered constrained works such as Exercices de Style/21 Seconds where 99/10 versions of the same scene are recounted each in a different style/by a different MC.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Oulipo movement has always maintained strong ties to mathematics. Simple rules such as replacing or omitting letters can be extended to entire algorithms. Similarly, So Solid Crew themselves turned to maths, calculating the 21 allocated seconds for each MC by dividing their three and a half minute song by 10 MCs and rounding to the nearest integer. History doesn’t repeat itself, but it often rhymes.\nA popular critique of 21 Seconds is that “none of the crew use their time to say anything remotely insightful”. Quite obviously, the purpose of So Solid Crew’s lyrics are not to provide insight to the listener. Otherwise they would have featured someone like Martin Lewis. But alas they did not. They gave themselves 21 seconds each, and they simply spent their time discussing this decision.\n\nI got 21 seconds to pass the mic\nI got 21 seconds to say what I gotta say\nYou won’t like me anyway\nBut I won’t hesitate\n\nThis introspective discussion is what separates So Solid Crew from other proponents of potential literature and Oulipo. The Crew spend almost their entire time discussing the very structure that they themselves have imposed. In doing so, they have meticulously weaved an ouroboros of structure and meaning, both existing in unison, neither without the other.\n\n\n\nIn the style of Claude Lévi-Strauss, given this self-referential meta-structure that the Crew have imposed on themselves, we should in theory be able to invert the formula. In direct contrast there should exist music blissfully un-self-aware, unconstrained but meaningless.\nJacob Collier (b. 1994) is a virtuoso. He was born with the ability to play seemingly any instrument. As a result, he is entirely unconstrained in what he can sonically produce. He can command audiences to sing complex harmonics with just his hands and converse with Jazz artists through the medium of diminished chords, resolving just before the next unskippable ad kicks in.\nAnd yet, I have never met anyone that actually enjoys his music. It is directionless. The lack of any semblance of constraint or difficulty is Collier’s blessing and ultimate curse. He is at least aware of this, claiming asylum under the band of artists with ’ creative infinity syndrome’. I’m sure at one point So Solid Crew were in the same category. But in the face of this, Megaman, Lisa et. al, were unwavering in their commitment that less-is-more. Not only did they implement this philosophy to great effect, but they told their audience exactly what they were doing. The wizard’s curtain of creativity was being thrown away all together. Short-form video like TikTok is a close candidate for a new cultural structure that will forever seem normal. Maybe Jacob Collier could do us all a favour and restrict the length of his songs to 21 seconds.\n\n\n\nThe Oulipo movement was partially founded to discover new literary structures, to progress the search towards potential literature. However, the majority of these structures have not flown the cultural nest. But to Raymond Queneau and others that doesn’t seem to matter, and neither does it seem to matter to Megaman or Lisa. It was, and still is, the very act of subverting the rules of engagement that reminds people of the meaning in the words we read and the music we hear. So Solid Crew made this point in a more direct way than most, and for that alone, they are here to stay. Romeo Done."
  },
  {
    "objectID": "posts/writing/index.html",
    "href": "posts/writing/index.html",
    "title": "Values Redux",
    "section": "",
    "text": "I started this blog two years ago because I wanted to discover my own personal writing style, or figure out if I even liked writing in the first place. At the time I remember hearing1 that the only way to get better is to write more2.\n1 This could have been fully hallucinated but that doesn’t matter2 Something that I always used to say about getting better at darts during my stint as President of the University of Manchester Darts Society, again probably unfoundedThis blog has evolved alongside myself throughout the last few years. I originally wrote about anything: topics related to optimisation, machine learning, or just basic tutorials on plotting graphs. To maintain motivation, I discovered it was important to write about things that I found personally interesting and by extension would enjoy reading. This was my heuristic for the first 3-4 months, write what I would want to read.\nAs I would talk about what I wrote about to friends, we would discuss extensions, how the themes related to books we’d read, or items in the news. Eventually what I found interesting to read evolved and with it what I wrote about. It was a cycle of reflection that led to something that I no longer consider an exercise in writing, but a fully-fledged platform for self-expression. I believe this form of self-expression will be an increasingly important ability to demonstrate as people shun large easy-to-attach-to socially acceptable causes in favour of a more personal way of understanding others. In effect the ability to answer the question: what do you believe in?\nWhy is this important? After a year or so I realised that a lot of what I found interesting to read3 was centered around values. Either an explicit description of where values4 originate from5, or biographies of people who have embodied a set of values6. Looking back I can point to two discussions where this focus on values seemed to emerge. The first was when an academic and mentor at Imperial College encouraged me to think deeper about what I stand for. He had served in the British Army before becoming an academic and has one of the most impressive careers of anyone I met at Imperial. He spoke about joining the Army post-PhD and immediately being given a set of values that he has never lost. It doesn’t particularly matter what they are7, but it was the idea of standing for something, and demonstrating intention when planning and executing a mission8 with them in mind. It doesn’t just give you the confidence to succeed but demonstrates to others that you are serious. When someone like that tells you he doesn’t know what you stand for, you start to think a bit deeper about how to live your life.\n3 And by extension write about4 Western or otherwise5 Even if the majority of people do not appreciate this6 Whether or not you agree that they are ‘correct’.7 You can look these up easily8 Used both literally and figuratively9 Embodying the very values he writes about in a classic moveThe second experience was just after reading Dominion by Tom Holland, which I had bought at random from a charity shop in Paddington. I conincidentally ended up playing cricket at the same time as him every week at The Oval and after a few weeks built up the courage to tell him how I very much enjoyed his book, and it encouraged me to write and think about my own values more. He was of course extremely gracious9 and the whole experience brought it all much closer to home. I realised that you were allowed to discuss and think on a deeper level about why you do things. You can choose to believe in a correct way of living, and disagree with those whose beliefs are at odds with that. This might seem relatively obvious but I think it’s important not to take it for granted.\nIn light of the shift in focus of this blog and aligning with my slightly-invented situation that it is becoming/will become increasingly important to own the values you consider correct, I will use the remainder of this post to outline my values, what has influenced them, how they developed, and where I’ve written about them before10.\n10 Consciously or notI don’t think it’s fair to load what I will present below with a few definitions or short summaries. These beliefs are more easily defined by my experiences, books, and writing. It wouldn’t do them justice to summarise them in an easily digestible way. Values shouldn’t be easily digestible, that would render them light and meaningless. I think values should be heavy. You would, and should have to read the same books, experience the same emotions, go to the same places and have the same discussions in exactly the same order to fully understand them. But that’s life, and that’s how I think people should understand their own beliefs (and by extension appreciate that this is how others see their own beliefs).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI am unashamedly a Baillie Gifford11 fan. I’ve found it to be a relatively good heuristic for what to read. Plus if you read 24 books a year over 30 years you’ll only ever read about 700 books, which seems oddly small for a lifetime. Probably a small-ish sized room walls worth of books. If I see a shortlisted sticker then it gets weighted a little more strongly in my head.\n11 The prize not the investment manager12 I believe Anna Keay was on a podcast I listened to but this might be a hallucination.The Restless Republic appeared to me during my PhD12 when I was looking to read anything that wasn’t about chemical flow reactors or Bayesian optimisation. Something real and entertaining.\nIt recounts the years where Britain was a republic between the violent death of King Charles I, and General Monck’s march for restoration. Told through the lens of a few different individuals, in my opinion it highlights the role that individuals serve in building and destroying institutions and how people respond to these changes.\nI read Going to Church in Medieval England shortly after. Set in the 400 year period before the republic, to me it highlights how institutions form lasting societal impressions, that are generally under appreciated by the population. It breaks down every aspect of what it would have been like to attend church, and the developments that have stemmed as a result, or been lost13. Having lived in Putney (which was very much a part of historical London), I would walk down to the medieval St Mary’s church on the Thames to sit and read this book at the weekend. This church also ironically appears in The Restless Republic as the location of the Putney Debates, where parliamentarians planned what their post-Charles I republic would look like14. The Britain that this book leaves behind is one that feeds into The Restless Republic, not only temporally but also on a physical level through St Mary’s church. A Britain where institutions have been dismantled but their traditions broadly remain, giving rise to new beginnings. Read in series these two books highlight how values are built up and enforced by institutions, and what happens when the rug is pulled underneath them.\n13 For example timekeeping and clocks, first appearing on church towers to indicate when mass was, as opposed to the often inconvenient sun-based system, particularly in the winter.14 I think the book mentions this but really the ideas debated here eventually went on to form the basis of the American constitution. If Wandsworth Council had any sense you should be seeing huge amounts of American tourists here, the Wetherspoons next door would be full to the brim. Separately, I’m also waiting for the Thomas Cromwell vistor centre / 4D cinema experience.Pandaemonium 1660-1886 follows on from the Britain that is left behind in The Restless Republic. It charts the Industrial Revolution entirely through eyewitness accounts. Similarly, I came away with an appreciation for the individual’s responsibility and ability to drive change, and how this can occur in an often quiet and understated setting. This book represents the purest form experiencing this change, unaffected by stylistic choices, and is commonly said to more closely resemble a film or a montage of the industrial revolution as opposed to a retelling.\nGoing back I would read these three books in chronological order. The combined story they tell is one that begins with a nation of farmers and bishops, and through the individual, ends with Britain as an industrial and economic superpower with the world’s strongest democracy. They have provided me with what I consider a good overview of how Britain was built over the last 800 years, by people working quietly away, unburdened by what has been. Institutions are built by great people who don’t stop to consider if they could, and they are brought down in periods of boredom by those who don’t consider if they should. They highlight how change happens, and how humans naturally respond. These aren’t necessarily values in and of themselves, but to me capture the feeling and need to progress.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA common theme that I have experienced when thinking about scientific or creative progress are feedback loops or cycles. People naturally inspire each other, and I found myself experiencing this in a number of settings.\nThe most contemporary example and probably where I first saw this point made so explicitly was after I bought a copy of The Creative Act by Rick Rubin shortly after it was first published. There is a great section where he discusses a period where The Beatles and the Beach Boys both inspired each other to produce great art (Rubber Soul and Pet Sounds), neither of which may have existed without the other.\n\nWhen Brian Wilson first heard the Beatles’ Rubber Soul, his mind was blown. “If I ever do anything in my life, I’m going to make that good an album,” he thought at the time. He went on to explain, “I was so happy to hear it that I went and started writing ‘God Only Knows.’” Being made happy by someone else’s best work, and then letting it inspire you to rise to the occasion, is not competition. It’s collaboration. When Paul McCartney heard the resulting Beach Boys album, Pet Sounds, he too was blown away and reduced to tears, proclaiming “God Only Knows” was to his ears the best song ever written. Buoyed by the experience, the Beatles played Pet Sounds over and over while creating another masterpiece, Sgt. Pepper’s Lonely Hearts Club Band. “Without Pet Sounds, Sgt. Pepper never would have happened,” Beatles producer George Martin said. “Pepper was an attempt to equal Pet Sounds.” This creative back-and-forth wasn’t based on commercial competition, it was based on mutual love. And we are all the beneficiaries of this upward spiral toward magnificence. No system exists that can rank which work is most reflective of the maker. Great art is an invitation, calling to creators everywhere to strive for still higher and deeper levels.\n\nI then encountered a similar theme in preparation for writing my PhD, where I read a few studies on knowledge discovery and reasoning. Reasoning tools are commonly partitioned into either deductive or inductive-based approaches where knowledge either stems from observations (i.e. data-driven) or from first principles (i.e. physics-first). As machine learning has become more prevalant, particularly in science and engineering, there has been a great debate as to whether neural networks or data-driven models can fully replace first-principles simulations. In principle, these are the same debates that were held in thousands of years ago, just applied to a different context. Some choose to ignore the nuance in the debate and simply combine both paradigms (i.e. hybrid modelling) forgoing logical consistency in favour of short term improvement of predefined metrics. What I believe, and what I think is presented best by Thomas Kuhn, is that it is the tension between inductive and deductive reasoning that drives scientific discovery. I articulated this point in my post on Machine Learning and Discovery. In it I discuss Mikuláš of Kadaň and Kepler. The first who built an accurate astronomical clock based on incorrect observations, the second deriving the correct first principles for planetary motion, partly15 inspired by the astronomical clock. Neither existing without the other. Just as The Beatles and the Beach Boys formed their own cycle of expression.\n15 As the story goes16 Guess17 e.g. St Mary’s church, PutneyFinally, I read both 1599 and Super-Infinite at a very similar time of year. 1599 charts a year16 in the artistic and personal life of William Shakespeare, with an analysis of the cultural events surrounding the plays he produced this year (Julius Caesar, Henry IV etc.), while Super-Infinite presents a biography of the love poet and Dean of St Paul’s Cathedral John Donne, similarly told through his work. Like my encounter with Tom Holland, living in London (and being raised in an area near Stratford-upon-Avon) both books resonated with me because of the immediacy of the locations and the ability for the city to maintain a deep sense of history17. I regularly see and experience locations, streets and buildings from both books. Both Shakespeare and John Donne would have lived in the same London at the same time, with each book mentioning the other writer/poet as a contemporary character. While there’s no concrete evidence that they did meet, reading the books together paints a portrait of a London with literary and theatrical scene where people are constantly responding, reacting, and trying to improve on others. Shakespeare for example killing off beloved character Falstaff in a bid to demand more intellectual engagement from his audience, or John Donne introducing his friends and lovers to metaphysics. For me, these cycles of inspiration and expression are what drives progress. I consider it important to listen to, learn from, and improve on others.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn early 2024 I visited Istanbul and wrote about how people have historically imposed values on others, how people have approached encountering other values, and how this relates to cultural and religious alignment in LLMs. Istanbul seemed to deal with these layers well to me. In recent years the Hagia Sophia was converted from a secular museum to a mosque, though in my opinion this practically has meant not much and was done quite sensitively. A similar city to Istanbul in this sense is Jerusalem. While I’ve never been, Simon Sebag Montefiore’s book provided me with an overview for how values and beliefs are often more layered and complex than distinct and digestible, and that there are historically better ways of dealing with this complexity than conflict.\nI read Empire of Signs before and during a trip to Japan. I wrote about it in my post on Japan and creative LLMs, and in my post on cultural competition in AI. In it Barthes presents an idealistic vision of Japan that holds a mirror up to our own beliefs, generally concerned with the lack of ‘whole’ that we focus on in the West and an increased emphasis on the in-between-ness of society, food, the arts and culture, the zen. Reading this book, and visiting Japan simultaneously made me appreciate the opposing and equally valid ways of looking at the world, and centering (or not centering in a Barthes-esque view of Japan) your life around your beliefs. I felt like I came away not only understanding Japan more18 but understanding my own way of living. To me it is about having respect for others and understanding how other people think, particularly if they have different or opposing values. In reality, there is no real reason for people to have a globally consistent set of values, or want to live life in the same manner. Respecting this, I believe, benefits everyone. In A Perfect Spy, Magnus Pym is a fictional double agent, who no one19 doubts would have a different ideology to themselves. He eventually runs out of room but not before explaining to the cast of other characters precisely why he did what he did. To me it is about ideologies, motivations and what may happen if you take for granted the layers of beliefs that occur in the real world, and underestimate the very reasonable circumstances that might lead to them. It is one of the best books I’ve ever read.\n18 With the help of my former flatmate now living in Tokyo.19 Eventually they realise thisTogether, these books and experiences highlight to me that people do think differently to each other, sometimes aligning with a personal set of values, and sometimes in direct opposition. On one end of the spectrum, under appreciating this can put your own beliefs at risk, but on the other end it has the capacity to cause real harm.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally I’ll highlight two last books, a trip, and two posts. As I previously mentioned I found Tom Holland’s Dominion in a charity shop in 2021 when I neither knew about him, the book, or the Rest is History podcast. Having read it and met him, it really does change the way you see the world. Bruce Clark’s Athens accompanied me on a trip there where I made a point of reading the relevant passages at the proscribed locations. His description of what Athenians, the demos, would have felt like standing at the top of the Acropolis was very evocative. These people had values that they believed to be correct. You can sense the confidence that they had 2500 years later, moreso than you can in a city with a similarly layered history such as Istanbul. As Tom Holland describes, these ideas morphed into Christianity, through philosophy and gave birth to the Western world. You cannot visit the acropolis and not want to be associated with this confidence in some way20, and there is no real reason why you shouldn’t. Since experiencing Athens, and reading Dominion, I’ve considered the values that have resulted and that have defined the West, and consider them to be the correct ones.\n20 as Elgin took too seriouslyFinally, I’ll describe the two pieces that I have enjoyed writing the most. The first, an analysis of Wallace and Gromit from the perspective of Ancient Greek mythologies and an analysis of Ratatouille that frames the animated rat a false prophet. Both are self-indulgent, but both highlight how often fun it is to think a bit deeper about why we live life the way we do, to look around and appreciate those with similar and different beliefs in a lighthearted manner. This ability for self-reflection is a feature in itself that shouldn’t be taken for granted. One that can provide humility and confidence in equal measure.\nThat’s it for now. I think these are strong enough experiences for me to live and benefit from them, but I appreciate that people could and should change21. I hope the half-life of this post is about a decade. As I say at the beginning, I believe values should be heavy and hard to shift or sway. Maybe in ten years I’ll write another version, or a reflection, but for now this is me.\n\n\n21 As a belief, meta."
  }
]