[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a PhD student at Imperial College London & 2023 Enrichment student at the Alan Turing Institute. I have a background in Chemical Engineering and still enjoy teaching labs at Imperial College. Alongside my work in process systems engineering, I am affiliated with Winchester School of Art producing installations with the Tate on the intersection between AI and art. My interests include Bayesian optimisation, human-in-the-loop machine learning, cricket üèè, and darts üéØ.\n\nImperial College London - PhD in Process Systems Engineering | 2021 - Current\n\nPresident‚Äôs Scholarship\n\nUniversity of Cambridge - MPhil in Chemical Engineering & Biotechnology | 2020 - 2021\n\nJohn Dudding Masters Studentship\n\nUniversity of Manchester - BEng in Chemical Engineering | 2017 - 2020"
  },
  {
    "objectID": "posts/plots/index.html",
    "href": "posts/plots/index.html",
    "title": "Scatter Plots",
    "section": "",
    "text": "Here I will outline my personal preferences for best plotting practices for 2D scatter plots using Matplotlib. I will be initialising the plot as demonstrated in my previous post.\nTo start with I will simulate a thousand data points that may have come from a dimensionality reduction technique such as t-SNE.\n\n\n/var/folders/wb/035mswds69j2l_sw8yg6tdjw0000gr/T/ipykernel_66542/585749580.py:4: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n  X = np.random.multivariate_normal(mean,cov,n)\n\n\nThis is as simple and basic as a plot can be in Matplotlib.\n\nfig,ax = plt.subplots(1,1)\nax.scatter(X[:,0],X[:,1]);\n\n\n\n\n\n\n\n\nFirst of all we need to size and scale the plot appropriately.\nBy default, the aspect-ratio of the axis will not be equal. If the axis have similar units or meaning this can provide misleading interpretation of the data. We will fix this first.\n\nAlways set the aspect ratio to be equal if both quantities are comparable in meaning and magnitude using ax.set_aspect('equal')\n\n\n\n\n\n\n\n\n\n\nWe will now add appropriate labels, with a legible fontsize to our axis, using LaTeX where appropriate.\n\nax.set_xlabel('$x_1$',fontsize=20)\nax.set_ylabel('$x_2$',fontsize=20)\n\n\n\n\n\n\n\n\n\n\nWith so many data points, adding some opacity can help the reader interpret the density. Note that reducting the opacity affects the fill and the edge of each point differently.\nAt this point I will set the color to black, considering we have no separable classes (yet).\n\nAlways consider removing the edge of each scatter-point if decreasing the opacity to make the plot look cleaner.\n\n\nax.scatter(X[:,0],X[:,1],color='k',linewidth=0,alpha=0.2);\n\n\n\n\n\n\n\nWith edges\n\n\n\n\n\n\n\nWithout edges\n\n\n\n\n\nAlternatively, the marker size may be changed to a smaller value and the opacity set to 1. This provides a less clear image of the distribution of points, but a clearer view of the specific locations of data.\n\nax.scatter(X[:,0],X[:,1],color='k',s=3);\n\n\n\n\n\n\n\n\n\n\nDespite these plots containing the same data, they are visually different and tell different stories to the reader. As always‚Ä¶\n\nIt is important to consider what message you wish to convey, and how visual elements such as opacity, and marker size support this message.\n\nIf the specific location of data is critically important, the ‚Äò+‚Äô symbol may be used. When scaling up the size of this marker appropriately, make sure to reduce the linewidth to ensure that the symbol isn‚Äôt too thick.\n\nax.scatter(X[:,0],X[:,1],color='k',s=50,linewidth=0.75,marker='+');\n\n\n\n\n\n\n\nStandard Linewidth\n\n\n\n\n\n\n\nReduced Linewidth\n\n\n\n\n\nIt may be helpful in this situation to ensure the grid is turned on, but remember to ensure it is behind other elements by using ax.set_axisbelow(True).\n\nax.grid(alpha=0.5)\nax.set_axisbelow(True)\n\n\n\n\n\n\n\n\n\n\nWhat if we have different classes that we want to highlight? For now I will plot these clusters in the tab: colour pallette.\n\nfor i in range(6):\n    X = clusters[i]\n    ax.scatter(X[:,0],X[:,1],s=20,alpha=0.75,linewidth=0);\n\n\n\n\n\n\n\n\n\n\nHowever, we now need to label our clusters. Adding a legend inside the plot will make it cluttered.\nTherefore we can add the legend outside the plot itself, on the right, above or below.\nBecause there are a reasonable number of labels, I will place the legend just above the plot in a single row, ensuring that the rounded frame is turned off.\n\nax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15),\n          ncol=5,frameon=False)\n\n\n\n\n\n\n\n\n\n\nHow to make the plot accessible to people who have colour-blindness? We may use different markers‚Ä¶\n\n\n\n\n\n\n\n\n\nHowever clearly the plot becomes illegible. I think in the scenario where colours are not desired (or their use should be minimised) the best option is to use multiple subplots, and opacity as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhilst we have increased the overall number of plots, this is an accessible alternative to using colours, and follows Edward Tufte‚Äôs philosophy of Small Multiples."
  },
  {
    "objectID": "posts/noisy_bo/index.html",
    "href": "posts/noisy_bo/index.html",
    "title": "BAYESIAN OPTIMISATION WITH NOISY MEASUREMENTS",
    "section": "",
    "text": "I have reworked some notes from (Garnett 2023) regarding noisy-expected improvement. Please refer to the original text for a more detailed explanation.\n\nThe assumptions that hold for the expected improvement utility function do not hold when measurements have noise.\nWe really want to find the point where the signal is optimised (Jones, Schonlau, and Welch 1998).\nHow do you determine if a measurement is signal or noise?\n\nWe begin by specifying an underlying objective function, which we consider unknown, shown in Figure¬†1. We have access to observations which we assume contain normally distributed noise.\n\n\nDefinition of underlying function‚Ä¶\ndef f(x):\n  return 2.4*np.sin(2.8*x) - ((x-3.5*np.pi) ** 2)/4 +  3.8*np.cos(1.7*x) - (x**2)/16 + np.random.normal(0,2)\n\n\n\n\n\n\n\nFigure¬†1: The underlying noisy funtion to be maximised alongside example observations.\n\n\n\n\nWe first begin by motivating the usual expected-improvement criteria. We are looking to maximise the expected increase between the of the maximum of the mean of the resulting Gaussian process after making an observation at \\(x\\) and the maximum of the current Gaussian process over potential observations \\(y\\) which are Gaussian distributed as a result of our \\(\\mathcal{GP}\\) model. \\[\\alpha_{EI}(x;\\mathcal{D}) = {\\color{red}\\int} \\left[{\\color{green}\\max \\mu_{\\mathcal{D}'}} - {\\color{blue}\\mu^*}\\right]{\\color{red}\\mathcal{N}(y;\\mu,s^2)\\text{d}y} \\tag{1}\\]\nBy formulating the expression with respect to the improvement in the mean of the current and subsequent Gaussian process models, we are expressing our interest in optimising the signal and not the noise or values of specific observations.\n\nImportant: Given a hypothetical observation \\(y\\), the value of the mean of the resulting Gaussian process \\(\\mu_{\\mathcal{D}'}\\) at given set of potential locations \\(\\mathbf{x}'\\) is\n\\[ \\mu_{\\mathcal{D}'} =  \\mu_{\\mathcal{D}} + \\frac{K_\\mathcal{D}(\\mathbf{x}',x)}{s}\\frac{y-\\mu}{s},\\]\nwhere \\(\\mu\\) and \\(s^2\\) are the mean and standard deviation of the distribution of potential values \\(y\\) could take.\n\nWhen we express this distribution in terms of the standard normal distribution \\(z := \\mathcal{N}(0,1)\\), we have \\(y = \\mu + sz\\) and as a result\n\\[ \\mu_{\\mathcal{D}'} =  \\mu_{\\mathcal{D}}(\\mathbf{x}') + \\frac{K_\\mathcal{D}(\\mathbf{x}',x)}{s}z.\\]\nPutting this back into Equation¬†1, we now only have to take the expectation over the standard normal distribution resulting in the following.\n\\[\\alpha_{EI}(x,\\mathcal{D}) = \\int \\max_{\\mathbf{x}'} \\left(\\mu_{\\mathcal{D}}(\\mathbf{x}') + \\frac{K_\\mathcal{D}(\\mathbf{x}',x)}{s}z \\right)\\phi(z)\\;\\text{d}z - \\mu^* \\tag{2}\\]\nTo begin with we will sample some initial data:\n\n# our initial dataset\nx_data = np.random.uniform(0,3*np.pi,8)\ny_data = np.array([f(x_i) for x_i in x_data])\n\n\n\nGP model definition and training‚Ä¶\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n    \n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\ndef build_GP(x,y,its):\n  likelihood = gpytorch.likelihoods.GaussianLikelihood()\n  model = ExactGPModel(x, y, likelihood)\n\n  model.train()\n  likelihood.train()\n\n  optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n\n  mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\n  for i in range(its):\n      optimizer.zero_grad()\n      output = model(x)\n      loss = -mll(output, y)\n      loss.backward()\n      optimizer.step()\n  return model,likelihood\nmodel,likelihood = build_GP(x_data,y_data,2000)\nGP = {'model':model,'likelihood':likelihood}\n\n\nFigure¬†2 shows this data with an initial Gaussian process (importantly assuming in-exact observations).\n\n\n\n\n\nFigure¬†2: A Gaussian process fit to the initial dataset.\n\n\n\n\nNow we will naively construct Equation¬†2 using 40 samples from the standard normal distribution, and naively calculate an approximation to the inner maximisation by evaluating at 100 location throughout the space (which defines our vector \\(\\mathbf{x}' \\in\\mathbb{R}^{100}\\)) and returning the max value from these.\n\ndef noisy_EI(x,GP):\n  model = GP['model']; model.eval()\n  likelihood = GP['likelihood']; likelihood.eval()\n  N = torch.distributions.Normal(0,1)\n  predicted_output = likelihood(model(x))\n  mean = predicted_output.mean\n  var = predicted_output.variance\n  x_prime_vals = torch.linspace(0,3*np.pi,100).reshape(-1,1)    \n  mu_vals = likelihood(model(x_prime_vals)).mean\n  cov_vals = model.covar_module(x_prime_vals,x)\n  integral = 0 \n  samples = 2000\n  z_vals = N.sample((samples,1))[:,0]\n  for z in z_vals:\n    integral += torch.max(mu_vals + (cov_vals*z)[:,0]) \n  return integral / samples\n\nNow if we plot this function alongside the time taken to evaluate the acquisition function at 100 locations.\n\n\nNAIVE TIME TAKEN:  26.5347 s. AVERAGE TIME PER EVALUATION:  0.2653 s\n\n\n\n\n\n\n\n\n\n\nImportant: For a fixed set of ‚Äòimprovement locations‚Äô \\(\\mathbf{x}'\\), the resulting posterior mean at each location can be interpreted as a 1D line as a function of \\(z\\):\n\\[\\mu_{\\mathcal{D}'}(z|x') = \\mu_{\\mathcal{D}}(x') + \\frac{K_{\\mathcal{D}}(x',x)}{s}z \\quad \\forall x'\\in \\mathbf{x}' \\tag{3}\\]\nTherefore finding the inner maximum new posterior mean as a function of \\(z\\) can be translated to calculating the upper envelope of a set of lines. This upper envelope represents for a given \\(z\\), the maximum posterior mean across all of the locations in \\(\\mathbf{{x}'}\\). The main idea is to remove the inner \\(\\max\\) operator and replace it with something more tractable enabling analytical integration.\n\\[\\int \\max \\left[\\text{lines} (z)\\right] \\phi(z) \\;\\text{d}z \\rightarrow \\int \\text{upper envelope} (z) \\phi(z) \\;\\text{d}z \\rightarrow \\text{analytical solution}\\]\nWhich has an analytical solution for a piecewise linear upper envelope. To do so:\n\nn = 100\na = np.random.uniform(0,0.2,n)\nb = np.random.uniform(-10,10,n)\n\n\n\n\n\n\nWe will first get rid of any lines that are definitely dominated between -5 and 5 (the range of \\(z\\) values we are interested in). I‚Äôm not going to explain this too much but it‚Äôs quite easy to derive this condition.\n\nl_store = []; u_store = []\nzl = -5; zu = 5\nfor i in range(len(a)):\n    l = a[i]*zl + b[i]\n    u = a[i]*zu + b[i]\n    l_store.append(l)\n    u_store.append(u)\n\nL_i = np.argmax(l_store)\nU_i = np.argmax(u_store)\n\ndel_i = []\nfor i in range(len(a)):\n    if l_store[i] &lt; l_store[U_i] and u_store[i] &lt; u_store[L_i]:\n        del_i.append(i)\n\na = np.delete(a,del_i)\nb = np.delete(b,del_i)\n\nNow we will sort the lines by gradient\n\nsorted_indices = np.argsort(a)\na = a[sorted_indices]; \nb = b[sorted_indices]; \n\n\n\n\n\n\nThen initialise the set of dominated lines (a and b values respectively), respective intervals, and the current largest \\(z\\) on the envelope.\n\ndom_a = []\ndom_b = []\ninterval_store = []\nenvelope_z = -5\n\nFor each line in order of increasing gradient (here we start with the first line indexed at \\(j=0\\))\n\nj = 0 \n\nAdd the line to the set of dominating lines\n\ndom_a.append(a[j])\ndom_b.append(b[j])\n\nCalculate the \\(z\\) intercept of the line with all lines of a larger gradient. As we only calculate this intersection with these remaining lines, the overall algorithm has \\(\\mathcal{O}(n\\log n)\\) complexity.\n\nz_intercept_store = []\nfor i in range(j+1,n):\n    z_intercept = (dom_b[-1] - b[i])/(a[i] - dom_a[-1])\n    z_intercept_store.append(z_intercept)\n\n\n\n\n\n\n\n\n\n\nThe intercept with the lowest value of \\(z\\) will be a potential vertex for the envelope, but we must check that there are no lines above it.\nTo do so we calculate the \\(\\mu\\) value of each line at \\(min({\\mathbf{z})}\\).\n\nmu_vals = []\nz_intercept = np.min(z_intercept_store)\nmu_intercept = dom_a[-1]*z_intercept + dom_b[-1]\n\nfor i in range(j+1,n):\n    mu_vals.append(a[i]*z_intercept + b[i])\n\n\n\n\n\n\n\n\n\n\nIf the maximum value of \\(\\mu\\) is on the dominating line, and this value is larger than the current largest \\(z\\) value in the envelope‚Ä¶then we know this must lie on the upper envelope. Otherwise forget about this dominating line by setting it‚Äôs definition to None.\n\nif abs(mu_intercept-np.max(mu_vals)) &lt; 1e-5 and z_intercept &gt; envelope_z:\n  interval_store.append([envelope_z,z_intercept])\n  envelope_z = z_intercept\nelse:\n  dom_a[-1] = None\n  dom_b[-1] = None\n  interval_store.append([None,None])\n\nNow we can plot the interval we have just calculated.\n\n\n\n\n\n\n\n\n\nIf we now repeat this procedure starting with the line with the next largest gradient we can search for next vertex on the upper envelope.\n\n\nFull algorithm for calculating upper envelope of a set of lines‚Ä¶\ndef upper_env(a,b):\n\n  l_store = []; u_store = []\n  zl = -5; zu = 5\n  for i in range(len(a)):\n      l = a[i]*zl + b[i]\n      u = a[i]*zu + b[i]\n      l_store.append(l)\n      u_store.append(u)\n\n  L_i = np.argmax(l_store)\n  U_i = np.argmax(u_store)\n\n  del_i = []\n  for i in range(len(a)):\n      if l_store[i] &lt; l_store[U_i] and u_store[i] &lt; u_store[L_i]:\n          del_i.append(i)\n\n  a = np.delete(a,del_i)\n  b = np.delete(b,del_i)\n  n = len(a)\n\n  sorted_indices = np.argsort(a)\n  a = a[sorted_indices]\n  b = b[sorted_indices]\n\n  dom_a = []\n  dom_b = []\n  interval_store = []\n  envelope_z = -5\n\n  for j in range(n):\n    dom_a.append(a[j])\n    dom_b.append(b[j])\n\n    z_intercept_store = []\n    for i in range(j+1,n):\n        z_intercept = (dom_b[-1] - b[i])/(a[i] - dom_a[-1])\n        z_intercept_store.append(z_intercept)\n\n    mu_vals = []\n    try:\n      z_intercept = np.min(z_intercept_store)\n    except:\n      interval_store.append([envelope_z,5])\n      break \n    mu_intercept = dom_a[-1]*z_intercept + dom_b[-1]\n    for i in range(j+1,n):\n        mu_vals.append(a[i]*z_intercept + b[i])\n\n    if abs(mu_intercept-np.max(mu_vals)) &lt; 1e-9 and z_intercept &gt; envelope_z:\n      interval_store.append([envelope_z,z_intercept])\n      envelope_z = z_intercept\n    else:\n      dom_a[-1] = None\n      dom_b[-1] = None\n      interval_store.append([None,None])\n\n  del_store = []\n  for i in range(len(dom_a)):\n      if dom_a[i] == None:\n          del_store.append(i)\n\n  dom_a = np.delete(dom_a,del_store)\n  dom_b = np.delete(dom_b,del_store)\n  interval_store = np.delete(interval_store,del_store,axis=0)\n\n  zl = -5\n  zu = 5\n\n  interval_store[interval_store &gt; zu] = zu\n  interval_store[interval_store &lt; zl] = zl\n\n  return dom_a,dom_b,interval_store\n\n\n\n\n\n\n\n\n\n\n\nThe expected value over \\(z\\) (integrating over potential experimental outputs) of this upper envelope (which we use inplace of the inner \\(\\max\\) operator), where \\(z \\sim \\mathcal{N}(0,1)\\) is given by:\n\\[\n\\sum_i \\int^{c_{i+1}}_{c_{i}}(a_iz+b_i)\\phi(z) \\; \\text{d}z \\rightarrow \\sum_i b_i [\\Phi (c_{i+1})-\\Phi(c_i)] + a_i[\\phi(c_i) - \\phi(c_{i+1})]\\]\nwhere \\(\\Phi\\) is the standard cumulative normal probability function, \\(\\phi\\) is the standard normal probability function, \\(c\\) are the \\(z\\) values of the upper envelope intervals, and \\(a\\) and \\(b\\) are the gradient and intercept of upper envelope intervals corresponding to Equation¬†3.\nIntuitively, we are solving the expected improvement integral analytically for each line segement within the upper envelope of lines (which we use as it is equivalent to the inner \\(\\max\\) operator).\n\nFor the sake of completeness we will wrap this up into an acquisition function, and run a basic Bayesian optimisation loop\n\ndef noisy_EI_lines(x,GP):\n  with torch.no_grad(), gpytorch.settings.fast_pred_var():\n    model = GP['model']\n    likelihood = GP['likelihood']\n    model.eval()\n    likelihood.eval()\n    x_prime_vals = torch.linspace(0,3*np.pi,200)\n    predicted_output = likelihood(model(x))\n    mean = predicted_output.mean.item()\n    var = predicted_output.variance.item()\n    a = []\n    b = likelihood(model(x_prime_vals)).mean.numpy()\n    for x_prime in x_prime_vals.reshape(-1,1):\n      conc_x = torch.cat((x,x_prime))\n      covar = likelihood(model(conc_x)).covariance_matrix\n      a.append((covar[0,1]/var).item())\n    dom_a,dom_b,interval_store = upper_env(np.array(a),np.array(b))\n    N = torch.distributions.Normal(0,1)\n    sum = 0\n    for i in range(len(interval_store)):\n      c_i = torch.tensor(interval_store[i,0])\n      c_i1 = torch.tensor(interval_store[i,1])\n      sum += dom_b[i]*(N.cdf(c_i1) - N.cdf(c_i)) + dom_a[i]*(np.exp(N.log_prob(c_i)) - np.exp(N.log_prob(c_i1)))\n  return sum.item()\n\n\n\nNAIVE TIME TAKEN:  26.0427 s. AVERAGE TIME PER EVALUATION:  0.2604 s\nUPPER ENVELOPE TIME TAKEN:  4.6367 s. AVERAGE TIME PER EVALUATION:  0.0464 s\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nGarnett, Roman. 2023. ‚ÄúBayesian Optimization.‚Äù Cambridge University Press. https://bayesoptbook.com.\n\n\nJones, Donald R., Matthias Schonlau, and William J. Welch. 1998. Journal of Global Optimization 13 (4): 455‚Äì92. https://doi.org/10.1023/a:1008306431147."
  },
  {
    "objectID": "posts/convergence_plots/index.html",
    "href": "posts/convergence_plots/index.html",
    "title": "Convergence Plots",
    "section": "",
    "text": "Here I will outline my personal preferences for best convergence plotting practices using Matplotlib.\nLet‚Äôs start by plotting some convergence data in just about the worst way, and slowly improve on this. It is my view that when LaTeX is available you should always use it.\n\nAlways use the LaTeX extension to render figures by including plt.rcParams['text.usetex'] = True' at the top of your script.\n\nHere I have simulated some convergence data where we have 4 different benchmarks, each containing 16 separate repeat runs.\n\n\n\n\n\n\n\n\n\nUsing plt.figure() makes it harder to interact with the axis (even if you only have a single axis), making it more difficult to adjust the size and layout.\nNow we will initialise our plot correctly using the object-oriented syntax, sizing the figure appropriately for the given context. By constraining the layout we ensure that the figure fits within the given size.\n\nAlways initialise a plot using the object-oriented interface fig,ax = plt.subplots..., defining the figure size appropriately.\n\n\nNever initialise a plot using the pyplot interface plt.figure....\n\n\nfig,ax = plt.subplots(1,1,figsize=(7,3),layout=\"constrained\")\n\n\n\n\n\n\n\n\nNow we will draw our attention to plotting the data itself. It is important to think about what message you want to convey to the reader. Clearly providing each run for each benchmark is not useful. I will plot the mean and standard deviation of regret across each benchmark. I will use a line plot to convey the mean and a shaded region to convey the standard deviation.\n\nAlways ensure that the linewidth=... option is set appropriately, and more importantly consistently.\n\n\nfig,ax = plt.subplots(1,1,figsize=(7,3),layout=\"constrained\")\nfor b in range(benchmarks):\n    mean = np.mean(data[b,:,:],axis=0)\n    std = np.std(data[b,:,:],axis=0)\n    ax.plot(iterations,mean,color=cols[b])\n    ax.fill_between(iterations,mean-std,mean+std,alpha=0.2,color=cols[b])\nplt.show()\n\n\n\n\n\n\n\n\n\nAlways separate your data generation and plotting code. This will make it easier to adjust the plot later. Save data at generation (for example during optimisation) and then load it in for plotting.\n\nNow we have decided what specifically to plot and all the information we wish to present is here we can work on making the plot more interpretable.\nWe should first add some axis labels as follows. Ensure the font size is large enough to be legible. You can use LaTeX within labels by using $ $ as standard, when doing so include an r in front of the string to ensure it gets parsed appropriately.\n\nAlways choose a legible font size and text for axis labels, using LaTeX where appropriate.\n\n\nax.set_xlabel(r'Iteration, $\\tau$',fontsize=18)\nax.set_ylabel(r'Regret, $r_\\tau$',fontsize=18)\n\n\n\n\n\n\n\n\n\n\nNow we will focus on the ticks. Ticks are important because they enable the reader to interpret the values of the data.\nIt is important to consider what tick values are appropriate to display. For example in this case, I want to convey to the reader the differences in overall convergence characteristics between these methods. Therefore, it may not be necessary to display intermediate regret values.\n\nAlways select appropriate tick values for your context, adjusting the fontsize as necessary.\n\n\nx_ticks = [0,10,20,30,40,50,60,70,80,90,100]\ny_ticks = [0,0.5,1]\nax.set_xticks(x_ticks,x_ticks,fontsize = 18)\nax.set_yticks(y_ticks,y_ticks,fontsize = 18)\n\n\n\n\n\n\n\n\n\n\nWhilst it is important to provide a legend to ensure the reader can interpret the data, not everything has to have a label.\nIn this case I want to convey which color corresponds to which benchmark, but I also need to convey what the error bars mean. In order not to distract from the plot, I will choose to omit this from the plot itself and instead include it in the caption.\nIncluding a label for the error bars does not contribute to the story and the message of the plot.\n\nAlways include labels for the relevant information, such as distinguishing between benchmarks. If the plot is too crowded place the legend outside of the plot.\n\n\nNever overwhelm the reader with information and labels. Some information is fine to include within the caption, as the caption and the image are considered complimentary and dependent.\n\n\nlabels = [f'Benchmark {b}' for b in range(1,benchmarks+1)]\nfor b in range(benchmarks):\n    mean = np.mean(data[b,:,:],axis=0)\n    std = np.std(data[b,:,:],axis=0)\n    ax.plot(iterations,mean,color=cols[b],label=labels[b])\n    ax.fill_between(iterations,mean-std,mean+std,alpha=0.2,color=cols[b])\nax.legend(frameon=False,fontsize=14)\n\n\n\n\n\n\n\n\n\n\nYou will notice that I have chosen not to include the frame on the caption. This makes the plot look cleaner and less cluttered.\nFor scatter plots the frame can help distinguish between the data and the legend, but for line plots it may not be necessary.\nWhen using fill_between a faint outline is drawn which can make the plot look cluttered, I always like to remove this by setting linewidth=0.\n\nAlways remove the linewidth of fill_between for a cleaner plot.\n\nI always use and reccomend the tab: set of colours in Matplotlib. A number of people have put significant effort into choosing these so you don‚Äôt have to. Here is a list of examples that you can copy and paste.\nThese colours are softer than the default colours and are more aesthetically pleasing.\n\ncols = ['tab:blue','tab:orange','tab:green','tab:red',\\\n    'tab:purple','tab:brown','tab:pink','tab:gray','tab:olive','tab:cyan']\nfor b in range(benchmarks):\n    mean = np.mean(data[b,:,:],axis=0)\n    std = np.std(data[b,:,:],axis=0)\n    ax.plot(iterations,mean,color=cols[b],label=labels[b])\n    ax.fill_between(iterations,mean-std,mean+std,alpha=0.2,color=cols[b],linewidth=0)\n\n\nAlways use the tab: set of colours over the default colours.\n\n\n\n\n\n\n\n\n\n\nNow we can adjust the axis limits to ensure that the data is placed appropriately. In this case, the regret values are fine but the iterations should start at 0 and end at 100.\n\nax.set_xlim([0,100])\n\n\nAlways adjust the axis limits appropriately. The x-axis particularly may be too large.\n\n\n\n\n\n\n\n\n\n\nFinally, it is important to consider whether a grid is appropriate. In this case I will for demonstration, but it may be distracting. I will adjust the transparency to make it less pronounced.\n\nax.grid(alpha=0.4)\n\n\nAlways consider using ax.grid() if the individual values of the data are important.\n\n\n\n\n\n\n\n\n\n\nIf you choose not to use a grid, then I always prefer if the right and top axis are removed, resulting in a cleaner plot.\n\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\n\n\nAlways remove the right and top axis if you choose not to use a grid.\n\n\n\n\n\n\n\n\n\n\nThat is just about it for this plot. Always to remember to save figures as .svg or .pdf. If you require a .png then use the dpi= option in fig.savefig to increase the resolution.\n\nAlways save your figure as .svg or .pdf.\n\n\nfig.savefig('results.svg')\n\nFinally, read the documentation!."
  },
  {
    "objectID": "posts/values/index.html",
    "href": "posts/values/index.html",
    "title": "Generative Models as Mirrors and Idols; From Istanbul",
    "section": "",
    "text": "1600 years ago, as Emperor of Rome and patron of Constantinople where he is buried, Theodosius II oversaw the central theological questions in the wake of the Council of Nicea and doctrine of Trinity: did Christ have distinct human and divine aspects? Was the human form of Christ the same form as the rest of us? Alongside a university to aid in answering these questions, and the city walls, Theodosius commissioned the construction of a vast cistern to provide fresh water for the city‚Äôs inhabitants and palaces1.1¬†Pretentious opening but please indulge me because later on it goes downhill and I end up using the phrases ‚Äòworld-view‚Äô and ‚Äòcut-through‚Äô.\nIt is in a glass-bound spotlit coffee shop above this cistern that I write this post2, complete with visitors centre and free WiFi3. Below me, 32 nine-meter high marble columns support myself and others, our coffees, my bottle of water. We are mercilessly paying homage to Theodosious‚Äô original intentions to provide refreshment here - what once served a purpose continues to do so under a different context. Theodosius‚Äô grave is now lost, most likely beneath the Fatih Mosque where Islamic minorets now anchor the heavens to the city and vice versa. The Christian pasts of the Romans and Byzantines have coalesced with the more recent history of the Ottoman empire, places like this a constant reminder of the slow march of time which seems to have begun earlier and progressed faster here than in any other city.2¬†Not all of it obviously.3¬†Please pretend I‚Äôm still there for the entirety of the following, otherwise it doesn‚Äôt work.\n\n\n\n\n\nTheodosius Cistern, Taken Februrary 2024\n\n\nIstanbul as it is now, is a city with a complex relationship between Christianity, Islam, and the secular. The Hagia Sofia, a Christian church for 1000 years, an Islamic mosque for 500, and most recently a Museum for 85, is now a Mosque again. As Turkey undergoes broad desecularisation under Erdogan, it seems only appropriate that in a city that has seen so much religious change, I shamelessly engage in mapping the state and future of machine learning onto past and present religious divides (as I have previously done).\n\n\nThis post never quite recovers the highs of the introduction, so please feel free to stop here. Click here to return to homepage.\nWith the large amount of resources required to train state-of-the-art models, teams of researchers have courted the tech establishment in order to gain access to large amounts of computational power. The stakes are supposedly high; a well-trained model in language or image generation may serve as a powerful tool to flood social media with undetectable posts aligned with a given set of values, infiltrate computer networks, or aid in obtaining technological advancements or scientific discovery. It is therefore no surprise that alongside those with business interests at heart, there are nation-states wishing to develop state-of-the-art machine learning models.\nWith so many factors to take into account such as architecture, dataset, and fine-tuning method, the system boundaries when creating machine learning models are necessarily drawn wider than the model itself, encompassing the team, organisation, financial backers, and their values and interests.\n\n\n\nGoogle Gemini‚Äôs response, as of Feb 25 2024. Source\n\n\nPeople have recently blamed Google‚Äôs corporate culture for a series of questionable large-language model and image generation failures, generating enormous amounts of cut-through. One could argue that it was ultimately the Western Christian values of equality and respect imposed by those that designed the system that resulted in an overly biased model. In this case I would argue that Gemini was projecting a radically Christian world-view, demonstrating respect for those that unquestionably do not deserve it, and nuance in judgements that only the most stoic entity would consider remotely balanced.4 In this case, the model and its outputs are fundamentally tied to those that create it.4¬†The individual failure of the model can be put down to bad fine tuning, but the content emerges pre-fine-tuning and what is intended to emerge post-fine-tuning must be taken into account.\nAs traditionally Islamic nation states with large amounts of resource such as the UAE and Saudi Arabia seek to develop their own machine learning models, it is not a stretch to assume that machine learning models and Islamic values will inevitably cross paths as state actors seek to gain from the benefits of AI. With this in mind, how do Islamic values map onto the existential threat of AI? What would a language model imbued with Islamic values provide?\n\n\nSlight diversion follows\nWestern Christian values allow for a level of flexibility when confronted with the prospect of super-human machine intelligence. The act of bringing ‚Äòsomething‚Äô into existence is broadly acceptable within this framework (and sometimes encouraged). The questions that preoccupy most people with this worldview are ones of artificial-general-intelligence rights. Can a machine be concious? Is it morally acceptable to turn off a concious machine? How do we treat an AGI?\nHowever, from an Islamic perspective this initial act of creation strikes a more existential tone when the complimentary values of Tahwid5 and Shirk6 are considered. There are two key conflicting arguments concerning the pursuit of human-surpassing intellect.5¬†Tawhid is the indivisible unification concept of monotheism in Islam. Source6¬†To attribute divinity to anything or anyone else, is shirk ‚Äì an unpardonable sin according to the Qur‚Äôan.\nThe first perspective is that AGI may help to uncover the secrets of the universe, or provide new ways of interpreting the Qu‚Äôran. Through the use of artificial intelligence, one has become closer to God. This perspective is one that Arabic scholars under the Abbasid Caliphate took when they provided the groundwork for modern mathematics and the number system we use today. In contrast to this viewpoint, an overreliance on super-human machine intelligence (should this be achieved, this seems reasonable) may be interpreted as an idolatrous act of Shirk. Though future rulings may reasonably put this balance down to the data itself that a model is trained on7. There is a delicate balance between science, art and discovery, and religion that continues to be repeated throughout the Abrahamic religions and will be repeated as machine learning gains human-surpassing abilities.7¬†How should existing religions face the prospect of a model trained on other religious texts?\n\n\nDiversion ends.\nAny machine learning model produced by a nation-state with a given ideology may be deliberately or unconciously imbued with these values, just as models produced in the West enforce our set of values. As the uncompromising mirror of generative AI allows us to confront our worldview, we must not be surprised when opposing worldviews instanciate in generative models around the world.\nWhere I am in Istanbul8 is a physical reminder of the neverending race to maintain a set of values within a community, or enforce a set of values on those without them. As a city it has ultimately benefited, layers of history have resulted in a broadly interesting place to visit.98¬†Stated as fact assuming previous instructions were followed.9¬†Five word summary for the billboard.\nGenerative AI now provides a new battleground to enforce a value set in an a way altogether different from any that have come before. It is actually harder to not engage with this as Google has found out. From an organisational perspective, which dominates public-facing machine learning for now10, companies must ensure their values are well defined, clear and consistent11. If not they will emerge regardless, which might not be such a bad thing for the rest of us.10¬†As opposed to nation-states11¬†With all the authority of a final-year PhD student in a machine learning-adjacent field."
  },
  {
    "objectID": "posts/books_twenty_three/index.html",
    "href": "posts/books_twenty_three/index.html",
    "title": "2023 Books",
    "section": "",
    "text": "1000 Years of Joys and Sorrows - Ai Weiwei\nLucky Kunst - Gregor Muir\nThe Restless Republic - Anna Keay\nGazza Agonistes - Ian Hamilton (REVIEW)\nAthens - Bruce Clark\nOedipus at Kolonos - Sophocles\n1599 - James Shapiro\nSuper-Infinite - Katherine Rundell\nThe Escape Artist - Jonathan Freedland\nThe Books of Jacob - Olga Tokarczuk\nGoing to Church in Medieval England - Nicholas Orme\nDeath and the Penguin - Andrey Kurkov\nKilling Thatcher - Rory Carroll (REVIEW)\nPolitics on the Edge - Rory Stewart\nDrive your Plow over the Bones of the Dead - Olga Tokarczuk\nK-PUNK - Mark Fisher"
  },
  {
    "objectID": "posts/wsa_rotunda/index.html",
    "href": "posts/wsa_rotunda/index.html",
    "title": "Generative Lab Identities",
    "section": "",
    "text": "This post details a consulting/research project for the Winchester School of Art (WSA) digital Rotunda platform, alongside Studio 3015. If you have a similar project or concept, feel free to contact me at tom.savage(at)hotmail.com\n\n\nTask\nThe goal with this project is to enable existing research lab identities to change in response to specific media such as text and images. Through the use of generative AI, the identity for each lab becomes dynamic, changing based on contextual information.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefining the Designs\nTo achieve this I began by recreating these existing identities using the svgpathtools package, enabling me to change the forms via Python1.1¬†By doing so I not only have the ability to programmatically change the form of a logo, but I also obtain the SVG for ‚Äòfree‚Äô which can be used within WSA graphics.\nAspects of a given logo are then parameterised in the following ways:\n\nIndividual ‚Äòshapes‚Äô are allowed to grow and shrink, enabling either a more solid unified form, or a more distributed representation.\nIndividual ‚Äòshapes‚Äô are switched from circles to squares, enabling a more digital and less connected form.\n\n\n\n\nThese provide a representation of how physically distributed a project is, and how interdisciplinary a project is respectively. Through parameterisation, each lab‚Äôs identity is allowed to respond to inputs, but in a controlled and deliberate way, maintaining the overall form and identity.\n\n\nInferring Parameters\nTo infer the parameters representing the physical distribution, and interdisciplinary nature of a project we prompt an LLM with both a description and image.\nThe information flow is as follows:\n\nSome prompt engineering is performed to ensure that JSON is consistently returned2, and reliable explanations for why the parameters have been chosen. As part of this I include examples of lab descriptions and respective parameters within the prompt as context and to provide examples of outputs. When applying image data, the image is first described, and then in a separate call, the description applied to inform the logo.2¬†In tests successful 97-99% of the time, failing when the parameter explanation overflows.\nEach sentiment and subsequent parameter is prompted separately due to the low-cost and fast LLM inference3, to ensure that forgetting or context-shift doesn‚Äôt occur.3¬†See below.\n\n\nLLM Inference\nLLM inference is performed on Anthropic‚Äôs Claude 3 Haiku. I find it is extremely low cost, fast, and it‚Äôs performance is enough to infer the sentiment of the project descriptions. Despite it‚Äôs speed and cost, it is surprisingly effective at accurately describing images4.4¬†Example follows‚Ä¶\n\nUTC: 2024-03-14T11:50:31Z\nModel Name: claude-3-haiku-20240307\nPrompt: Describe this image.\nImage:\n\n\n\n\n\nOutput: This image shows various electronic and technical equipment. There is a device that appears to be a test instrument or measurement tool, with knobs and displays. Next to it is a device that looks like a portable audio player or recorder. On the table in front of these devices is a document or diagram labeled ‚ÄúCode in Line / Rehabilitation Hand-Night‚Äù, which seems to be some kind of technical or engineering schematic. There is also a red tool or instrument that appears to be used for testing or working with the equipment.\nInput Tokens: 795 ($0.25/1000000 tokens)\nOutput Tokens: 108 ($1.25/1000000 tokens)\nTotal Cost: $0.00033375 (0.03¬¢)\nTime: 3.12 (s)\n\n\n\nCreating an Interface\nThe goal of the interface for the generative lab identity inference is to make it as simple as possible for those with a non-technical background to use, on any device5.5¬†Including Winchester School of Art teaching and administrative staff, and students\nTo do so, I packaged the Python implementation into a Flask app with a basic password-protected front-end. The application is then hosted on Heroku, a cloud-based platform.\nThis exposes the application on the web for anyone to use, protects against the LLM API credits being used by the public6, and removes the need to install or interact with code.6¬†Assuming the password remains a secret to those with access\n\n\n\nThe user is prompted to provide a project title, description, project visual, and the lab itself. This information is then used as previously described to produce an independent lab logo, with explanation.\n\n\nCreating a Loading Image\nIn order to design a visually interesting graphic for the front end, I decided to fine-tine a diffusion model. To begin with I created a new 3D model of The Rotunda in AutoCAD.\n\n \n\nUsing this 3D model I created a number of synthetic images of the building, combining these with an existing dataset of real photographs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis dataset of approximately 40 images was then used to fine-tuned a Stable Diffusion model7 using textual inversion to learn the concept of The Rotunda.7¬†Locally on my M2 Pro Macbook over the course of 3 days.\n\n\n\n\nThis custom token is then used to perform inference (or generation in the context of generative AI) to generate novel representations and views of the building.\nThese representations (in the form of a GIF) provide a sense of the technology involved within the lab logo generation tool, this GIF is also displayed whilst inference is being performed in the background.\n\n\nThe images, whilst individually imperfect, collectively result in a coherent representation.88¬†Likely the most unnecessarily complex login/loading GIF ever produced.\n\nThe full application is as follows:"
  },
  {
    "objectID": "posts/human_in_the_loop/index.html",
    "href": "posts/human_in_the_loop/index.html",
    "title": "Expert-Guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems",
    "section": "",
    "text": "Domain experts often possess valuable physical insights that are overlooked in fully automated decision-making processes such as Bayesian optimisation. In this article we apply high-throughput (batch) Bayesian optimisation alongside anthropological decision theory to enable domain experts to influence the selection of optimal experiments. Our methodology exploits the hypothesis that humans are better at making discrete choices than continuous ones and enables experts to influence critical early decisions. At each iteration we solve an augmented multi-objective optimisation problem across a number of alternate solutions, maximising both the sum of their utility function values and the determinant of their covariance matrix, equivalent to their total variability. By taking the solution at the knee point of the Pareto front, we return a set of alternate solutions at each iteration that have both high utility values and are reasonably distinct, from which the expert selects one for evaluation. We demonstrate that even in the case of an uninformed practitioner, our algorithm recovers the regret of standard Bayesian optimisation."
  },
  {
    "objectID": "posts/human_in_the_loop/index.html#abstract",
    "href": "posts/human_in_the_loop/index.html#abstract",
    "title": "Expert-Guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems",
    "section": "",
    "text": "Domain experts often possess valuable physical insights that are overlooked in fully automated decision-making processes such as Bayesian optimisation. In this article we apply high-throughput (batch) Bayesian optimisation alongside anthropological decision theory to enable domain experts to influence the selection of optimal experiments. Our methodology exploits the hypothesis that humans are better at making discrete choices than continuous ones and enables experts to influence critical early decisions. At each iteration we solve an augmented multi-objective optimisation problem across a number of alternate solutions, maximising both the sum of their utility function values and the determinant of their covariance matrix, equivalent to their total variability. By taking the solution at the knee point of the Pareto front, we return a set of alternate solutions at each iteration that have both high utility values and are reasonably distinct, from which the expert selects one for evaluation. We demonstrate that even in the case of an uninformed practitioner, our algorithm recovers the regret of standard Bayesian optimisation."
  },
  {
    "objectID": "posts/human_in_the_loop/index.html#introduction",
    "href": "posts/human_in_the_loop/index.html#introduction",
    "title": "Expert-Guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems",
    "section": "1 Introduction",
    "text": "1 Introduction\nBayesian optimisation has been successfully applied in a number of complex domains including engineering systems where derivatives are often not available, such as those that involve simulation or propriety software. By removing the human from decision-making processes in favour of maximising statistical quantities such as expected improvement, complex functions can be optimised in an efficient number of samples. However, these engineering systems are often engaged with by domain experts such as engineers or chemists, and as such the behaviour of the underlying function cannot be considered completely unknown a-priori. Therefore, there exists significant scope to take advantage of the benefits of Bayesian optimisation in optimising expensive derivative-free problems, whilst enabling domain experts to inform the decision-making process, putting the human back into the loop. By providing an optimal set of alternatives to an expert to select their desired evaluation, we ensure that any one choice presents information gain about the optimal solution. Simultaneously, we ensure the choices are distinct enough to avoid the human making an effective gradient calculation. Alternative solution information such as utility function value, predictive output distribution and visualisations are provided to the expert as a pseudo-likelihood. The decision-maker then effectively performs discrete Bayesian reasoning, internally conditioning the provided information with their own prior expertise and knowledge of the solutions provided. In addition to improved convergence (depending on the ability of the domain expert), our methodology enables improved interpretability in higher dimensions, as the decision-maker has the final say in what is evaluated. Our approach works with any utility function and NSGA-II (Deb et al. 2002) is applied for multi-objective optimisation, efficiently handling the non-convex utility-space.\nFigure¬†1 demonstrates our methodology\n\n\n\nFigure¬†1: Overview of our methodology, where an augmented batch Bayesian optimisation problem is solved using multi-objective optimisation, providing an expert with a set of alternate solutions.\n\n\nBy allowing an expert to influence the experimental design through a discrete decision step, we mitigate the expert needing to make continuous decisions throughout, and do not rely on an expert `prior‚Äô of the global optimum that necessarily must be defined before optimisation. Approaches that rely on an expert-defined prior may need to redefine this at significant human-cost throughout optimisation in light of new information. Similarly, the expert has no influence over the actual solutions evaluated and the optimisation is merely weighted towards broad regions in solution-space."
  },
  {
    "objectID": "posts/human_in_the_loop/index.html#previous-work",
    "href": "posts/human_in_the_loop/index.html#previous-work",
    "title": "Expert-Guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems",
    "section": "2 Previous Work",
    "text": "2 Previous Work\n(Kanarik et al. 2023) demonstrated that experts improve the initial convergence in Bayesian optimisation for semiconductor processes. However, this can be counterproductive in later stages. Algorithmic integration of expert knowledge has been explored Hvarfner et al. (2022). (Hvarfner et al. 2022) and (Ramachandran et al. 2020) use user-defined priors to weight the acquisition function, while (Liu 2022) diminishes this weight over time. These methods are static and don‚Äôt allow real-time expert input. (Gupta et al. 2023) allows continuous expert involvement, using a linear Gaussian process to approximate human intuition, achieving sub-linear regret bounds. (Kumar et al. 2022) and (Maus et al. 2022) present similar frameworks, offering alternative solutions for evaluation, with the expert making the final decision latterly in a molecular design setting."
  },
  {
    "objectID": "posts/human_in_the_loop/index.html#method",
    "href": "posts/human_in_the_loop/index.html#method",
    "title": "Expert-Guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems",
    "section": "3 Method",
    "text": "3 Method\nWe first maximise a given utility function \\(\\mathcal{U}\\) for a given dataset \\(\\mathcal{D}_t:= \\{(\\mathbf{x}_i,y_i)\\}_{i=1}^t\\):\n\n\\[\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\usepackage{amsmath,amsfonts,amssymb,algorithm,algorithmic}\n\\]\n\n\\[\n   \\mathbf{x}^* = \\argmax_{x\\in\\mathcal{X}\\subseteq\\mathbb{R}^n} \\; \\mathcal{U}(x),\n\\tag{1}\\]\nresulting in the optimal next evaluation, \\(\\mathbf{x}^*\\), in a utility sense. Let \\(p\\) be the number of alternate solutions provided to the expert and construct the decision variable matrix \\(\\mathbf{X} \\in \\mathbb{R}^{(p-1)\\times n}\\) by concatenating \\(p-1\\) alternate solutions \\(\\mathbf{X} := [\\mathbf{x}_1,\\dots,\\mathbf{x}_{p-1}]\\). We then define the high-throughput (batch) utility function \\(\\hat{\\mathcal{U}}\\) which is specified as the sum of the individual utilities of alternate solutions within \\(\\mathbf{X}\\)\n\\[\\begin{align}\n    \\hat{\\mathcal{U}}(\\mathbf{X}) = \\sum_{i=0}^{p-1} \\mathcal{U}(\\mathbf{X}_i).\n\\end{align}\\] Similarly, we introduce \\(\\hat{\\mathcal{S}}\\) as a measure for capturing the variability among both the optimal and alternative solutions. Specifically, let \\(\\hat{\\mathcal{S}}\\) be the determinant of the covariance matrix \\(K_{\\mathbf{X}_{\\text{aug}}}\\) for the augmented set \\(\\mathbf{X}_{\\text{aug}}= \\mathbf{X} \\cup \\mathbf{x}^*\\): \\[\\begin{align*}\n\\hat{\\mathcal{S}}(\\mathbf{X},\\mathbf{x}^*) &= |K_{\\mathbf{X_{\\text{aug}}}}| \\\\\nK_{\\mathbf{X}_{\\text{aug}}} &= [k(\\mathbf{X}_{\\text{aug},i},\\mathbf{X}_{\\text{aug},j})]^p_{i,j=1}\n\\end{align*}\\] \\(\\hat{\\mathcal{S}}\\) quantifies the ‚Äòvolume of information‚Äô spanned by the alternative solutions \\(\\mathbf{X}\\) as well as the optimal solution \\(\\mathbf{x}^*\\). Maximising \\(\\hat{\\mathcal{U}}\\) will result in all alternative solutions proposed being the same as \\(\\mathbf{x}^*\\), that is \\([\\mathbf{x}^*_1,\\dots,\\mathbf{x}^*_{p-1}]\\). Contrary to this, maximising \\(\\hat{\\mathcal{S}}\\) will result in a set of solutions that are maximally-spaced both with respect to other alternatives, but also \\(\\mathbf{x}^*\\). At iteration \\(t\\), we then solve the following multi-objective optimisation problem: \\[\\begin{align}\\label{multi-objective}\n    [\\mathbf{X}^*_1,\\dots,\\mathbf{X}^*_m] = \\max_{\\mathbf{X}} \\; \\left(\\hat{\\mathcal{U}}(\\mathbf{X};\\mathcal{D}_t),\\hat{\\mathcal{S}}(\\mathbf{X},\\mathbf{x}^*)\\right),\n\\end{align}\\] resulting in a set of \\(m\\) solutions along the Pareto front of both objectives. From this we define \\(\\mathbf{X}^*_{k}\\) as the solution at knee-point of the Pareto front. The \\(p-1\\) solutions contained within \\(\\mathbf{X}^*_k\\) optimally trade off the sum of their utility values, with their variability. This ensures that when provided to an expert, alongside \\(\\mathbf{x}^*\\), any individual solution will have high expected information gain, and the solutions themselves will be distinct enough to ensure the expert isn‚Äôt made to make an effective gradient calculation.\nThe practitioner is then made to choose a solution to evaluate from this set of alternatives. To do so, they are provided with information such as the utility value of each solution, expected output distributions (obtained from the Gaussian process), and information regarding previous solutions that they may wish to draw upon. In doing so, the practitioner effectively performs an internal discrete Bayesian reasoning, conditioning previous prior information and expert opinion with the mathematical quantities provided to make an informed decision. Our algorithm can be located within the Appendix.\nFigure¬†2 demonstrates the intended behaviour of our approach. We present a one-dimensional case study, optimising a function obtained through sampling a Gaussian process prior, specified by a Matern 5/2 kernel with lengthscale \\(l=0.5\\). In this case study we provide 3 alternatives to an expert, who‚Äôs choice we select randomly. We provide details of optimisation methods and hyper-parameters within the Appendix.\n\n\n\n\n\n\n\n(a) The objective function and utility function after 6 function evaluations. The 3 alternative solutions that maximise the solution distance can be seen in red, whilst the black solutions denote those contained within the knee-solution of the high-throughput multi-objective problem. The yellow optimal solution is included alongside these two alternatives to an expert. In this case choice 4 is selected randomly from the 3 alternatives and the optimum.\n\n\n\n\n\n\n\n\n\n(b) The information provided to the expert regarding the three alternative solutions. In this case choice 1 and choice 3 have relatively similar utility values and predicted output distributions to choice 4 (the optimal of the acquisition function). The expert is then allowed to distinguish between these similar solutions in a way the computer cannot through their prior domain knowledge. By conditioning their prior information on the given values, the expert is effectively performing internal Bayesian reasoning.\n\n\n\n\nFigure¬†2: A standard iteration of our approach on a one-dimensional case-study."
  },
  {
    "objectID": "posts/human_in_the_loop/index.html#computational-results-discussion",
    "href": "posts/human_in_the_loop/index.html#computational-results-discussion",
    "title": "Expert-Guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems",
    "section": "4 Computational Results & Discussion",
    "text": "4 Computational Results & Discussion\nTo assess our approach, we benchmark it against standard Bayesian optimisation. In order to incorporate and assess human interaction in an automated manner, we hypothesise a number of different human behaviours. The ‚ÄòExpert‚Äô practitioner represents an ideal, where the best solution (that is the one with the highest true objective value) is always selected. Equivalently, to test the performance of our approach under the influence of a practitioner with misaligned knowledge, we present an ‚ÄòAdversarial‚Äô practitioner who consistently selects the solution with the lowest true objective value. In addition, we present a probabilistic practitioner, who selects the solution with the best true objective value with some probability. Finally, we present the behaviour of a ‚ÄòTrusting‚Äô practitioner who selects the solution with the largest utility (as these values are presented), equivalent to standard Bayesian optimisation as this solution is obtained through standard single objective optimisation (Equation¬†1). In practice, the expert will condition the information provided with their prior beliefs. In our approach this includes information regarding the expected distribution of the objective of each solution, as well as the utility value of each solution. Whilst we cannot benchmark real human behaviour due to the random nature of the objective functions, and practical issues, the behaviours described summarise key aspects in order to generate useful insights into our approach, we leave this for future work. The human behaviours applied are summarised within Table¬†1.\n\n\nTable¬†1: Human Behaviours Applied for Benchmarking\n\n\n\n\n\n\nBehaviour Type\nDescription\n\n\n\n\nExpert\nSelects the solution with the best true function value.\n\n\nAdversarial\nSelects the solution with the worst true function value.\n\n\nTrusting\nSelects the solution with the maximum utility value.\n\n\np(Best)\nSelects the solution with the best true function value with probability p(Best), otherwise selects a random solution.\n\n\n\n\nWe perform optimisation over 50 functions, each representing a sample from a Gaussian process prior with lengthscale 0.3 using the upper-confidence bound (UCB) utility function. Figure¬†3 demonstrates the average and standard deviation of simple regret, and average regret (both defined within Garnett (2023)) for each human behaviour across 1D and 2D objective functions.\nResults for 5D, and specific functions can be located within the Appendix.\n\n\n\n\n\n\n\n(a) Average regret quantities over 1D objective functions.\n\n\n\n\n\n\n\n\n\n(b) Average regret quantities over 2D objective functions.\n\n\n\n\nFigure¬†3: Regret expectation over 50 functions, \\(f \\sim \\mathcal{GP}(\\mu \\equiv 0, K_M (d,\\nu = 0.3))\\) where \\(K_M\\) is the Mat'ern 5/2 kernel function. 4 alternate choices are presented to the practitioner, and the utility function \\(\\mathcal{U}(x)\\) used is the upper-confidence bound.\n\n\nThe expectation of average regret tends towards zero for all behaviours, indicating empirical convergence. Focusing on results from the set of 1D functions, an ‚ÄòExpert‚Äô provides improved convergence than standard Bayesian optimisation (‚ÄòTrusting‚Äô) throughout all iterations, with benefits diminishing throughout the later stages, where the standard automated approach recovers the ‚ÄòExpert‚Äô average regret value, confirming previous observations regarding the importance of human input throughout earlier iterations, and conversely diminishing importance of expert opinion during `fine-tuning‚Äô Kanarik et al. (2023). Improved convergence of simple regret occurs in cases where the practitioner selects the ‚Äòbest‚Äô solution from a set 75%, 50%, and to a lesser extent 25% of the time out of 4 available choices, similarly reflected in trends across average regret. The results demonstrated in Figure¬†3 indicate the potential for our approach to improve the convergence of Bayesian optimisation even in cases where the practitioner is correct about a decision only partially. When the expert makes a random selection (\\(p(\\text{Best}) = 0.25\\), for 4 alternate solutions), standard Bayesian optimisation convergence is recovered, indicating the effectiveness of the choices presented by asking a practitioner to select between distinct solutions, each of which individually has a high utility value. This is reflected throughout 1D, 2D and 5D functions. Only in the case where the practitioner actively selects the worst solution (i.e.¬†they are misaligned), is performance worse. In higher-dimensions an adversarial practitioner performs worse, as they are performing inefficient space-filling in an increasingly larger volume before good solutions are found.\nThe methodology we present may also be interpreted as an approach for high-throughput/batch Bayesian optimisation, with an additional preference for solutions that are well-distributed throughout the search space. Our intention for future work is to benchmark it against other existing batch Bayesian optimisation methodologies Gonz√°lez et al. (2015), including those with similar multi-objective formulations (Bischl et al. (2014), Habib, Singh, and Ray (2016), Maus et al. (2022)). We will also investigate to what extent large-language models can perform the selection step."
  },
  {
    "objectID": "posts/human_in_the_loop/index.html#appendix-a-algorithm",
    "href": "posts/human_in_the_loop/index.html#appendix-a-algorithm",
    "title": "Expert-Guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems",
    "section": "Appendix A (Algorithm)",
    "text": "Appendix A (Algorithm)\nAlgorithm¬†1 demonstrates our approach.\n\nAlgorithm 1"
  },
  {
    "objectID": "posts/human_in_the_loop/index.html#appendix-b-optimisation-details",
    "href": "posts/human_in_the_loop/index.html#appendix-b-optimisation-details",
    "title": "Expert-Guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems",
    "section": "Appendix B (Optimisation Details)",
    "text": "Appendix B (Optimisation Details)\nThroughout this paper we apply 4 alternative solutions at each iteration (one of which will be the optimum of the utility function). Throughout all optimisation problems we generate an initial sample of 4 experiments, distributed via a static Latin hypercube design-of-experiments. When training the Gaussian process that models the objective-space we perform a multi-start of 8 gradient-based optimisation runs using Adam with a learning rate of 1e-3 for 750 iterations to determine GP hyperparameters. To solve the multi-objective augmented optimisation problem, resulting in a Pareto set of sets of alternative solutions we run NSGA-II Deb et al. (2002) for 150 iterations, with a population size of 100, 30 offspring at each iteration, a 0.9 probability of crossover, and 20 mutations per iteration. To generate the single optimum of the acquisition function, we perform a multi-start of 36 gradient-based optimisation runs using L-BFGS-B Zhu et al. (1997) with a tolerance of 1e-12 and a maximum iterations of 500. For the 1D, 2D, and 5D expectations over functions stemming from samples of a Gaussian process prior (with lengthscale 0.3), the lower and upper bounds used are 0 and 10 respectively for each dimension. All other bounds for functions can be located at http://www.sfu.ca/~ssurjano/optimization.html"
  },
  {
    "objectID": "posts/human_in_the_loop/index.html#appendix-c.-regret-plots",
    "href": "posts/human_in_the_loop/index.html#appendix-c.-regret-plots",
    "title": "Expert-Guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems",
    "section": "Appendix C. (Regret Plots)",
    "text": "Appendix C. (Regret Plots)\n\n\n\nFigure¬†4: Regret expectation over 50 5D functions (\\(d=5\\)), \\(f \\sim \\mathcal{GP}(\\mu \\equiv 0, K_M (d,\\nu = 0.3))\\) where \\(K_M\\) is the Matern 5/2 kernel function. 4 alternate choices are presented to the practitioner, and the utility function \\(\\mathcal{U}(x)\\) used is the upper-confidence bound.\n\n\nWe subsequently present the expectation of regret for a number of functions across all human behaviours. We present 4 alternate choices are presented to the hypothetical practitioner, and the utility function \\(\\mathcal{U}(x)\\) used is the upper-confidence bound. Each function is optimised 16 times, across a number of random initialisations, and average regret quantities are plotted. Optimisation details are presented in the previous Appendix section.\n\n\nClick to expand additional regret plots\n\n\n\n\nFigure¬†5: 2D Ackley Function\n\n\n\n\n\nFigure¬†6: 5D Ackley Function\n\n\n\n\n\nFigure¬†7: 10D Ackley Function\n\n\n\n\n\nFigure¬†8: 2D Griewank Function\n\n\n\n\n\nFigure¬†9: 5D Griewank Function\n\n\n\n\n\nFigure¬†10: 10D Griewank Function\n\n\n\n\n\nFigure¬†11: 5D Powell Function\n\n\n\n\n\nFigure¬†12: 2D Rastrigin Function\n\n\n\n\n\nFigure¬†13: 5D Rastrigin Function\n\n\n\n\n\nFigure¬†14: 10D Rastrigin Function\n\n\n\n\n\nFigure¬†15: 10D Rosenbrock Function"
  },
  {
    "objectID": "posts/oulipo/index.html",
    "href": "posts/oulipo/index.html",
    "title": "The Potential Literature Horseshoe",
    "section": "",
    "text": "defines = ['defined', 'specified'];\nstructure = ['structure','form','composition']\nmay = ['may','could','might','may well']\nlanguage = ['language','literature','text','writing'];\nbooks = ['3000 year old Beowolf','600 year old Sir Gawain and the Green Knight'];\npurely = ['purely','solely']\nmillennia = ['millennia','hundreds of years','thousands of years','centuries']\nproduce = ['produce','construct','create']\ntechniques = ['techniques','methods']\nwriters = ['writers','linguists','proponents','members'];\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you refresh this page, the article will be randomised. There are \\(2.66 \\times 10^{19}\\) potential combinations of images and text, resulting in approximately Twenty Billion Billion articles, equivalent to the number of insects on earth. Vingt Milliards de Milliards.\n\n\nrnd = Math.floor(Math.random() * 2)\ns_string = {\n  if (rnd === 0) {\n    return html`&lt;img src=\"logo.png\" width=70% title=\"Basile Morin, [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0)\"&gt;`\n  }\n  else {\n    return html`&lt;img src=\"other_logo.png\" width=60% title=\"Basile Morin, [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0)\"&gt;`\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasile Morin, CC BY-SA 4.0\nOuvroir de Litt√©rature Potentielle or Oulipo has been  as ‚Äúthe search for new structures, which may be used by writers in any way they see fit‚Äù (Motte 1998, p2‚Äì3). For , authors have been guided by language constraints resulting in  s; From the alliterative verse of the , to the syllabic rigidity of Japanese haikus. By formalising a number of individual vocations, the original Oulipo  in the early 1960s were embarking on the revitalisation of  generation in search of discovering something deeper, sound familiar?\n\nrndl = Math.floor(Math.random() * 2)\nl_string = {\n  if (rnd === 0) {\n    return html`&lt;a href=\"https://en.wikipedia.org/wiki/A_Void\"&gt;not using the letter e&lt;/a&gt;`\n  }\n  else {\n    return html`replacing every noun with the seventh next noun in the dictionary (S+7 rule)`\n  }\n}\n\no_string = {\n  if (rnd === 0) {\n    return html`replacing every noun with the seventh next noun in the dictionary (S+7 rule)`\n  }\n  else {\n    return html`&lt;a href=\"https://en.wikipedia.org/wiki/A_Void\"&gt;not using the letter e&lt;/a&gt;`\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn order to define/identify new s,  in France turned to automated transformation . These often instantiated as rules or constraints, such as , , or only using a single vowel letter. As the originators of Oulipo explored ways to manipulate language and text in search of new s, they turned to mathematics for inspiration, thus leading to the final vocation; the transposition of mathematics to words. Naturally the extension was made to computers, and the combinatorial nature of  generation was quickly highlighted, as mathematician Claude Berge writes (Motte 1998, p152) :\n\n[‚Ä¶] we believe, that the aid of a computer, far from replacing the creative act of the artist, permits the latter rather to liberate himself from the combinatory search, allowing him also the best chance of concentrating on this ‚Äúclinamen‚Äù1 which, alone, can make of the text a true work of art.1¬†Clinamen (/kla…™Ààne…™m…ôn/;) is the Latin name Lucretius gave to the unpredictable swerve of atoms, in order to defend the atomistic doctrine of Epicurus. [‚Ä¶] it has come more generally to mean an inclination or a bias. Source\n\nIn applying combinatory literature, Oulipo , most notably Raymond Queneau in his 1961 work Cent Mille Milliards de Poemes (Hundred Thousand Billion Poems), have highlighted the impossible complexity of language. Elegantly embracing this complexity, Queneau simply presents the complete set of lines within a sonnet in the form of cut out strips, any combination of which  be constructed by the reader. In describing the combinatorial nature of Cent Mille Milliards de Poemes, Berge presents the following figure‚Ä¶\n\nrnd_nn = Math.floor(Math.random() * 2)\nnn_string = {\n  if (rnd === 0) {\n    return html`&lt;img src=\"nn.png\" width=70%&gt;`\n  }\n  else {\n    return html`&lt;img src=\"nn2.png\" width=100%&gt;`\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVerses act equivalently to neural network layers, and phrases correspond to discrete nodes‚Ä¶ could we be seeing the early sparks of the use of neural networks for language modelling?2 Of course, what is missing is the mathematical transformations from layer to layer, token embeddings, attention etcetera‚Ä¶ but it is interesting to consider the thematic similarities of complexity, , and distillation (or lack thereof) of  that both members of Oulipo and machine learning researchers have successfully applied.2¬†The modern use of neural networks to model language can probably be attributed to Bengio et. al, 2003, 42 years later.\n\nrnd_gp = Math.floor(Math.random() * 2)\ngp_string = {\n  if (rnd === 0) {\n    return [html`&lt;img src=\"oulipo.jpeg\" width=60%&gt;`,html`&lt;em&gt;Oulipo Group √† Boulogne, avenue de la Reine, on September 23, 1975, in the garden of Fran√ßois Le Lionnais‚Äôs house. At the center Raymond Queneau and Fran√ßois Le Lionnais, on his side, with a nespaper in the hands. On the very left Italo Calvino. &lt;a href=\"https://www.arshake.com/en/oulipo-ouvroir-de-litterature-potentielle/\"&gt;Source&lt;/a&gt;.&lt;/em&gt;`]\n  }\n  else {\n    return [html`&lt;img src=\"raymond.jpeg\" width=60%&gt;`,html`&lt;em&gt;Raymond Queneau with the group Les Fr√®res Jacques in 1954, Getty Images/Keystone.&lt;a href=\"https://ici.radio-canada.ca/ohdio/premiere/emissions/aujourd-hui-l-histoire/segments/entrevue/351266/oulipo-richard-boivin\"&gt;Source&lt;/a&gt;.&lt;/em&gt;`]\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGiven that a modern day large language model such as GPT-4  reasonably be described as a very large set of well  mathematical rules, could this ever  a valid literary ? How are LLMs distinct from more basic rules such as S+7? In addition, multi-model models can now see and hear as well as read. According to David Chalmers these additional senses  result in fish-level consciousness in the next ten years. Consciousness aside, at the very least LLMs will appear smarter than a human by then. What are the implications for Potential Literature when a set of rules and resulting  can think for itself?\nUnlike the output from an LLM, a haiku can be instantly recognised and verified as coming from its defining 5-7-5 syllabic writing . When the  itself is visible in the output, the focus is turned  to what Berge refers to as the clinamen. It is here that the meaning is gained.\nThe difference between LLMs and a well-established language  such as a haiku is one of complexity, and the  that results. By extending a series of basic transformations and rules towards LLMs, a horseshoe effect is achieved. The sheer quantity of rules within an LLM reflects a removal of linguistic constraints, order is returned from chaos, and the resulting underlying  is obscured.33¬†Though for now (Jan 2024) LLMs produce hallucinations, incorrect facts and statements which in turn partially disclose the underlying structure of the neural network.\n\nrnd_d = Math.floor(Math.random() * 3)\ndep_string = {\n  if (rnd_d === 0) {\n    return html`&lt;img src=\"dep.png\" width=60%\"&gt;`\n  }\n  else if (rnd_d === 1) {\n    return html`&lt;img src=\"dep2.png\" width=60%&gt;`\n  }\n  else if (rnd_d === 2) {\n    return html`&lt;img src=\"dep3.png\" width=60%&gt;`\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHerein lies a paradox for the use of language models as a direct tool for creativity, as a platform for a distinct linguistic structure. A language model is trained to  text that is indistinguishable from the text within its training distribution. However, as soon as it achieves this task, the underlying structure becomes obscured as complete ‚Äòorder‚Äô is achieved, and the impact is lost. The relationship between the rules and their resulting linguistic  is distinctly one-way, it is nearly4 impossible to reason whether the content has been generated from a neural network, yet alone infer the parameters, or even recognise the architecture. Would a haiku still be a haiku if you couldn‚Äôt immediately infer its syllabic structure?4¬†This nearly contains a level of nuance, as identifying whether an output has been generated by an LLM is an open research area (See On the Reliability of Watermarks for Large Language Models and A Watermark for Large Language Models). However, the point remains. To the average reader, without the use of cryptographic tools, modern LLM output (as of Jan 2024) remains practically indistinguishable from human-level text.\nTo answer the question posted in the title: Has Machine Learning Solved Oulipo? The answer, is no. The beauty of Potential Literature is that by enforcing literary constraints and therefore defining new s, we not only place the emphasis  on the meaning, but also enable the reader to interpret  in new ways through a sense of disorder. As a set of rules and constraints, an LLM succeeding in its objective only serves to obscure the resulting linguistic structure in a bid to ‚Äòseem human‚Äô, regaining order from chaos.\nIt is clear that these two vocations in the field of Potential Literature: defining rules and constraints for , as well as identifying new language structures, are inherently linked. However, these two objectives conflict when rule set is as flexible as it is within an LLM.\nAs Italo Calvino wrote in late 1967, in a remarkable foreshadowing (Motte 1998, Cybernetics and Ghosts (1967) p13-14):\n\nMotte, W. F. 1998. Oulipo: A Primer of Potential Literature. Dalkey Archive Paperbacks. Dalkey Archive Press.\n\nThe true literature machine will be one that itself feels the need to produce disorder, as a reaction against its preceding production of order: a machine that will produce avant-garde work to free its circuits when they are choked by too long a production of classicism. [‚Ä¶]. To gratify critics who look for similarities between things literary and things historical, sociological, or economic, the machine could correlate its changes of style to the variations in certain statistical indices of production, or income, or military expenditure, or the distribution of decision-making powers. That indeed will be the literature that corresponds perfectly to a theoretical hypothesis: it will, at last, be the literature.\n\nCalvino, I‚Äôm sure would argue, that we have the alignment of LLMs all wrong to  truly novel literature, and how would Raymond Queneau respond to the combinatorial possibilities of large-language models?\nCent Mille Milliards de Poemes can be seen as analogous to a partially trained neural network itself, as a mathematical object. There is beauty in the disorder of the relationship between tokens, and the rules that can be combined to  language. However as soon as an output is produced and the combinatorics collapses, the  is obscured, and like a phantom the meaning disappears."
  },
  {
    "objectID": "temp/spectral_ai/index.html",
    "href": "temp/spectral_ai/index.html",
    "title": "ON THE SPECTRALITY OF ARTIFICIAL INTELLIGENCE",
    "section": "",
    "text": "AI takes language, images, data and, through the use of latent variables places these ideas within space.\nThrough tokenisation: images, language, and art is given space. Conceptually is taken from the aether and into an (albiet high-dimensional) space.\nIn the context of electronic life, Art was produced by participants and embedding was used. Large language models used to correlate the prompts etc etc ‚Ä¶. use of embedding and using AI to infer the relations beteen things, underlying world model.\nIn this moment these ideas enter the spectral domain.\nMathematical operations then move and shift this space to fit these ideas, and out the other side other language comes out of the other side.\nDespite being of the same form as the inputs, these outputs come back ‚Äòwith something‚Äô, somewhat tainted ‚Ä¶ why?\nThese tainted outputs, language and images are easy to spot for most people, and can be considered unnerving, or unnatural.\nNot only are the outputs themselves are uncanny, but they are made uncanny regardless by entering and exiting this mathematical space. By exiting the aether, entering space, and then reenterig the aether. Having been rigorourly mathematically defined, transformed, then undefined they return into the conversation somewhat different.\nGenerative art is very prevelant but it has diminished impact in other circles. Art produced by code can be interpreted similarly as AI producing things.\nThe code itself exists as an inbetween for ideas, concepts have been laundered in a very rigid space, and out the other side they come out somewhat tainted.\nIs there an example of another medium that has been shunned in this sense throughout history? How did it get overcome, what barriers were there for people who didn‚Äôt get it?\nThis may explain the lack of prevelance of coded art and code as a medium, the sense of impersonality and the separation between the art and the artist as code is an intermediary.\nIn electronic life, by giving ideas a space they become uncanny. By showing people how AI works.\nBy Mapping human ideas created in isolation, they become spectral and uncanny, as they are presented and made to exist within a digital space (vector space) which has spectral properties over concepts just in the aether.\nThere is a relationship here between collected ideas and group conciousness.\nCan the US be concious? With embedding and AI, placing ideas in space is like this.\nIs the presentation of ideas and the fact they‚Äôre all linked what makes it uncanny? The spectral spaces of artificial intelligence.\nThe idea that people are a part of a group conciousness alongside a personal conciousness.\nBy peeling back the curtain, some people reject this. They are shown that you are part of a group conciousness and AI allows us to interact with this directly. AI has also itself been created by the group conciousness!\nThis could be why AI and art is shunned, the idea that you exist and your ideas exist within a wider spectral space of group collective conciousness. And AI forces us to reason with this fact, that we are spectral and our ideas exist whtin this spectral space.\nthe uncanny-ness we feel is a feeling towards ourselves and our group collective conciousness.\nPeople don‚Äôt like their ideas existing within a wider space. They can a spectrality that forces them to acknowledge their place within a potential wider conciousness.\nWhere does AI and art fit within all this?\nThere is an arguement that AI models themselves not their inputs or outputs are spectral. They provide space to the ideas and it is the model itself, the transformation from a normal piece of text to a space, that is unnerving.\nYou have a piece of text entering this digital space, but the process that does this is AI. The AI therefore isn‚Äôt the spectral space itself that the idea enters, but it is the enabler.\nAI itself forces people to acknowledge the existence of this space of ideas and‚Ä¶ therefore this group collective conciousness, which individual conciousnesses woud reject.\nAI models are the transformation into this space, the messengers, the transporters, they have a friendly relationship with the space itself and ezist alongside but they are the portal.\nIt is the interaction with AI that is the interesting thing. How people face and interact with the ability to enter a spectral space. And how the ideas that exist the space are interpreted.\nWhen people are afraid of AI are they afraid of the consequences or are they afraid of the AI taking actions (performing transformations) to enable these consequences?\nAre people afraid that these consequences are born of the spectral domain?\nAre we afraid of this presence that is pulling strings within the spectral space of ideas? That has given ideas a physicality. AI has the ability to bring ideas and concepts into a physical space.\nLots of this applies only to embeddings like text embeddings, but in the case of feed forward neurals, it is the neural network itself that is this space.\nThe idea that an outcome or impact can be felt as a result of something being taken from something as fluid and creative as images, and art‚Ä¶ go through a rigid and well defined procedure and come out as an image or language‚Ä¶ the network has a haunting to it wherein we don‚Äôt trust the outcomes as much, especially because sometimes we can see errors.\n6 fingers for example are literally like an alien, but also remind us of a lack of understanding.\nDerrida 1994: ‚Ä¶capable beyond the opposition between the presence and the impresence, actuality and non actuality, life and nonlife, and thinking of the positioning of the spector.\nAI as a conceptual spector, allowing ideas to enter a spectral space and then letting them leave.\nElectronic life forced people to face this spectral presence.\nSecular horror offers a glimpse of the absolute black nothingness that lies between the maze of appearences. A revelation that there is no transecendant reality, onlt the final death of meaning.\nOur group collective conciousness that has created both AI methods and AI models‚Ä¶the result is these methodologies not having any actual understanding. It is the final death of meaning, the realisation that us and only us have created this spector.\nWhat other art has forced people to reason this way?\n\nIndividuals created AI (groups and collectives)\nAI models and AI methods have both been defined by us.\n\nSpectral determinism, to have never been a subject in own right.\nWhat adds an additional dimension is that we set the rules. AI has been defined and created by us. The ability of us to define a spectral space could lead to spectral determinsism via AI. Or collective conciousness determinsism?\nWe create something, we create the data, and get it doesn‚Äôt feel right, the outputs have interfaced with our group collective conciousness?\nWe create AI through years of research, AI presents ideas from ‚Äòus‚Äô, individuals faces with both these facts.\n\nWe created a spector and the spectral space\nWe exist within that spectral space"
  },
  {
    "objectID": "temp/ml_in_ce/index.html",
    "href": "temp/ml_in_ce/index.html",
    "title": "A True Outlook of ML in Chem Eng",
    "section": "",
    "text": "This post isn‚Äôt a review of the history Machine Learning in Chemical Engineering, but a series of observations of trends from the perspective of a graduate student at a time of change. There won‚Äôt be any citations and I won‚Äôt use names, institutions.\nSomething to motivate why this perspective matters, not biased towards previous research‚Ä¶\nStarted as an undergraduate in 2017, Masters during COVID in 2020 and a PhD in Machine learning & Chemical Engineering in late 2021.\nThoughts about conferences, education, conflicts and paradoxes. A manifesto written at a time of change.\nHighlight the rift in literature, from a quality perspective.\nWhat do I want to say? - Conferences are backwards facing and encourage incremental steps. Many established academics end their talks with ‚Äòuse ML‚Äô - Existing paradaigms such as global optimisation dominate discussion‚Ä¶ other domains have progressed much faster with a much more open outlook. ML shouldn‚Äôt work but a combination of brute force, data, scale results in things we didn‚Äôt realise would happen‚Ä¶ Step out into the unknown. - CE research is too quick to frame and place things into existing paradaigms. - Easy to get away with bad research‚Ä¶how? - Where are the true developments - Comment on the trned of ML companies and AI companies (RL for datacenter cooling, Elli Lilly, Novartis)‚Ä¶what is the compettive advantage of CE?"
  },
  {
    "objectID": "posts/electronic_life/index.html",
    "href": "posts/electronic_life/index.html",
    "title": "Electronic Life",
    "section": "",
    "text": "Electronic Life was debuted at the Late @ Tate Britain ‚ÄúFREEDOM FREQUENCIES‚Äù.\nAI is the new disruptor.\nCome to the Taylor Digital Studio at 8pm to talk directly with Electronic Life - a live AI entity who will respond to your thoughts and queries on new technology and art making.\nAnd meet with young people from Element who have been experimenting with, and challenging, creative AI tools. Through the generation of new texts, images, and music they have generated playful, lyrical, and often surreal motifs to express ideas about their lives. Their work will be presented throughout the evening by ‚ÄòElectronic Life‚Äô, which has been built from data captured throughout their creative making process.\nElectronic Life was devised in collaboration between Ed D‚ÄôSouza and Sunil Manghani from Winchester School of Art, University of Southampton with members of Element, Tate Collective Producers, and Tom Savage at Alan Turing Institute.\nElement produces creative projects with care-experiences young people who are often from marginalised and minoritised communities.\n\nELECTRONIC LIFE is a combination of over 2000 images and prompts, text embeddings, a large-language model, an image model, and a speech-to-text model all running locally on a MacBook (M2 Pro) not connected to the internet.\nOver the course of 8 weeks, participants from Element produced 2000 individual images on DALL-E, spanning a total of around 400 individual prompts.\nThese were scraped from the DALL-E online history providing an initial dataset for ELECTRONIC LIFE, formed of the collective ideas of participants.\nWhilst participants formed their ideas independently, they do not exist in isolation within ELECTRONIC LIFE. The pretrained jina-embeddings-v2-base-en embedding model was used to embed the prompt data into vector space.\n\\[\\text{prompt} \\xrightarrow{\\text{Jina Embeddings}} x \\in \\mathbb{R}^{276}\\]\nTo visualise this collective consciousness of ideas from participants, t-SNE was used to map the high dimensional prompt vectors into a lower, two-dimensional space.\n\\[x \\in \\mathbb{R}^{276} \\xrightarrow{\\text{t-SNE}} x \\in \\mathbb{R}^{2}\\]\n\n\n\n\n\nTo explore this space visually, a parametric background function was specified. The function used is similar to a radial basis function, where each prompt ‚Äòweights‚Äô the space in a given location based on the distance to the evaluated location. By providing a set of ever-changing synthetic weights, the background function can be made to warp and shift around the embedded prompts.\nThis is implemented in JAX, enabling two important aspects.\n\nThe ability to calculate and draw the background dyanmically and in real time with vmap.\nAnalytical derivatives through JAX‚Äôs automatic differentiation.\n\n\n\n\n\n\nUsing this differentiable background, weighted around the embedded prompts, a momentum-based gradient descent is performed, providing a way of exploring the common themes and ideas. The momemtum term encourages movement throughout the space, and the parametric background ensures that individual ideas are emphasised.\nThroughout the event, audience members are invited to interact with ELECTRONIC LIFE. A number of actions occur when an interaction is triggered.\n\nThe audience member speaks into a microphone and OpenAI‚Äôs Whisper model performs a speech-to-text conversion.\nThe resulting text is embedded into the same space as the previously embedded prompts, and cosine similarity is used to return the ‚Äònearest‚Äô prompt within the dataset, which we will call the memory.\nAn LLM is prompted to relate the audience members comment with the memory within ELECTRONIC LIFE‚Äôs internal dataset. A thread is created and inference is performed using Zephyr 7B-\\(\\beta\\) via llama.cpp to generate a response.\nSimultaneously we obtain the image associated with the memory, another thread is created and the image model BakLLaVA is prompted to describe the image within the dataset.\nFinally, the background function is weighted around the memory in the two-dimensional embedding space and all information returned using the pyttsx3 text-to-speech library.\n\n\n\nVideo\n\n\n\n\n\n‚Äã\nElectronic Life: devised by Ed D‚ÄôSouza and Sunil Manghani from Winchester School of Art, University of Southampton with members of Element and Tom Savage at Alan Turing Institute, with support from Tate Learning Partnerships, Tate Collective Producers and Tate Young Peoples Programmes. Supported by University of Southampton."
  },
  {
    "objectID": "posts/gazza_agonistes/index.html",
    "href": "posts/gazza_agonistes/index.html",
    "title": "Gazza Agonisties, Ian Hamilton - Review",
    "section": "",
    "text": "There is no discernible anniversary associated to this review of Gazza Agonistes. The book was first published 29 years ago and describes the events surrounding the life of footballer Paul Gascoigne throughout the 1990s. The author, literary critic and poet Ian Hamilton passed away in 2001. However, his depiction of the rise and fall of a talented man is timeless and universal. Hamilton writes from the perspective of a lifelong fan, willing Gazza up the field from behind the pen, a ‚Äòself-styled Gazzamane‚Äô. He celebrates the highs with literary flourish, and reports on the lows with all the authority of a disapproving parent.\n\n\n\n\n\nIan Hamilton - Collected Poems: Fair use, Link\n\n\nI was first made aware of Gazza Agonistes via a series of recent tweets showing footballers gracing the covers of the London Review of Books throughout the 1980s, with titles such as ‚ÄòSocial democracy, Sociology, Soccer‚Äô. Karl Miller, the editor at the time pioneered as the Guardian put in his obituary, ‚Äòa distinguished style of soccer journalism‚Äô. Gazza Agonistes is perfectly placed within this subgenre and seems so far away from the current state of football critique, consisting largely of Mark Goldbridge looking sad.\nWriting in 1994 (with a 1998 afterword written after Gazza‚Äôs exclusion from the World Cup squad that year), Hamilton describes being attracted to Gazza‚Äôs ‚Äúlegendary scampers‚Äù whilst at he played for his childhood club Newcastle, later moving to Tottenham, Hamilton‚Äôs childhood club. This transaction of loyalties, from one man‚Äôs sporting allegiances to another, provides Hamilton with the emotional license to report on Gazza in a way that is distinct from the tabloid journalists that plague the footballer‚Äôs career.\n\n\n\n\n\nPaul Gascoigne and Vinnie Jones\n\n\nGazza Agonistes highlights just how much the culture of football has changed throughout the last 20 years. Players were expected to take accountability for their own actions; justifications for drunken nights were laundered by the tabloids and aired out to dry in the court of public opinion. The concept of the beautiful game seemed much more tangible when those involved talked and behaved just like those watching in the stands. Gazza would go to the pub with his mates like us, end up in fights like us, and famously shed a tear like us. There is an argument against this level of scrutiny. It destroyed Paul Gascoigne; why should people earning a living have to answer for their actions outside of their work?\nFor better or for worse, player engagements on social media and with journalists are now heavily vetted. Clubs don‚Äôt want to risk anything that could undermine their own image, and in turn their net worth. Owners are looking to sell at reputational highs, whilst fans want owners to leave when their clubs are at their lowest. Who knows what Hamilton would think of the players who are expected to answer for nation-states, and how the Gazza of then would respond to allegations of sports-washing.\n\n\n\n\n\nNessun Dorma performed by Andrea Botcelli after Leicester won the 2015/16 Premier League, a song forever associated with the beautiful game following the 1990 world cup.\n\n\nGazza Agonistes is a reminder of a simpler, more passionate footballing time; best enjoyed whilst listening to the soundtrack of the 1990 Italian World Cup Nessun Dorma. For all of Gazza‚Äôs mistakes, and the treatment he endured, his epic struggles will not be repeated any time soon, and neither will an account as poetic as Hamilton‚Äôs."
  },
  {
    "objectID": "posts/transubstantiation/index.html",
    "href": "posts/transubstantiation/index.html",
    "title": "The Machine Learning Reformation",
    "section": "",
    "text": "Attending mass was a mandatory requirement in medieval England and formed a central foundation for many aspects of parish life. The highlight of mass was the elevation of the host, physically transformed into the body of Christ. This act could only be performed by the parish priest and was consumed only by those priviledged few within the chancel (Orme 2021).\nObserving the genuine body of Christ was of the utmost importance for the laity. However, physical (chancel screens) and mental (understanding Latin) barriers stood between them and the clergy. In time, groups such as the Lollards, lead by John Wycliffe rejected transubstantiation and its importance, setting the course for the democratisation of Christianity in England. However, these barriers served as a reminder of the heirarchy the Catholic Church had created, until the reformation in 1547.\nSam Altman, Elon Musk, and many others continue to insist that artificial general intelligence (AGI) will be achieved in the next few years. If and when AGI is announced, divisions will open up, Microsoft will contractually lose access to OpenAI‚Äôs latest models (OpenAI 2024), there will be calls of heresy and false prophets, and the accompanying video will almost certainly be presented by someone on an unfathomable compensation package.\nThere will be those, armed with heavily quantized open-source LLMs running locally on Macbooks, akin to Gutenberg Bibles, that will insist that AGI is not real. AGI was never the goal for the majority of people. AI has the ability to change the way that engineers, writers, and artists work, and the democratisation of open-source LLMs, serves to benefit the lives of everyone1 equally.\nThe current power struggles within machine learning are fought on inherently Christian lines. From the French Yann ‚ÄòJohn Calvin‚Äô LeCun‚Äôs open-source ideology and scepticism in the real presence of AGI, to Sam Altman‚Äôs claim to primacy and inevitable monopoly on the transubstantiation of a super-us.\nA key assumption is that if AGI occurs, it will be useful. This will be the nail in the coffin that will seal the fate of the Macbook Lollards in the end. But what happens if it is not?\nIn more recent years Michael Craig-Martin affirmed the medieval world view of transubstantiation with An Oak Tree. Capturing the sense of belief that was felt by medieval Christians by enforcing that a glass of water placed high on a glass shelf was in fact a genuine oak tree. To believe that that glass of water is an oak tree, is to make a leap of faith.\nAs Tom Holland (Holland 2019) would point out, we have made, and continue to make these leaps of faith on a daily basis. The concept of human rights for example is one that we simply choose to believe, in the view that it will make the world a better place2.\nAn Oak Tree invites us to take this leap, and embrace it, unapologetically confronting us with its presence. Despite January 1st being an arbitrary day, it feels like a new beginning because we all collectively believe it‚Äôs a new beginning.\nAs AI approaches levels of human intelligence, some will choose to make the leap of faith and some will not. We should relish and enjoy the spectacle of AGI, as medieval parishioners would have done at mass every Sunday.\nHowever, we must not lose sight of the ability of machine learning to change our lives in more tangible ways.\nIt will not be an AI that replaces your job, but someone with the ability to use AI."
  },
  {
    "objectID": "posts/transubstantiation/index.html#footnotes",
    "href": "posts/transubstantiation/index.html#footnotes",
    "title": "The Machine Learning Reformation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWith enough compute‚Ü©Ô∏é\nItself a goal that we deem somewhat important.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/trajectory_plots/index.html",
    "href": "posts/trajectory_plots/index.html",
    "title": "Optimisation Trajectory Plots",
    "section": "",
    "text": "Here I will outline my personal preferences for best plotting practices for optimisation trajectory plots using Matplotlib. I will be initialising the plot as demonstrated in my previous posts here and here.\nI won‚Äôt outline a single solution here, but rather a number of options and best practices.\nI‚Äôll start by plotting the following objective function:\n\\[\\min_{x_1,x_2} \\quad 100(x_2-x_1^2)^2 + (x_1-1)^2 \\]\n‚Ä¶and in Python, using MLX.\n\nimport mlx.core as mx\n\ndef f(x1,x2):\n    return 100*(x2-(x1**2))**2 + (x1-1)**2 +1\n\nFor now we will use the bounds -3 and 3 for both \\(x_1\\) and \\(x_2\\). We can apply MLX‚Äôs vmap functionality to really quickly evaluate an entire grid of points in parallel.\n\nf_grid = mx.vmap(mx.vmap(f,in_axes=(None,0)),in_axes=(0,None))\nn = 100 \nlb = -3; ub = 3\nx1 = mx.array([2.0])\nx1 = mx.linspace(start=lb,stop=ub,num=n)\nx2 = mx.linspace(start=lb,stop=ub,num=n)\n\ny = f_grid(x1,x2).T\n\nJust for peace-of-mind, lets compare the time it takes to evaluate the function on a 100 X 100 grid the standard way:\n\nimport timeit \n\ndef normal_mat(x1,x2):\n    y = np.zeros((n,n))\n    for i in range(n):\n        for j in range(n):\n            y[i,j] = f(x1[i],x2[j])\n    return y.T\n\nnumpy_time = timeit.timeit(lambda: normal_mat(np.array(x1),np.array(x2)),number=10)\nmlx_time = timeit.timeit(lambda: f_grid(x1,x2),number=10)\n\nprint('Numpy (s): ',numpy_time,' ','MLX (s): ',mlx_time,'\\n')\nprint(np.round(numpy_time/mlx_time,4),' times faster.',);\n\nNumpy (s):  0.20080224995035678   MLX (s):  0.0006692500319331884 \n\n300.0407  times faster.\n\n\nWe begin by plotting these evaluations the default way‚Ä¶\n\nX1,X2 = np.meshgrid(x1,x2);\nplt.contourf(X1,X2,y,levels=100);\n\n\n\n\n\n\n\n\nAs with 2D scatter plots, we will set up the plot using Matplotlib‚Äôs object orientated syntax, set the aspect ratio to be equal using ax.set_aspect('equal'), and add labels of an appropriate size.\n\nfig,ax = plt.subplots(1,1,figsize=(6,6))#| \nax.set_xlabel('$x_1$',fontsize=16)\nax.set_ylabel('$x_2$',fontsize=16)\nax.set_aspect('equal')\n\n\n\n\n\n\n\n\n\n\nImportantly, we need a colorbar in order to interpret values of the function. By doing it in the following way we have more control over its position and options.\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nfun = ax.contourf(X1,X2,y,levels=100)\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes('right', size='5%', pad=0.1)\nfig.colorbar(fun, cax=cax, orientation='vertical').set_label(label='Objective Function Value',size=16)\n\n\n\n\n\n\n\n\n\n\nSince we will be plotting an optimisation trajectory on top of this, we should use a softer, more subtle colourmap. My personal favourite is Spectral (but I‚Äôm going to reverse it by appending _r).\n\nfun = ax.contourf(X1,X2,y,levels=100,cmap='Spectral_r')\n\n\n\n\n\n\n\n\n\n\nThis function has very steep sides, with a shallow u-shaped valley. It would be better to plot the logarithm of the objective function. We can do this by defining where we want the levels to be using geomspace which creates logarithmically spaced values between the minimum and maximum values of the objective function.\nI also change the ticks of the colourbar to match the levels of the contour plot.\n\nlevs = np.geomspace(np.min(np.array(y)), np.max(np.array(y)), num=10)\nfun = ax.contourf(X1,X2,y,levs,cmap='Spectral_r',locator=ticker.LogLocator())\n\nticks = np.round(levs,2)\ncbar = fig.colorbar(fun, cax=cax, orientation='vertical')\ncbar.set_label(label='Objective Function Value',size=16)\ncbar.set_ticks(ticks)\ncbar.set_ticklabels(ticks)\n\n\n\n\n\n\n\n\n\n\nConsidering we will be plotting a trajectory on top of this background it makes sense to make it a little more subtle. We can do this by reducing the alpha value of the colourmap.\nTo maintain some distinction between level-sets, we can plot some faded, thin contour lines in addition to the filled contour.\n\nfun = ax.contourf(X1,X2,y,levs,cmap='Spectral_r',locator=ticker.LogLocator(),alpha=0.4)\nax.contour(X1,X2,y,levs,colors='k',linewidths=0.5,alpha=0.5)\n\n\n\n\n\n\n\n\n\n\nOK now to solve an optimisation problem, I will write probably the shortest first-order optimisation algorithm possible in MLX.\n\nx = mx.array([2.0,-2.0])\ngrad = mx.grad(f,argnums=(0,1))\nx_store = np.array(x)\nfor i in range(500):\n    x = x - 0.0002*mx.array(grad(x[0],x[1]))\n    x_store = np.vstack((x_store,np.array(x)));\n\nNow I will plot the trajectory on top of the objective function, alongside highlighting the initial solution, and the final solution.\nI‚Äôll leave these black just to avoid any clashes with the background colourmap.\n\nax.plot(x_store[:,0],x_store[:,1],lw=2,c='k')\nax.scatter(x_store[0,0],x_store[0,1],s=100,c='k',marker='.',zorder=10,lw=2)\nax.scatter(x_store[-1,0],x_store[-1,1],s=100,c='k',marker='+',zorder=10,lw=2)\n\n\n\n\n\n\n\n\n\n\nI think there‚Äôs many more things you could do here, such as highlighting step sizes etc‚Ä¶ but I‚Äôll leave it there for now."
  },
  {
    "objectID": "posts/skoob_review/index.html",
    "href": "posts/skoob_review/index.html",
    "title": "Skoob, Bloomsbury - Bookshop Review",
    "section": "",
    "text": "Skoob is a second-hand bookshop that has been operating since 1979 at locations throughout London, before moving to the Grade II listed Brunswick Centre in Bloomsbury in 2007. Skoob has generous opening hours of 10:30am ‚Äì 7:30pm (apart from on Sundays) and is open 7 days a week.\nOutside, the shop maintains an unremarkable glass and concrete look. It is engulfed by a modernist concrete development and flanked by a big Waitrose. However, walking downstairs from the discrete entrance you are greeted by a rich burgundy floor surrounding a bespoke Skoob logo mosaic. Looking up, there are books everywhere. Where there is a flat wall, there is a bookshelf. Often, books are stacked horizontally in what normally would serve as breathing room between rows, with those on lower shelves spilling out onto the floor. The utilitarian shelves seem to bear the weight of the Brunswick Centre above. The aisles of Waitrose seem a world away.\n\n\n\n\n\nSkoob, taken December 2023\n\n\nThe shop is laid out like a large old key. Turning left, the central spine branches off into a series of small nooks, some with a single chair, some without. The only sound is a light hum emitted from the foil insulated air conditioning pipe that runs along the low ceiling and through the top shelves, forcing books horizontally to make way for the fixture. Operationally, this ensures that Skoob is a completely unnoticeable temperature and volume. Alongside the complete lack of natural light, time stands still. Turning right, the circular bow of the key contains the small staff area and till with shelves that loop around encircling the custodians. Categories of books in Skoob are broad and largely academic. A successful trip would be to come with a given a period, place, or politic in mind\nSkoob forces you to bend and stretch to find worthwhile books (a lot of which are not). The small spaces and full-length shelves, no two of which are the same, make it a physical challenge akin to a marathon as opposed to a sprint, making the task all the more rewarding. Upon purchase, and after travelling back upstairs you are thrust back into a more organised, less interesting world. Whilst Skoob could reasonably be described as chaotic and quite hard work, it is the perfect environment to discover something completely new."
  },
  {
    "objectID": "posts/aiche/index.html",
    "href": "posts/aiche/index.html",
    "title": "AIChE 2023 & Polar Gaussian Processes",
    "section": "",
    "text": "Yankee Hotel Foxtrot - Wilco\n\n\n\nThis AIChE was in Orlando. However, due to a delay at US customs during a layover at Chicago O‚ÄôHare I was prescribed the unfortunate detour of an evening in downtown Chicago.\nI begrudgingly recreated the cover of one of my favourite albums as well as ate deep dish pizza and the like‚Ä¶ However after arriving in Orlando I presented three talks:\n\n\n\n‚Äã\nA Data-Driven Framework for the Design of Reactor Simulations: Exploiting Multiple Continuous Fidelities: Data Driven Optimisation\nIn this talk I outlined the problem formulation and methodology for the optimisation of coiled-tube reactor geometry.\nAdditive manufacturing has enabled the creation of reactors with non-traditional geometries, so to take advantage of this we parameterise the wall and path of a coiled tube reactor and formulate a large, expensive derivative free/black box optimisation problem.\nBecause the closed loop procedure of meshing and simulating a given geometry is so expensive we formulate it as a multi-fidelity Bayesian optimisation problem with two independent continuous fidelities.\n\n\n\n\n\n TL;DR we use the insights generated via optimisation to design two new flow reactors. We experimentally validate these and demonstrate improved plug-flow behaviour at low Reynolds numbers under steady-flow conditions.\n\n\n\n\n\nFor the full read take a look at the preprint. (Savage et al. 2023) Thanks to my co-authors: Nausheen, who recently won the IMechE Process Industries Division Most Distinguished Developing Career Achievement Prize, and Jonathan who without which the 3D printing and validation wouldn‚Äôt have been possible, as well as Prof Omar Matar and Dr Antonio del Rio Chanona.\n\n\n\n\n\nMe presenting the talk. Spot the 3D printed reactor on the table.\n\n\n\n\nExtra polar Gaussian process content and shoutout to GPJax\n\n‚Äã\nAs part of this work I parameterised the walls of a coiled tube reactor using a series of polar Gaussian Processes, allowing me to shift and warp the wall whilst ensuring it remains closed and continuous (AKA no leaks!). As part of this I ended up contributing to the open-source GPJax library which has been my library of choice for Gaussian processes for the last year.\nPolar Gaussian processes work by applying a kernel function that operates using modulo-arithmatic. This ensures that as opposed to a data point at \\(\\theta=0\\) and \\(\\theta=2\\pi\\) having zero correlation, they infact have perfect correlation. In short, the correlation between data ‚Äòwraps around‚Äô to the beginning.\nIf we define a standard covariance function such as the Matern function, and simply plot samples from a respective Gaussian process on a polar axis, the results are clearly not continuous in the polar domain.\n\nimport matplotlib.pyplot as plt\nimport numpy as np \n\ndef matern(t1,t2):\n  d = 2*(t1-t2)**2; p = 2\n  return (1+(np.sqrt(3)*d)/p) * np.exp(-(np.sqrt(3)*d)/p)\n\ndef plot_samples(k):\n  n = 100\n  theta = np.linspace(0,2*np.pi,n)\n  covariance_matrix = np.array([[k(theta[i],theta[j]) for i in range(n)] for j in range(n)])\n  mean_function = np.array([1 for i in range(n)])\n\n  fig,ax = plt.subplots(1,1,subplot_kw={'projection': 'polar'})\n  ax.set_rticks([]); ax.set_rlim(-4,4)\n  cols = ['tab:blue','tab:green','tab:red','tab:orange']\n  for i in range(4):\n    prior_sample = np.random.multivariate_normal(mean_function,covariance_matrix)\n    ax.plot(theta,prior_sample,c=cols[i])\n\nplot_samples(matern)\n\n\n\n\nSamples from a non-polar Gaussian Process\n\n\n\n\nThe samples are not closed because our kernel function believes that \\(2\\pi\\) and \\(0\\) are very ‚Äòfar away‚Äô, when in fact they are perfectly correlated. A polar kernel function should reflect this fact:\n\ndef polar(t1,t2):\n  tau = np.pi\n  # angular distance\n  d = abs(((t1-t2+np.pi) % (2*np.pi)) - np.pi)\n  return (1 + tau * d / np.pi) * np.clip(1 - d / np.pi, 0, np.inf) ** tau\n\nplot_samples(polar)\n\n\n\n\nSamples from a polar Gaussian Process\n\n\n\n\nAs we can see, samples from this Gaussian process are continuous in the polar domain! We can then condition as we normally would on data.\nIn the case of defining the cross-section, we choose to exactly interpolate between 6 equally spaced points. The resulting posterior mean function then defines the cross-section.\n\n\n\n\n‚Äã\nMulti-Fidelity Data-Driven Design and Analysis of Reactor and Tube Simulations: Data Science for Complex Fluids and Complex Flows\nIn this talk I discussed the flows that we induced through the optimisation of plug-flow performance.\nDespite only specifying that we wanted the residence time distribution to be as tight and symmetric as possible, we found out that our optimised designs specifically induced Dean vortices at low Reynolds numbers.\n\n\n  Early formation of Dean vortices induced in our optimal design.\n\n\n\n\nSummary slide for the presentation Multi-Fidelity Data-Driven Design and Analysis of Reactor and Tube Simulations at AIChE 2023.\n\n\n\n\nDesigning Robust Energy Policies for Low-Carbon Technology Adoption: Design, Analysis, and Optimization of Sustainable Energy Systems and Supply Chains I\n\nThis talk was highlighting work completed with Dr Gbemi Olueye at the Imperial College Centre for Environmental Policy.\nThere are a number of really great energy policy optimisation problems that include aspects such as the relationship between technology uptake and cost (via a learning rate). Due to obvious reasons broadly concerned with ‚Äòthe future‚Äô it is important to account for uncertainty.\nLarge scale uncertainties concerning ‚Äòthe future‚Äô (which I differentiate from smaller ‚Äòstochastic‚Äô uncertainties) are often specified by upper and lower bounds. To deal with these we formulate and solve a robust optimisation problem using a parallelised cutting planes approach. This enables us to:\n\nMaintain the original nonlinear energy policy problem\nSolve the problem (approximately) robustly really fast, enabling us to experiment with uncertainty.\n\nIt‚Äôs this experimentation that is enabled by the methodology that enables us to provide trade-off graphs to policy makers as the solution itself. Something that I argue should always be done for this class of problem, given the political mandate we as engineers and scientists do not have.\nFor a full overview, click here for the preprint. (Savage, Chanona, and Oluleye 2023)\n\n\n\n‚Äã\nThat is about it for Orlando. It was great to catch up with some old colleagues from Cambridge, and some new ones. See you next year in San Diego!\n\n\n\n\n\nReferences\n\nSavage, Tom, Nausheen Basha, Jonathan McDonough, Omar K Matar, and Ehecatl Antonio del Rio Chanona. 2023. ‚ÄúMachine Learning-Assisted Discovery of Novel Reactor Designs.‚Äù arXiv. https://doi.org/10.48550/ARXIV.2308.08841.\n\n\nSavage, Tom, Antonio del Rio Chanona, and Gbemi Oluleye. 2023. ‚ÄúRobust Market Potential Assessment: Designing Optimal Policies for Low-Carbon Technology Adoption in an Increasingly Uncertain World.‚Äù arXiv. https://doi.org/10.48550/ARXIV.2304.10203."
  },
  {
    "objectID": "posts/thatcher_review/index.html",
    "href": "posts/thatcher_review/index.html",
    "title": "Killing Thatcher, Rory Carroll - Review",
    "section": "",
    "text": "Maggie‚Äôs potential assassin, ironically named Magee is presented as someone who can‚Äôt catch a break. He has a troubled relationship with his wife, is arrested countless times, and it goes without saying, didn‚Äôt kill Thatcher.\n\n\n\n\n\nPatrick Magee : PA Images / Alamy Stock Photo. Photographer: John Giles. Taken on 22 March 1999, BELFAST UK.*\n\n\nCarroll‚Äôs detailed account of his whereabouts in England are interspersed with Magee‚Äôs own reflections later on in life, which are characteristically dry: ‚Äúmy life was too chaotic‚Äù. Carroll does well to chip away and reveal a portrait of a man who presented himself deliberately dull enough that no one seems to remember what he looks like (useful if you have attempted to assassinate a sitting prime minister). The bleak reality of being an anonymous IRA operative in England is contrasted with the historical implications of the task at hand. Guy Faulks planted a bomb under the Houses of Parliament that didn‚Äôt even go off, and people burn an effigy of him every year. The events of the book seem surreal to those born after the troubles; ‚Äúone of the great what ifs‚Äù that Thatcher herself rarely chose to comprehend.\n\n\n\n\n\nThe Grand Hotel, Brighton. D4444n at the English Wikipedia, Public domain, via Wikimedia Commons\n\n\nCarroll dwells on the turning points of the era, such as the death of Bobby Sands and the rise of Gerry Adams‚Äô Armalite / Ballot Box strategy, providing the relevant context for someone unfamiliar with the period. However, by focussing largely on the ‚ÄòEngland Department‚Äô and specifically a single operation, these public-facing protagonists are side-lined in favour of a different type of history. One that is populated with normal people: students, office workers and landlords. These accounts provide important insight into the human cost of the troubles on both sides. Light relief comes somewhat unsurprisingly not from the members of the IRA (and Gerry Adams), but from the British detectives charged with locating Magee. The tangle of regional agencies searching for Magee as he drifts throughout the UK, whose jurisdiction changes every other page, results in a thrilling chase of comically bureaucratic proportions.\nMaybe it‚Äôs too soon for a screen adaptation of these events, Carroll has been sensitive with his accounts and sources (with obvious reason), and wounds are still fresh, particularly those of Thatcher‚Äôs then ‚Äúloyal enforcer‚Äù Norman Tebbit whose wife was paralysed by the attempt. However, the book is an account of a major event in modern British history that is genuinely hard to put down. Following the success of The Crown with its relentless march from the past to the present, and the Happy Valley genre of regional crime drama, Killing Thatcher has all the hallmarks of a hit series. Regardless of how people judge the event, Carroll has provided a valuable account, to remember."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TRS",
    "section": "",
    "text": "Generative Lab Identities\n\n\nFor Winchester School of Art\n\n\n\n\n\n\nMar 14, 2024\n\n\nTom Savage, Studio 3015\n\n\n\n\n\n\n\n\nGenerative Models as Mirrors and Idols; From Istanbul\n\n\nThe Inescapable, Neverending Race for Values\n\n\n\n\n\n\nMar 4, 2024\n\n\nTom Savage\n\n\n\n\n\n\n\n\nThe Machine Learning Reformation\n\n\nWhat the late middle ages can teach us about the future of AI\n\n\n\n\n\n\nFeb 3, 2024\n\n\nTom Savage\n\n\n\n\n\n\n\n\nOptimisation Trajectory Plots\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nTom Savage\n\n\n\n\n\n\n\n\nScatter Plots\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nTom Savage\n\n\n\n\n\n\n\n\nThe Potential Literature Horseshoe\n\n\nThe First Large Language Model or: Has Machine Learning Solved Oulipo?\n\n\n\n\n\n\nJan 9, 2024\n\n\nTom Savage\n\n\n\n\n\n\n\n\nSkoob, Bloomsbury - Bookshop Review\n\n\nPublished in The Felix January 2024\n\n\n\n\n\n\nJan 7, 2024\n\n\nTom Savage\n\n\n\n\n\n\n\n\n2023 Books\n\n\nA list\n\n\n\n\n\n\nDec 16, 2023\n\n\nTom Savage\n\n\n\n\n\n\n\n\nBayesian Optimisation with Noisy Measurements\n\n\nA review\n\n\n\n\n\n\nDec 5, 2023\n\n\nTom Savage\n\n\n\n\n\n\n\n\nElectronic Life\n\n\nLate @ Tate Britain\n\n\n\n\n\n\nNov 29, 2023\n\n\nTom Savage, Sunil Manghani, Ed D‚ÄôSouza\n\n\n\n\n\n\n\n\nAIChE 2023 & Polar Gaussian Processes\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\nTom Savage\n\n\n\n\n\n\n\n\nExpert-Guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems\n\n\nNeurIPS workshop on Modern Experimental Desisgn and Active Learning in the Real World\n\n\n\n\n\n\nOct 27, 2023\n\n\nTom Savage, Antonio del Rio Chanona\n\n\n\n\n\n\n\n\nConvergence Plots\n\n\n\n\n\n\n\n\n\nOct 24, 2023\n\n\nTom Savage\n\n\n\n\n\n\n\n\nGazza Agonisties, Ian Hamilton - Review\n\n\nPublished in The Felix October 2023\n\n\n\n\n\n\nOct 22, 2023\n\n\nTom Savage\n\n\n\n\n\n\n\n\nKilling Thatcher, Rory Carroll - Review\n\n\nPublished in The Felix October 2023\n\n\n\n\n\n\nOct 21, 2023\n\n\nTom Savage\n\n\n\n\n\n\nNo matching items"
  }
]