[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I am a PhD student at Imperial College London & 2023 Enrichment student at the Alan Turing Institute. I have a background in Chemical Engineering and still enjoy teaching labs at Imperial College. My interests include Bayesian optimisation, human-in-the-loop machine learning, cricket üèè, and darts üéØ.\n\nEducation\n\nImperial College London\n\nPhD in Process Systems Engineering | 2021 - Current\n\n\n\nUniversity of Cambridge\n\nMPhil in Chemical Engineering & Biotechnology | 2020 - 2021\n\n\n\nUniversity of Manchester\n\nBEng in Chemical Engineering | 2017 - 2020"
  },
  {
    "objectID": "posts/notes_on_basilisk/index.html",
    "href": "posts/notes_on_basilisk/index.html",
    "title": "BASILISK RECIPES",
    "section": "",
    "text": "What and Why\nThis post will consist of recipes for setting up a number of basic Basilisk simulations, with a focus on 3D domains.\nWhile the documentation consists of a large number of detailed fully constructed examples, I don‚Äôt think there is enough effort placed to outline the ‚Äòbuilding blocks‚Äô.\nI will attempt to convey usage of these building blocks through a number of examples for each concept.\n\n\nDimensionality Analysis\nIncluding square brackets after a quantity will give it a ‚Äòdimension‚Äô.\ndouble L = 2. [1] is equivalent to \\(L = 2 \\text{ m}\\)\ndouble g = 9.81. [1,-2] is equivalent to \\(g = 9.81 \\text{ ms}^{-2}\\)\nBasilisk C can then ensure that physical dimensions make sense when performing operations.\n\n\nBoundary Conditions\nBoundary conditions for pressure and velocity fields are set as follows, where right, left, top and bottom indicate the respective boundaries of the field.\nx-component of the velocity on the left side of the domain = 1 \\(\\rightarrow\\) u.n[left] = dirichlet(1.);\nno-slip conditions on the top of the domain \\(\\rightarrow\\) u.n[top] = dirichlet(0.);\nno-slip conditions on the bottom of the domain \\(\\rightarrow\\) u.n[bottom] = dirichlet(0.);\nfluid flows at ‚Äòoutlet‚Äô (right) \\(\\rightarrow\\) u.n[top] = neumann(0.);\nint main() {\n  run();\n}"
  },
  {
    "objectID": "posts/noisy_bo/index.html",
    "href": "posts/noisy_bo/index.html",
    "title": "BAYESIAN OPTIMISATION WITH NOISY MEASUREMENTS",
    "section": "",
    "text": "I have reworked some notes from (garnett_bayesoptbook_2023?) regarding noisy-expected improvement. Please refer to the original text for a more detailed explanation.\n\nThe assumptions that hold for the expected improvement utility function do not hold when measurements have noise.\nWe really want to find the point where the signal is optimised (Jones, Schonlau, and Welch 1998).\nHow do you determine if a measurement is signal or noise?\n\nWe begin by specifying an underlying objective function, which we consider unknown, shown in Figure¬†1. We have access to observations which we assume contain normally distributed noise.\n\n\nDefinition of underlying function‚Ä¶\ndef f(x):\n  return 2.4*np.sin(2.8*x) - ((x-3.5*np.pi) ** 2)/4 +  3.8*np.cos(1.7*x) - (x**2)/16 + np.random.normal(0,2)\n\n\n\n\n\n\n\nFigure¬†1: The underlying noisy funtion to be maximised alongside example observations.\n\n\n\n\nWe first begin by motivating the usual expected-improvement criteria. We are looking to maximise the expected increase between the of the maximum of the mean of the resulting Gaussian process after making an observation at \\(x\\) and the maximum of the current Gaussian process over potential observations \\(y\\) which are Gaussian distributed as a result of our \\(\\mathcal{GP}\\) model. \\[\\alpha_{EI}(x;\\mathcal{D}) = {\\color{red}\\int} \\left[{\\color{green}\\max \\mu_{\\mathcal{D}'}} - {\\color{blue}\\mu^*}\\right]{\\color{red}\\mathcal{N}(y;\\mu,s^2)\\text{d}y} \\tag{1}\\]\nBy formulating the expression with respect to the improvement in the mean of the current and subsequent Gaussian process models, we are expressing our interest in optimising the signal and not the noise or values of specific observations.\n\nImportant: Given a hypothetical observation \\(y\\), the value of the mean of the resulting Gaussian process \\(\\mu_{\\mathcal{D}'}\\) at given set of potential locations \\(\\mathbf{x}'\\) is\n\\[ \\mu_{\\mathcal{D}'} =  \\mu_{\\mathcal{D}} + \\frac{K_\\mathcal{D}(\\mathbf{x}',x)}{s}\\frac{y-\\mu}{s},\\]\nwhere \\(\\mu\\) and \\(s^2\\) are the mean and standard deviation of the distribution of potential values \\(y\\) could take.\n\nWhen we express this distribution in terms of the standard normal distribution \\(z := \\mathcal{N}(0,1)\\), we have \\(y = \\mu + sz\\) and as a result\n\\[ \\mu_{\\mathcal{D}'} =  \\mu_{\\mathcal{D}}(\\mathbf{x}') + \\frac{K_\\mathcal{D}(\\mathbf{x}',x)}{s}z.\\]\nPutting this back into Equation¬†1, we now only have to take the expectation over the standard normal distribution resulting in the following.\n\\[\\alpha_{EI}(x,\\mathcal{D}) = \\int \\max_{\\mathbf{x}'} \\left(\\mu_{\\mathcal{D}}(\\mathbf{x}') + \\frac{K_\\mathcal{D}(\\mathbf{x}',x)}{s}z \\right)\\phi(z)\\;\\text{d}z - \\mu^* \\tag{2}\\]\nTo begin with we will sample some initial data:\n\n# our initial dataset\nx_data = np.linspace(0,3*np.pi,24)\ny_data = np.array([f(x_i) for x_i in x_data])\n\n\n\nGP model definition and training‚Ä¶\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n    \n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\ndef build_GP(x,y,its):\n  likelihood = gpytorch.likelihoods.GaussianLikelihood()\n  model = ExactGPModel(x, y, likelihood)\n\n  model.train()\n  likelihood.train()\n\n  optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n\n  mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\n  for i in range(its):\n      optimizer.zero_grad()\n      output = model(x)\n      loss = -mll(output, y)\n      loss.backward()\n      optimizer.step()\n  return model,likelihood\nmodel,likelihood = build_GP(x_data,y_data,2000)\nGP = {'model':model,'likelihood':likelihood}\n\n\nFigure¬†2 shows this data with an initial Gaussian process (importantly assuming in-exact observations).\n\n\n\n\n\nFigure¬†2: A Gaussian process fit to the initial dataset.\n\n\n\n\nNow we will naively construct Equation¬†2 using 40 samples from the standard normal distribution, and naively calculate an approximation to the inner maximisation by evaluating at 100 location throughout the space (which defines our vector \\(\\mathbf{x}' \\in\\mathbb{R}^{100}\\)) and returning the max value from these.\n\ndef noisy_EI(x,GP):\n  model = GP['model']\n  likelihood = GP['likelihood']\n  N = torch.distributions.Normal(0,1)\n  predicted_output = likelihood(model(x))\n  mean = predicted_output.mean\n  var = predicted_output.variance\n  z_vals = torch.linspace(-2,2,40)\n  integral = 0 \n  for z in z_vals:\n    x_prime_vals = torch.linspace(0,3*np.pi,100).reshape(-1,1)    \n    new_mean_vals = model.mean_module(x_prime_vals)\n    new_cov_vals = ((model.covar_module(x_prime_vals,x)/var)*z.item())[:,0]\n    integral += torch.max(new_mean_vals + new_cov_vals)\n  integral /= 100\n  return -integral\n\nNow if we plot this function alongside the time taken to evaluate the acquisition function at 100 locations.\n\n\nTIME TAKEN:  0.9433 s. AVERAGE TIME PER EVALUATION:  0.0094 s\n\n\n\n\n\n\n\n\n\n\n\nImportant: For a fixed set of ‚Äòimprovement locations‚Äô \\(\\mathbf{x}'\\), the resulting posterior mean at each location can be interpreted as a 1D line as a function of \\(z\\):\n\\[\\mu_{\\mathcal{D}'}(z|x') = \\mu_{\\mathcal{D}}(x') + \\frac{K_{\\mathcal{D}}(x',x)}{s}z \\quad \\forall x'\\in \\mathbf{x}'\\]\nTherefore finding the inner maximum new posterior mean as a function of \\(z\\) can be translated to calculating the upper envelope of a set of lines. This upper envelope represents for a given \\(z\\), the maximum posterior mean across all of the locations in $. The main idea is to remove the inner \\(\\max\\) operator and replace it with something more tractable.\n\\[\\int \\max \\left[\\text{lines} (z)\\right] \\phi(z) \\;\\text{d}z \\rightarrow \\int \\text{upper envelope} (z) \\phi(z) \\;\\text{d}z\\]\nWhich is tractable for a piecewise linear upper envelope.\n\nFor the sake of completeness we will set up and run a basic Bayesian optimisation loop, but I‚Äôll just use a naive implementation for now.\n\n\nGradient-based multi-start optimisation of utility function‚Ä¶\ndef aq_and_grad(x,GP):\n  x = np.array(x,dtype=np.float32)\n  x = torch.from_numpy(x.reshape(-1,1))\n  x = torch.tensor(x, requires_grad=True)\n  aq = noisy_EI(x,GP)\n  aq.backward()\n  f_val = aq.detach().numpy()\n  f_grad = x.grad.numpy()[0]\n  return (-f_val,-f_grad)\n\ndef optimise_aq(GP):\n  x0_list = np.linspace(0,3*np.pi,16)\n  f_best = 1E10\n  x_best = None\n  for x0 in x0_list:\n    res = minimize(aq_and_grad, x0, method='L-BFGS-B', jac=True,\n                  bounds=((0,3*np.pi),),args=(GP), options={'maxiter': 100})\n    if res.fun &lt; f_best:\n      f_best = res.fun\n      x_best = res.x\n\n  return x_best\n\n\nPlotting every 5 iterations, for a total of 50 iterations.\n\nfor iteration in range(50):\n  # train GP\n  model,likelihood = build_GP(x_data,y_data,2000)\n  GP = {'model':model,'likelihood':likelihood}\n  GP['model'].eval(); GP['likelihood'].eval()\n\n  # optimise acquisition function\n  x_best = optimise_aq(GP)\n  y_best = f(x_best)\n\n  # add to dataset\n  x_data = torch.cat((x_data,torch.tensor(x_best)))\n  y_data = torch.cat((y_data,torch.tensor(y_best)))\n\n  if (iteration) % 5 == 0:\n    fig,ax = plt.subplots(2,1,figsize=figsize,sharex=True)\n    ax[0] = plot_model(ax[0],GP,x_data,y_data,in_loop=iteration+1)\n    ax[1] = plot_aq(ax[1],GP)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReferences\n\nJones, Donald R., Matthias Schonlau, and William J. Welch. 1998. Journal of Global Optimization 13 (4): 455‚Äì92. https://doi.org/10.1023/a:1008306431147."
  },
  {
    "objectID": "posts/convergence_plots/index.html",
    "href": "posts/convergence_plots/index.html",
    "title": "CONVERGENCE PLOTS",
    "section": "",
    "text": "Here I will outline my personal preferences for best convergence plotting practices using Matplotlib.\nLet‚Äôs start by plotting some convergence data in just about the worst way, and slowly improve on this. It is my view that when LaTeX is available you should always use it.\n\nAlways use the LaTeX extension to render figures by including plt.rcParams['text.usetex'] = True' at the top of your script.\n\nHere I have simulated some convergence data where we have 4 different benchmarks, each containing 16 separate repeat runs.\n\n\n\n\n\nUsing plt.figure() makes it harder to interact with the axis (even if you only have a single axis), making it more difficult to adjust the size and layout.\nNow we will initialise our plot correctly using the object-oriented syntax, sizing the figure appropriately for the given context. By constraining the layout we ensure that the figure fits within the given size.\n\nAlways initialise a plot using the object-oriented interface fig,ax = plt.subplots..., defining the figure size appropriately.\n\n\nNever initialise a plot using the pyplot interface plt.figure....\n\n\nfig,ax = plt.subplots(1,1,figsize=(7,3),layout=\"constrained\")\n\n\n\n\nNow we will draw our attention to plotting the data itself. It is important to think about what message you want to convey to the reader. Clearly providing each run for each benchmark is not useful. I will plot the mean and standard deviation of regret across each benchmark. I will use a line plot to convey the mean and a shaded region to convey the standard deviation.\n\nAlways ensure that the linewidth=... option is set appropriately, and more importantly consistently.\n\n\nfig,ax = plt.subplots(1,1,figsize=(7,3),layout=\"constrained\")\nfor b in range(benchmarks):\n    mean = np.mean(data[b,:,:],axis=0)\n    std = np.std(data[b,:,:],axis=0)\n    ax.plot(iterations,mean,color=cols[b])\n    ax.fill_between(iterations,mean-std,mean+std,alpha=0.2,color=cols[b])\nplt.show()\n\n\n\n\n\nAlways separate your data generation and plotting code. This will make it easier to adjust the plot later. Save data at generation (for example during optimisation) and then load it in for plotting.\n\nNow we have decided what specifically to plot and all the information we wish to present is here we can work on making the plot more interpretable.\nWe should first add some axis labels as follows. Ensure the font size is large enough to be legible. You can use LaTeX within labels by using $ $ as standard, when doing so include an r in front of the string to ensure it gets parsed appropriately.\n\nAlways choose a legible font size and text for axis labels, using LaTeX where appropriate.\n\n\nax.set_xlabel(r'Iteration, $\\tau$',fontsize=18)\nax.set_ylabel(r'Regret, $r_\\tau$',fontsize=18)\n\n\n\n\n\n\nNow we will focus on the ticks. Ticks are important because they enable the reader to interpret the values of the data.\nIt is important to consider what tick values are appropriate to display. For example in this case, I want to convey to the reader the differences in overall convergence characteristics between these methods. Therefore, it may not be necessary to display intermediate regret values.\n\nAlways select appropriate tick values for your context, adjusting the fontsize as necessary.\n\n\nx_ticks = [0,10,20,30,40,50,60,70,80,90,100]\ny_ticks = [0,0.5,1]\nax.set_xticks(x_ticks,x_ticks,fontsize = 18)\nax.set_yticks(y_ticks,y_ticks,fontsize = 18)\n\n\n\n\n\n\nWhilst it is important to provide a legend to ensure the reader can interpret the data, not everything has to have a label.\nIn this case I want to convey which color corresponds to which benchmark, but I also need to convey what the error bars mean. In order not to distract from the plot, I will choose to omit this from the plot itself and instead include it in the caption.\nIncluding a label for the error bars does not contribute to the story and the message of the plot.\n\nAlways include labels for the relevant information, such as distinguishing between benchmarks. If the plot is too crowded place the legend outside of the plot.\n\n\nNever overwhelm the reader with information and labels. Some information is fine to include within the caption, as the caption and the image are considered complimentary and dependent.\n\n\nlabels = [f'Benchmark {b}' for b in range(1,benchmarks+1)]\nfor b in range(benchmarks):\n    mean = np.mean(data[b,:,:],axis=0)\n    std = np.std(data[b,:,:],axis=0)\n    ax.plot(iterations,mean,color=cols[b],label=labels[b])\n    ax.fill_between(iterations,mean-std,mean+std,alpha=0.2,color=cols[b])\nax.legend(frameon=False,fontsize=14)\n\n\n\n\n\n\nYou will notice that I have chosen not to include the frame on the caption. This makes the plot look cleaner and less cluttered.\nFor scatter plots the frame can help distinguish between the data and the legend, but for line plots it may not be necessary.\nWhen using fill_between a faint outline is drawn which can make the plot look cluttered, I always like to remove this by setting linewidth=0.\n\nAlways remove the linewidth of fill_between for a cleaner plot.\n\nI always use and reccomend the tab: set of colours in Matplotlib. A number of people have put significant effort into choosing these so you don‚Äôt have to. Here is a list of examples that you can copy and paste.\nThese colours are softer than the default colours and are more aesthetically pleasing.\n\ncols = ['tab:blue','tab:orange','tab:green','tab:red',\\\n    'tab:purple','tab:brown','tab:pink','tab:gray','tab:olive','tab:cyan']\nfor b in range(benchmarks):\n    mean = np.mean(data[b,:,:],axis=0)\n    std = np.std(data[b,:,:],axis=0)\n    ax.plot(iterations,mean,color=cols[b],label=labels[b])\n    ax.fill_between(iterations,mean-std,mean+std,alpha=0.2,color=cols[b],linewidth=0)\n\n\nAlways use the tab: set of colours over the default colours.\n\n\n\n\n\n\nNow we can adjust the axis limits to ensure that the data is placed appropriately. In this case, the regret values are fine but the iterations should start at 0 and end at 100.\n\nax.set_xlim([0,100])\n\n\nAlways adjust the axis limits appropriately. The x-axis particularly may be too large.\n\n\n\n\n\n\nFinally, it is important to consider whether a grid is appropriate. In this case I will for demonstration, but it may be distracting. I will adjust the transparency to make it less pronounced.\n\nax.grid(alpha=0.4)\n\n\nAlways consider using ax.grid() if the individual values of the data are important.\n\n\n\n\n\n\nIf you choose not to use a grid, then I always prefer if the right and top axis are removed, resulting in a cleaner plot.\n\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\n\n\nAlways remove the right and top axis if you choose not to use a grid.\n\n\n\n\n\n\nThat is just about it for this plot. Always to remember to save figures as .svg or .pdf. If you require a .png then use the dpi= option in fig.savefig to increase the resolution.\n\nAlways save your figure as .svg or .pdf.\n\n\nfig.savefig('results.svg')\n\nFinally, read the documentation!."
  },
  {
    "objectID": "posts/vortex/index.html",
    "href": "posts/vortex/index.html",
    "title": "ON THE IDENTIFICATION OF A VORTEX",
    "section": "",
    "text": "Coherent Structures (CS) in turbulent flows are now commonly defined as vortices.\nTherefore, we require an objective definition of a vortex in order to explain coherent structures.\n\nProposal: define a vortex by the eigenvalues of \\(\\mathbf{S}^2 + \\Omega^2\\); where \\(\\mathbf{S}\\) and \\(\\Omega\\) are the symmetric and antisymettric parts of the velocity gradient tensor \\(\\nabla \\mathbf{u}\\).\n\nCaptures the pressure minimum in a plane perpendicular to the vortex axis.\n\nTurbulent shear flows are dominated by coherent structures.\nCoherent Structures are spatially coherent, temporally evolving vortical motions.\nUnderstanding:\n\nEntrainment\nMixing\nHeat / Mass Transfer\nChemical Reaction / Combustion\nDrag\nAerodynamic Noise Generation\nTurbulence\n\nRequires an understanding of coherent structures, and therefore vortex dynamics.\n\nVortex intuition throughout time.\n1945: a tube whose surface consists of vortex lines.\n\nAll vortices are vortex tubes, but not all vortex tubes are vortices (i.e.¬†within a laminar flow pipe).\n\n1979: a multitude of material particles rotating around a common center.\n1988: a region containing both a postive second invariant of \\(\\nabla \\mathbf{u}\\) and low pressure.\n1990: a region of complex eigenvalues of \\(\\nabla \\mathbf{u}\\).\n\nThe size of a vortex in a viscous fluid depends on the identifier‚Äôs threshold, therefore this study is limited to vortex cores.\nVortex core requirements:\n\nMust have net vorticity \\(\\rightarrow\\) net circulation. i.e.¬†fluid is swirling continuously within the vortex core and potential flow regions (inflow or outflow) are therefore excluded. A potential vortex has zero-cross section (i.e.¬†the middle of the vortex?).\nGeometry should be Galilean invariant. i.e.¬†regardless of how ‚Äòfast time is going‚Äô the vortex core should remain the same shape.\n\n\nInadequacies of intuitive measures\n\nPressure minimum\n\n\nIn 3D pressure may be minimised at a point, or a plane.\nAble to derive irrotational flows that have minimum pressure planes despite having no vorticity or swirl.\n\n\\[u_r = -a(t)r, \\; u_z = 2a(t)z, \\; u_{\\theta} = 0\\]\n\n\n\n\n\n\nIn Stokes flow at low \\(Re\\), pressure gradient is balanced by the viscosity term. Vortices can be formed, whilst pressure is defined only by viscosity (i.e.¬†constant with no minima).\nIsopressure surfaces do not correspond with vortical surfaces due to scale difference.\n\n\nClosed or spiralling streamlines and pathlines\n\n\nPathlines do not satisfy Galilean invariance.\n\nTwo or more vortices at different speeds in the same reference frame would be obscured.\n\nA particle may not complete a full revolution of the vortex centre during the lifetime of a vortex.\n\n\nIsovorticity surfaces\n\n\nVorticity sheets are not a vortex despite having large vorticity.\nIf the background shear is comparable to vorcitiy magnitude vortex cores can be missed.\n\nPreviously proposed definitions (satisfying Galilean-invarance)\n\nComplex eigenvalues (\\(\\sigma\\)) of the velocity gradient tensor (\\(\\nabla\\mathbf{u}\\)).\n\nComplex eigenvalues imply that the local streamline pattern is closed or spirals within a reference frame moving with a point."
  },
  {
    "objectID": "posts/electronic_life/index.html",
    "href": "posts/electronic_life/index.html",
    "title": "ELECTRONIC LIFE",
    "section": "",
    "text": "Electronic Life was debuted at the Late @ Tate Britain ‚ÄúFREEDOM FREQUENCIES‚Äù.\nAI is the new disruptor.\nCome to the Taylor Digital Studio at 8pm to talk directly with Electronic Life - a live AI entity who will respond to your thoughts and queries on new technology and art making.\nAnd meet with young people from Element who have been experimenting with, and challenging, creative AI tools. Through the generation of new texts, images, and music they have generated playful, lyrical, and often surreal motifs to express ideas about their lives. Their work will be presented throughout the evening by ‚ÄòElectronic Life‚Äô, which has been built from data captured throughout their creative making process.\nElectronic Life was devised in collaboration between Ed D‚ÄôSouza and Sunil Manghani from Winchester School of Art, University of Southampton with members of Element, Tate Collective Producers, and Tom Savage at Alan Turing Institute.\nElement produces creative projects with care-experiences young people who are often from marginalised and minoritised communities.\n\nELECTRONIC LIFE is a combination of over 2000 images and prompts, text embeddings, a large-language model, an image model, and a speech-to-text model all running locally on a MacBook (M2 Pro) not connected to the internet.\nOver the course of 8 weeks, participants from Element produced 2000 individual images on DALL-E, spanning a total of around 400 individual prompts.\nThese were scraped from the DALL-E online history providing an initial dataset for ELECTRONIC LIFE, formed of the collective ideas of participants.\nWhilst participants formed their ideas independently, they do not exist in isolation within ELECTRONIC LIFE. The pretrained jina-embeddings-v2-base-en embedding model was used to embed the prompt data into vector space.\n\\[\\text{prompt} \\xrightarrow{\\text{Jina Embeddings}} x \\in \\mathbb{R}^{276}\\]\nTo visualise this collective consciousness of ideas from participants, t-SNE was used to map the high dimensional prompt vectors into a lower, two-dimensional space.\n\\[x \\in \\mathbb{R}^{276} \\xrightarrow{\\text{t-SNE}} x \\in \\mathbb{R}^{2}\\]\n\n\n\n\n\nTo explore this space visually, a parametric background function was specified. The function used is similar to a radial basis function, where each prompt ‚Äòweights‚Äô the space in a given location based on the distance to the evaluated location. By providing a set of ever-changing synthetic weights, the background function can be made to warp and shift around the embedded prompts.\nThis is implemented in JAX, enabling two important aspects.\n\nThe ability to calculate and draw the background dyanmically and in real time with vmap.\nAnalytical derivatives through JAX‚Äôs automatic differentiation.\n\n\n\n\n\n\nUsing this differentiable background, weighted around the embedded prompts, a momentum-based gradient descent is performed, providing a way of exploring the common themes and ideas. The momemtum term encourages movement throughout the space, and the parametric background ensures that individual ideas are emphasised.\nThroughout the event, audience members are invited to interact with ELECTRONIC LIFE. A number of actions occur when an interaction is triggered.\n\nThe audience member speaks into a microphone and OpenAI‚Äôs Whisper model performs a speech-to-text conversion.\nThe resulting text is embedded into the same space as the previously embedded prompts, and cosine similarity is used to return the ‚Äònearest‚Äô prompt within the dataset, which we will call the memory.\nAn LLM is prompted to relate the audience members comment with the memory within ELECTRONIC LIFE‚Äôs internal dataset. A thread is created and inference is performed using Zephyr 7B-\\(\\beta\\) via llama.cpp to generate a response.\nSimultaneously we obtain the image associated with the memory, another thread is created and the image model BakLLaVA is prompted to describe the image within the dataset.\nFinally, the background function is weighted around the memory in the two-dimensional embedding space and all information returned using the pyttsx3 text-to-speech library.\n\n\n\nVideo\n\n\n\n\n\n‚Äã\nElectronic Life: devised by Ed D‚ÄôSouza and Sunil Manghani from Winchester School of Art, University of Southampton with members of Element and Tom Savage at Alan Turing Institute, with support from Tate Learning Partnerships, Tate Collective Producers and Tate Young Peoples Programmes. Supported by University of Southampton."
  },
  {
    "objectID": "posts/human_in_the_loop/index.html",
    "href": "posts/human_in_the_loop/index.html",
    "title": "EXPERT-GUIDED BAYESIAN OPTIMISATION FOR HUMAN-IN-THE-LOOP EXPERIMENTAL DESIGN OF KNOWN SYSTEMS",
    "section": "",
    "text": "Domain experts often possess valuable physical insights that are overlooked in fully automated decision-making processes such as Bayesian optimisation. In this article we apply high-throughput (batch) Bayesian optimisation alongside anthropological decision theory to enable domain experts to influence the selection of optimal experiments. Our methodology exploits the hypothesis that humans are better at making discrete choices than continuous ones and enables experts to influence critical early decisions. At each iteration we solve an augmented multi-objective optimisation problem across a number of alternate solutions, maximising both the sum of their utility function values and the determinant of their covariance matrix, equivalent to their total variability. By taking the solution at the knee point of the Pareto front, we return a set of alternate solutions at each iteration that have both high utility values and are reasonably distinct, from which the expert selects one for evaluation. We demonstrate that even in the case of an uninformed practitioner, our algorithm recovers the regret of standard Bayesian optimisation."
  },
  {
    "objectID": "posts/human_in_the_loop/index.html#abstract",
    "href": "posts/human_in_the_loop/index.html#abstract",
    "title": "EXPERT-GUIDED BAYESIAN OPTIMISATION FOR HUMAN-IN-THE-LOOP EXPERIMENTAL DESIGN OF KNOWN SYSTEMS",
    "section": "",
    "text": "Domain experts often possess valuable physical insights that are overlooked in fully automated decision-making processes such as Bayesian optimisation. In this article we apply high-throughput (batch) Bayesian optimisation alongside anthropological decision theory to enable domain experts to influence the selection of optimal experiments. Our methodology exploits the hypothesis that humans are better at making discrete choices than continuous ones and enables experts to influence critical early decisions. At each iteration we solve an augmented multi-objective optimisation problem across a number of alternate solutions, maximising both the sum of their utility function values and the determinant of their covariance matrix, equivalent to their total variability. By taking the solution at the knee point of the Pareto front, we return a set of alternate solutions at each iteration that have both high utility values and are reasonably distinct, from which the expert selects one for evaluation. We demonstrate that even in the case of an uninformed practitioner, our algorithm recovers the regret of standard Bayesian optimisation."
  },
  {
    "objectID": "posts/human_in_the_loop/index.html#introduction",
    "href": "posts/human_in_the_loop/index.html#introduction",
    "title": "EXPERT-GUIDED BAYESIAN OPTIMISATION FOR HUMAN-IN-THE-LOOP EXPERIMENTAL DESIGN OF KNOWN SYSTEMS",
    "section": "1 Introduction",
    "text": "1 Introduction\nBayesian optimisation has been successfully applied in a number of complex domains including engineering systems where derivatives are often not available, such as those that involve simulation or propriety software. By removing the human from decision-making processes in favour of maximising statistical quantities such as expected improvement, complex functions can be optimised in an efficient number of samples. However, these engineering systems are often engaged with by domain experts such as engineers or chemists, and as such the behaviour of the underlying function cannot be considered completely unknown a-priori. Therefore, there exists significant scope to take advantage of the benefits of Bayesian optimisation in optimising expensive derivative-free problems, whilst enabling domain experts to inform the decision-making process, putting the human back into the loop. By providing an optimal set of alternatives to an expert to select their desired evaluation, we ensure that any one choice presents information gain about the optimal solution. Simultaneously, we ensure the choices are distinct enough to avoid the human making an effective gradient calculation. Alternative solution information such as utility function value, predictive output distribution and visualisations are provided to the expert as a pseudo-likelihood. The decision-maker then effectively performs discrete Bayesian reasoning, internally conditioning the provided information with their own prior expertise and knowledge of the solutions provided. In addition to improved convergence (depending on the ability of the domain expert), our methodology enables improved interpretability in higher dimensions, as the decision-maker has the final say in what is evaluated. Our approach works with any utility function and NSGA-II (Deb et al. 2002) is applied for multi-objective optimisation, efficiently handling the non-convex utility-space.\nFigure¬†1 demonstrates our methodology\n\n\n\nFigure¬†1: Overview of our methodology, where an augmented batch Bayesian optimisation problem is solved using multi-objective optimisation, providing an expert with a set of alternate solutions.\n\n\nBy allowing an expert to influence the experimental design through a discrete decision step, we mitigate the expert needing to make continuous decisions throughout, and do not rely on an expert `prior‚Äô of the global optimum that necessarily must be defined before optimisation. Approaches that rely on an expert-defined prior may need to redefine this at significant human-cost throughout optimisation in light of new information. Similarly, the expert has no influence over the actual solutions evaluated and the optimisation is merely weighted towards broad regions in solution-space."
  },
  {
    "objectID": "posts/human_in_the_loop/index.html#previous-work",
    "href": "posts/human_in_the_loop/index.html#previous-work",
    "title": "EXPERT-GUIDED BAYESIAN OPTIMISATION FOR HUMAN-IN-THE-LOOP EXPERIMENTAL DESIGN OF KNOWN SYSTEMS",
    "section": "2 Previous Work",
    "text": "2 Previous Work\n(Kanarik et al. 2023) demonstrated that experts improve the initial convergence in Bayesian optimisation for semiconductor processes. However, this can be counterproductive in later stages. Algorithmic integration of expert knowledge has been explored Hvarfner et al. (2022). (Hvarfner et al. 2022) and (Ramachandran et al. 2020) use user-defined priors to weight the acquisition function, while (Liu 2022) diminishes this weight over time. These methods are static and don‚Äôt allow real-time expert input. (Gupta et al. 2023) allows continuous expert involvement, using a linear Gaussian process to approximate human intuition, achieving sub-linear regret bounds. (Kumar et al. 2022) and (Maus et al. 2022) present similar frameworks, offering alternative solutions for evaluation, with the expert making the final decision latterly in a molecular design setting."
  },
  {
    "objectID": "posts/human_in_the_loop/index.html#method",
    "href": "posts/human_in_the_loop/index.html#method",
    "title": "EXPERT-GUIDED BAYESIAN OPTIMISATION FOR HUMAN-IN-THE-LOOP EXPERIMENTAL DESIGN OF KNOWN SYSTEMS",
    "section": "3 Method",
    "text": "3 Method\nWe first maximise a given utility function \\(\\mathcal{U}\\) for a given dataset \\(\\mathcal{D}_t:= \\{(\\mathbf{x}_i,y_i)\\}_{i=1}^t\\):\n\n\\[\n\\DeclareMathOperator*{\\argmax}{arg\\,max}\n\\usepackage{amsmath,amsfonts,amssymb,algorithm,algorithmic}\n\\]\n\n\\[\n   \\mathbf{x}^* = \\argmax_{x\\in\\mathcal{X}\\subseteq\\mathbb{R}^n} \\; \\mathcal{U}(x),\n\\tag{1}\\]\nresulting in the optimal next evaluation, \\(\\mathbf{x}^*\\), in a utility sense. Let \\(p\\) be the number of alternate solutions provided to the expert and construct the decision variable matrix \\(\\mathbf{X} \\in \\mathbb{R}^{(p-1)\\times n}\\) by concatenating \\(p-1\\) alternate solutions \\(\\mathbf{X} := [\\mathbf{x}_1,\\dots,\\mathbf{x}_{p-1}]\\). We then define the high-throughput (batch) utility function \\(\\hat{\\mathcal{U}}\\) which is specified as the sum of the individual utilities of alternate solutions within \\(\\mathbf{X}\\)\n\\[\\begin{align}\n    \\hat{\\mathcal{U}}(\\mathbf{X}) = \\sum_{i=0}^{p-1} \\mathcal{U}(\\mathbf{X}_i).\n\\end{align}\\] Similarly, we introduce \\(\\hat{\\mathcal{S}}\\) as a measure for capturing the variability among both the optimal and alternative solutions. Specifically, let \\(\\hat{\\mathcal{S}}\\) be the determinant of the covariance matrix \\(K_{\\mathbf{X}_{\\text{aug}}}\\) for the augmented set \\(\\mathbf{X}_{\\text{aug}}= \\mathbf{X} \\cup \\mathbf{x}^*\\): \\[\\begin{align*}\n\\hat{\\mathcal{S}}(\\mathbf{X},\\mathbf{x}^*) &= |K_{\\mathbf{X_{\\text{aug}}}}| \\\\\nK_{\\mathbf{X}_{\\text{aug}}} &= [k(\\mathbf{X}_{\\text{aug},i},\\mathbf{X}_{\\text{aug},j})]^p_{i,j=1}\n\\end{align*}\\] \\(\\hat{\\mathcal{S}}\\) quantifies the ‚Äòvolume of information‚Äô spanned by the alternative solutions \\(\\mathbf{X}\\) as well as the optimal solution \\(\\mathbf{x}^*\\). Maximising \\(\\hat{\\mathcal{U}}\\) will result in all alternative solutions proposed being the same as \\(\\mathbf{x}^*\\), that is \\([\\mathbf{x}^*_1,\\dots,\\mathbf{x}^*_{p-1}]\\). Contrary to this, maximising \\(\\hat{\\mathcal{S}}\\) will result in a set of solutions that are maximally-spaced both with respect to other alternatives, but also \\(\\mathbf{x}^*\\). At iteration \\(t\\), we then solve the following multi-objective optimisation problem: \\[\\begin{align}\\label{multi-objective}\n    [\\mathbf{X}^*_1,\\dots,\\mathbf{X}^*_m] = \\max_{\\mathbf{X}} \\; \\left(\\hat{\\mathcal{U}}(\\mathbf{X};\\mathcal{D}_t),\\hat{\\mathcal{S}}(\\mathbf{X},\\mathbf{x}^*)\\right),\n\\end{align}\\] resulting in a set of \\(m\\) solutions along the Pareto front of both objectives. From this we define \\(\\mathbf{X}^*_{k}\\) as the solution at knee-point of the Pareto front. The \\(p-1\\) solutions contained within \\(\\mathbf{X}^*_k\\) optimally trade off the sum of their utility values, with their variability. This ensures that when provided to an expert, alongside \\(\\mathbf{x}^*\\), any individual solution will have high expected information gain, and the solutions themselves will be distinct enough to ensure the expert isn‚Äôt made to make an effective gradient calculation.\nThe practitioner is then made to choose a solution to evaluate from this set of alternatives. To do so, they are provided with information such as the utility value of each solution, expected output distributions (obtained from the Gaussian process), and information regarding previous solutions that they may wish to draw upon. In doing so, the practitioner effectively performs an internal discrete Bayesian reasoning, conditioning previous prior information and expert opinion with the mathematical quantities provided to make an informed decision. Our algorithm can be located within the Appendix.\nFigure¬†2 demonstrates the intended behaviour of our approach. We present a one-dimensional case study, optimising a function obtained through sampling a Gaussian process prior, specified by a Matern 5/2 kernel with lengthscale \\(l=0.5\\). In this case study we provide 3 alternatives to an expert, who‚Äôs choice we select randomly. We provide details of optimisation methods and hyper-parameters within the Appendix.\n\n\n\n\n\n\n\n(a) The objective function and utility function after 6 function evaluations. The 3 alternative solutions that maximise the solution distance can be seen in red, whilst the black solutions denote those contained within the knee-solution of the high-throughput multi-objective problem. The yellow optimal solution is included alongside these two alternatives to an expert. In this case choice 4 is selected randomly from the 3 alternatives and the optimum.\n\n\n\n\n\n\n\n\n\n(b) The information provided to the expert regarding the three alternative solutions. In this case choice 1 and choice 3 have relatively similar utility values and predicted output distributions to choice 4 (the optimal of the acquisition function). The expert is then allowed to distinguish between these similar solutions in a way the computer cannot through their prior domain knowledge. By conditioning their prior information on the given values, the expert is effectively performing internal Bayesian reasoning.\n\n\n\n\nFigure¬†2: A standard iteration of our approach on a one-dimensional case-study."
  },
  {
    "objectID": "posts/human_in_the_loop/index.html#computational-results-discussion",
    "href": "posts/human_in_the_loop/index.html#computational-results-discussion",
    "title": "EXPERT-GUIDED BAYESIAN OPTIMISATION FOR HUMAN-IN-THE-LOOP EXPERIMENTAL DESIGN OF KNOWN SYSTEMS",
    "section": "4 Computational Results & Discussion",
    "text": "4 Computational Results & Discussion\nTo assess our approach, we benchmark it against standard Bayesian optimisation. In order to incorporate and assess human interaction in an automated manner, we hypothesise a number of different human behaviours. The ‚ÄòExpert‚Äô practitioner represents an ideal, where the best solution (that is the one with the highest true objective value) is always selected. Equivalently, to test the performance of our approach under the influence of a practitioner with misaligned knowledge, we present an ‚ÄòAdversarial‚Äô practitioner who consistently selects the solution with the lowest true objective value. In addition, we present a probabilistic practitioner, who selects the solution with the best true objective value with some probability. Finally, we present the behaviour of a ‚ÄòTrusting‚Äô practitioner who selects the solution with the largest utility (as these values are presented), equivalent to standard Bayesian optimisation as this solution is obtained through standard single objective optimisation (Equation¬†1). In practice, the expert will condition the information provided with their prior beliefs. In our approach this includes information regarding the expected distribution of the objective of each solution, as well as the utility value of each solution. Whilst we cannot benchmark real human behaviour due to the random nature of the objective functions, and practical issues, the behaviours described summarise key aspects in order to generate useful insights into our approach, we leave this for future work. The human behaviours applied are summarised within Table¬†1.\n\n\nTable¬†1: Human Behaviours Applied for Benchmarking\n\n\n\n\n\n\nBehaviour Type\nDescription\n\n\n\n\nExpert\nSelects the solution with the best true function value.\n\n\nAdversarial\nSelects the solution with the worst true function value.\n\n\nTrusting\nSelects the solution with the maximum utility value.\n\n\np(Best)\nSelects the solution with the best true function value with probability p(Best), otherwise selects a random solution.\n\n\n\n\nWe perform optimisation over 50 functions, each representing a sample from a Gaussian process prior with lengthscale 0.3 using the upper-confidence bound (UCB) utility function. Figure¬†3 demonstrates the average and standard deviation of simple regret, and average regret (both defined within Garnett (2023)) for each human behaviour across 1D and 2D objective functions.\nResults for 5D, and specific functions can be located within the Appendix.\n\n\n\n\n\n\n\n(a) Average regret quantities over 1D objective functions.\n\n\n\n\n\n\n\n\n\n(b) Average regret quantities over 2D objective functions.\n\n\n\n\nFigure¬†3: Regret expectation over 50 functions, \\(f \\sim \\mathcal{GP}(\\mu \\equiv 0, K_M (d,\\nu = 0.3))\\) where \\(K_M\\) is the Mat'ern 5/2 kernel function. 4 alternate choices are presented to the practitioner, and the utility function \\(\\mathcal{U}(x)\\) used is the upper-confidence bound.\n\n\nThe expectation of average regret tends towards zero for all behaviours, indicating empirical convergence. Focusing on results from the set of 1D functions, an ‚ÄòExpert‚Äô provides improved convergence than standard Bayesian optimisation (‚ÄòTrusting‚Äô) throughout all iterations, with benefits diminishing throughout the later stages, where the standard automated approach recovers the ‚ÄòExpert‚Äô average regret value, confirming previous observations regarding the importance of human input throughout earlier iterations, and conversely diminishing importance of expert opinion during `fine-tuning‚Äô Kanarik et al. (2023). Improved convergence of simple regret occurs in cases where the practitioner selects the ‚Äòbest‚Äô solution from a set 75%, 50%, and to a lesser extent 25% of the time out of 4 available choices, similarly reflected in trends across average regret. The results demonstrated in Figure¬†3 indicate the potential for our approach to improve the convergence of Bayesian optimisation even in cases where the practitioner is correct about a decision only partially. When the expert makes a random selection (\\(p(\\text{Best}) = 0.25\\), for 4 alternate solutions), standard Bayesian optimisation convergence is recovered, indicating the effectiveness of the choices presented by asking a practitioner to select between distinct solutions, each of which individually has a high utility value. This is reflected throughout 1D, 2D and 5D functions. Only in the case where the practitioner actively selects the worst solution (i.e.¬†they are misaligned), is performance worse. In higher-dimensions an adversarial practitioner performs worse, as they are performing inefficient space-filling in an increasingly larger volume before good solutions are found.\nThe methodology we present may also be interpreted as an approach for high-throughput/batch Bayesian optimisation, with an additional preference for solutions that are well-distributed throughout the search space. Our intention for future work is to benchmark it against other existing batch Bayesian optimisation methodologies Gonz√°lez et al. (2015), including those with similar multi-objective formulations (Bischl et al. (2014), Habib, Singh, and Ray (2016), Maus et al. (2022)). We will also investigate to what extent large-language models can perform the selection step."
  },
  {
    "objectID": "posts/human_in_the_loop/index.html#appendix-a-algorithm",
    "href": "posts/human_in_the_loop/index.html#appendix-a-algorithm",
    "title": "EXPERT-GUIDED BAYESIAN OPTIMISATION FOR HUMAN-IN-THE-LOOP EXPERIMENTAL DESIGN OF KNOWN SYSTEMS",
    "section": "Appendix A (Algorithm)",
    "text": "Appendix A (Algorithm)\nAlgorithm¬†1 demonstrates our approach.\n\nAlgorithm 1"
  },
  {
    "objectID": "posts/human_in_the_loop/index.html#appendix-b-optimisation-details",
    "href": "posts/human_in_the_loop/index.html#appendix-b-optimisation-details",
    "title": "EXPERT-GUIDED BAYESIAN OPTIMISATION FOR HUMAN-IN-THE-LOOP EXPERIMENTAL DESIGN OF KNOWN SYSTEMS",
    "section": "Appendix B (Optimisation Details)",
    "text": "Appendix B (Optimisation Details)\nThroughout this paper we apply 4 alternative solutions at each iteration (one of which will be the optimum of the utility function). Throughout all optimisation problems we generate an initial sample of 4 experiments, distributed via a static Latin hypercube design-of-experiments. When training the Gaussian process that models the objective-space we perform a multi-start of 8 gradient-based optimisation runs using Adam with a learning rate of 1e-3 for 750 iterations to determine GP hyperparameters. To solve the multi-objective augmented optimisation problem, resulting in a Pareto set of sets of alternative solutions we run NSGA-II Deb et al. (2002) for 150 iterations, with a population size of 100, 30 offspring at each iteration, a 0.9 probability of crossover, and 20 mutations per iteration. To generate the single optimum of the acquisition function, we perform a multi-start of 36 gradient-based optimisation runs using L-BFGS-B Zhu et al. (1997) with a tolerance of 1e-12 and a maximum iterations of 500. For the 1D, 2D, and 5D expectations over functions stemming from samples of a Gaussian process prior (with lengthscale 0.3), the lower and upper bounds used are 0 and 10 respectively for each dimension. All other bounds for functions can be located at http://www.sfu.ca/~ssurjano/optimization.html"
  },
  {
    "objectID": "posts/human_in_the_loop/index.html#appendix-c.-regret-plots",
    "href": "posts/human_in_the_loop/index.html#appendix-c.-regret-plots",
    "title": "EXPERT-GUIDED BAYESIAN OPTIMISATION FOR HUMAN-IN-THE-LOOP EXPERIMENTAL DESIGN OF KNOWN SYSTEMS",
    "section": "Appendix C. (Regret Plots)",
    "text": "Appendix C. (Regret Plots)\n\n\n\nFigure¬†4: Regret expectation over 50 5D functions (\\(d=5\\)), \\(f \\sim \\mathcal{GP}(\\mu \\equiv 0, K_M (d,\\nu = 0.3))\\) where \\(K_M\\) is the Matern 5/2 kernel function. 4 alternate choices are presented to the practitioner, and the utility function \\(\\mathcal{U}(x)\\) used is the upper-confidence bound.\n\n\nWe subsequently present the expectation of regret for a number of functions across all human behaviours. We present 4 alternate choices are presented to the hypothetical practitioner, and the utility function \\(\\mathcal{U}(x)\\) used is the upper-confidence bound. Each function is optimised 16 times, across a number of random initialisations, and average regret quantities are plotted. Optimisation details are presented in the previous Appendix section.\n\n\nClick to expand additional regret plots\n\n\n\n\nFigure¬†5: 2D Ackley Function\n\n\n\n\n\nFigure¬†6: 5D Ackley Function\n\n\n\n\n\nFigure¬†7: 10D Ackley Function\n\n\n\n\n\nFigure¬†8: 2D Griewank Function\n\n\n\n\n\nFigure¬†9: 5D Griewank Function\n\n\n\n\n\nFigure¬†10: 10D Griewank Function\n\n\n\n\n\nFigure¬†11: 5D Powell Function\n\n\n\n\n\nFigure¬†12: 2D Rastrigin Function\n\n\n\n\n\nFigure¬†13: 5D Rastrigin Function\n\n\n\n\n\nFigure¬†14: 10D Rastrigin Function\n\n\n\n\n\nFigure¬†15: 10D Rosenbrock Function"
  },
  {
    "objectID": "posts/gazza_agonistes/index.html",
    "href": "posts/gazza_agonistes/index.html",
    "title": "BOOK REVIEW - GAZZA AGONISTES, IAN HAMILTON",
    "section": "",
    "text": "There is no discernible anniversary associated to this review of Gazza Agonistes. The book was first published 29 years ago and describes the events surrounding the life of footballer Paul Gascoigne throughout the 1990s. The author, literary critic and poet Ian Hamilton passed away in 2001. However, his depiction of the rise and fall of a talented man is timeless and universal. Hamilton writes from the perspective of a lifelong fan, willing Gazza up the field from behind the pen, a ‚Äòself-styled Gazzamane‚Äô. He celebrates the highs with literary flourish, and reports on the lows with all the authority of a disapproving parent.\n\n\n\n\n\nIan Hamilton - Collected Poems: Fair use, Link\n\n\nI was first made aware of Gazza Agonistes via a series of recent tweets showing footballers gracing the covers of the London Review of Books throughout the 1980s, with titles such as ‚ÄòSocial democracy, Sociology, Soccer‚Äô. Karl Miller, the editor at the time pioneered as the Guardian put in his obituary, ‚Äòa distinguished style of soccer journalism‚Äô. Gazza Agonistes is perfectly placed within this subgenre and seems so far away from the current state of football critique, consisting largely of Mark Goldbridge looking sad.\nWriting in 1994 (with a 1998 afterword written after Gazza‚Äôs exclusion from the World Cup squad that year), Hamilton describes being attracted to Gazza‚Äôs ‚Äúlegendary scampers‚Äù whilst at he played for his childhood club Newcastle, later moving to Tottenham, Hamilton‚Äôs childhood club. This transaction of loyalties, from one man‚Äôs sporting allegiances to another, provides Hamilton with the emotional license to report on Gazza in a way that is distinct from the tabloid journalists that plague the footballer‚Äôs career.\n\n\n\n\n\nPaul Gascoigne and Vinnie Jones\n\n\nGazza Agonistes highlights just how much the culture of football has changed throughout the last 20 years. Players were expected to take accountability for their own actions; justifications for drunken nights were laundered by the tabloids and aired out to dry in the court of public opinion. The concept of the beautiful game seemed much more tangible when those involved talked and behaved just like those watching in the stands. Gazza would go to the pub with his mates like us, end up in fights like us, and famously shed a tear like us. There is an argument against this level of scrutiny. It destroyed Paul Gascoigne; why should people earning a living have to answer for their actions outside of their work?\nFor better or for worse, player engagements on social media and with journalists are now heavily vetted. Clubs don‚Äôt want to risk anything that could undermine their own image, and in turn their net worth. Owners are looking to sell at reputational highs, whilst fans want owners to leave when their clubs are at their lowest. Who knows what Hamilton would think of the players who are expected to answer for nation-states, and how the Gazza of then would respond to allegations of sports-washing.\n\n\n\n\n\nNessun Dorma performed by Andrea Botcelli after Leicester won the 2015/16 Premier League, a song forever associated with the beautiful game following the 1990 world cup.\n\n\nGazza Agonistes is a reminder of a simpler, more passionate footballing time; best enjoyed whilst listening to the soundtrack of the 1990 Italian World Cup Nessun Dorma. For all of Gazza‚Äôs mistakes, and the treatment he endured, his epic struggles will not be repeated any time soon, and neither will an account as poetic as Hamilton‚Äôs."
  },
  {
    "objectID": "posts/aiche/index.html",
    "href": "posts/aiche/index.html",
    "title": "AICHE 2023 & POLAR GAUSSIAN PROCESSES",
    "section": "",
    "text": "Yankee Hotel Foxtrot - Wilco\n\n\n\nThis AIChE was in Orlando. However, due to a delay at US customs during a layover at Chicago O‚ÄôHare I was prescribed the unfortunate detour of an evening in downtown Chicago.\nI begrudgingly recreated the cover of one of my favourite albums as well as ate deep dish pizza and the like‚Ä¶ However after arriving in Orlando I presented three talks:\n\n\n\n‚Äã\nA Data-Driven Framework for the Design of Reactor Simulations: Exploiting Multiple Continuous Fidelities: Data Driven Optimisation\nIn this talk I outlined the problem formulation and methodology for the optimisation of coiled-tube reactor geometry.\nAdditive manufacturing has enabled the creation of reactors with non-traditional geometries, so to take advantage of this we parameterise the wall and path of a coiled tube reactor and formulate a large, expensive derivative free/black box optimisation problem.\nBecause the closed loop procedure of meshing and simulating a given geometry is so expensive we formulate it as a multi-fidelity Bayesian optimisation problem with two independent continuous fidelities.\n\n\n\n\n\n TL;DR we use the insights generated via optimisation to design two new flow reactors. We experimentally validate these and demonstrate improved plug-flow behaviour at low Reynolds numbers under steady-flow conditions.\n\n\n\n\n\nFor the full read take a look at the preprint. (Savage et al. 2023) Thanks to my co-authors: Nausheen, who recently won the IMechE Process Industries Division Most Distinguished Developing Career Achievement Prize, and Jonathan who without which the 3D printing and validation wouldn‚Äôt have been possible, as well as Prof Omar Matar and Dr Antonio del Rio Chanona.\n\n\n\n\n\nMe presenting the talk. Spot the 3D printed reactor on the table.\n\n\n\n\nExtra polar Gaussian process content and shoutout to GPJax\n\n‚Äã\nAs part of this work I parameterised the walls of a coiled tube reactor using a series of polar Gaussian Processes, allowing me to shift and warp the wall whilst ensuring it remains closed and continuous (AKA no leaks!). As part of this I ended up contributing to the open-source GPJax library which has been my library of choice for Gaussian processes for the last year.\nPolar Gaussian processes work by applying a kernel function that operates using modulo-arithmatic. This ensures that as opposed to a data point at \\(\\theta=0\\) and \\(\\theta=2\\pi\\) having zero correlation, they infact have perfect correlation. In short, the correlation between data ‚Äòwraps around‚Äô to the beginning.\nIf we define a standard covariance function such as the Matern function, and simply plot samples from a respective Gaussian process on a polar axis, the results are clearly not continuous in the polar domain.\n\nimport matplotlib.pyplot as plt\nimport numpy as np \n\ndef matern(t1,t2):\n  d = 2*(t1-t2)**2; p = 2\n  return (1+(np.sqrt(3)*d)/p) * np.exp(-(np.sqrt(3)*d)/p)\n\ndef plot_samples(k):\n  n = 100\n  theta = np.linspace(0,2*np.pi,n)\n  covariance_matrix = np.array([[k(theta[i],theta[j]) for i in range(n)] for j in range(n)])\n  mean_function = np.array([1 for i in range(n)])\n\n  fig,ax = plt.subplots(1,1,subplot_kw={'projection': 'polar'})\n  ax.set_rticks([]); ax.set_rlim(-4,4)\n  cols = ['tab:blue','tab:green','tab:red','tab:orange']\n  for i in range(4):\n    prior_sample = np.random.multivariate_normal(mean_function,covariance_matrix)\n    ax.plot(theta,prior_sample,c=cols[i])\n\nplot_samples(matern)\n\n\n\n\nSamples from a non-polar Gaussian Process\n\n\n\n\nThe samples are not closed because our kernel function believes that \\(2\\pi\\) and \\(0\\) are very ‚Äòfar away‚Äô, when in fact they are perfectly correlated. A polar kernel function should reflect this fact:\n\ndef polar(t1,t2):\n  tau = np.pi\n  # angular distance\n  d = abs(((t1-t2+np.pi) % (2*np.pi)) - np.pi)\n  return (1 + tau * d / np.pi) * np.clip(1 - d / np.pi, 0, np.inf) ** tau\n\nplot_samples(polar)\n\n\n\n\nSamples from a polar Gaussian Process\n\n\n\n\nAs we can see, samples from this Gaussian process are continuous in the polar domain! We can then condition as we normally would on data.\nIn the case of defining the cross-section, we choose to exactly interpolate between 6 equally spaced points. The resulting posterior mean function then defines the cross-section.\n\n\n\n\n‚Äã\nMulti-Fidelity Data-Driven Design and Analysis of Reactor and Tube Simulations: Data Science for Complex Fluids and Complex Flows\nIn this talk I discussed the flows that we induced through the optimisation of plug-flow performance.\nDespite only specifying that we wanted the residence time distribution to be as tight and symmetric as possible, we found out that our optimised designs specifically induced Dean vortices at low Reynolds numbers.\n\n\n  Early formation of Dean vortices induced in our optimal design.\n\n\n\n\nSummary slide for the presentation Multi-Fidelity Data-Driven Design and Analysis of Reactor and Tube Simulations at AIChE 2023.\n\n\n\n\nDesigning Robust Energy Policies for Low-Carbon Technology Adoption: Design, Analysis, and Optimization of Sustainable Energy Systems and Supply Chains I\n\nThis talk was highlighting work completed with Dr Gbemi Olueye at the Imperial College Centre for Environmental Policy.\nThere are a number of really great energy policy optimisation problems that include aspects such as the relationship between technology uptake and cost (via a learning rate). Due to obvious reasons broadly concerned with ‚Äòthe future‚Äô it is important to account for uncertainty.\nLarge scale uncertainties concerning ‚Äòthe future‚Äô (which I differentiate from smaller ‚Äòstochastic‚Äô uncertainties) are often specified by upper and lower bounds. To deal with these we formulate and solve a robust optimisation problem using a parallelised cutting planes approach. This enables us to:\n\nMaintain the original nonlinear energy policy problem\nSolve the problem (approximately) robustly really fast, enabling us to experiment with uncertainty.\n\nIt‚Äôs this experimentation that is enabled by the methodology that enables us to provide trade-off graphs to policy makers as the solution itself. Something that I argue should always be done for this class of problem, given the political mandate we as engineers and scientists do not have.\nFor a full overview, click here for the preprint. (Savage, Chanona, and Oluleye 2023)\n\n\n\n‚Äã\nThat is about it for Orlando. It was great to catch up with some old colleagues from Cambridge, and some new ones. See you next year in San Diego!\n\n\n\n\n\nReferences\n\nSavage, Tom, Nausheen Basha, Jonathan McDonough, Omar K Matar, and Ehecatl Antonio del Rio Chanona. 2023. ‚ÄúMachine Learning-Assisted Discovery of Novel Reactor Designs.‚Äù arXiv. https://doi.org/10.48550/ARXIV.2308.08841.\n\n\nSavage, Tom, Antonio del Rio Chanona, and Gbemi Oluleye. 2023. ‚ÄúRobust Market Potential Assessment: Designing Optimal Policies for Low-Carbon Technology Adoption in an Increasingly Uncertain World.‚Äù arXiv. https://doi.org/10.48550/ARXIV.2304.10203."
  },
  {
    "objectID": "posts/thatcher_review/index.html",
    "href": "posts/thatcher_review/index.html",
    "title": "BOOK REVIEW - KILLING THATCHER, RORY CARROLL",
    "section": "",
    "text": "Maggie‚Äôs potential assassin, ironically named Magee is presented as someone who can‚Äôt catch a break. He has a troubled relationship with his wife, is arrested countless times, and it goes without saying, didn‚Äôt kill Thatcher.\n\n\n\n\n\nPatrick Magee : PA Images / Alamy Stock Photo. Photographer: John Giles. Taken on 22 March 1999, BELFAST UK.*\n\n\nCarroll‚Äôs detailed account of his whereabouts in England are interspersed with Magee‚Äôs own reflections later on in life, which are characteristically dry: ‚Äúmy life was too chaotic‚Äù. Carroll does well to chip away and reveal a portrait of a man who presented himself deliberately dull enough that no one seems to remember what he looks like (useful if you have attempted to assassinate a sitting prime minister). The bleak reality of being an anonymous IRA operative in England is contrasted with the historical implications of the task at hand. Guy Faulks planted a bomb under the Houses of Parliament that didn‚Äôt even go off, and people burn an effigy of him every year. The events of the book seem surreal to those born after the troubles; ‚Äúone of the great what ifs‚Äù that Thatcher herself rarely chose to comprehend.\n\n\n\n\n\nThe Grand Hotel, Brighton. D4444n at the English Wikipedia, Public domain, via Wikimedia Commons\n\n\nCarroll dwells on the turning points of the era, such as the death of Bobby Sands and the rise of Gerry Adams‚Äô Armalite / Ballot Box strategy, providing the relevant context for someone unfamiliar with the period. However, by focussing largely on the ‚ÄòEngland Department‚Äô and specifically a single operation, these public-facing protagonists are side-lined in favour of a different type of history. One that is populated with normal people: students, office workers and landlords. These accounts provide important insight into the human cost of the troubles on both sides. Light relief comes somewhat unsurprisingly not from the members of the IRA (and Gerry Adams), but from the British detectives charged with locating Magee. The tangle of regional agencies searching for Magee as he drifts throughout the UK, whose jurisdiction changes every other page, results in a thrilling chase of comically bureaucratic proportions.\nMaybe it‚Äôs too soon for a screen adaptation of these events, Carroll has been sensitive with his accounts and sources (with obvious reason), and wounds are still fresh, particularly those of Thatcher‚Äôs then ‚Äúloyal enforcer‚Äù Norman Tebbit whose wife was paralysed by the attempt. However, the book is an account of a major event in modern British history that is genuinely hard to put down. Following the success of The Crown with its relentless march from the past to the present, and the Happy Valley genre of regional crime drama, Killing Thatcher has all the hallmarks of a hit series. Regardless of how people judge the event, Carroll has provided a valuable account, to remember."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TRS",
    "section": "",
    "text": "BAYESIAN OPTIMISATION WITH NOISY MEASUREMENTS\n\n\nA REVIEW\n\n\n\n\n\n\nDec 5, 2023\n\n\nTom Savage\n\n\n\n\n\n\n\n\nBASILISK RECIPES\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nTom Savage\n\n\n\n\n\n\n\n\nON THE IDENTIFICATION OF A VORTEX\n\n\nNOTES\n\n\n\n\n\n\nDec 4, 2023\n\n\nTom Savage\n\n\n\n\n\n\n\n\nELECTRONIC LIFE\n\n\nLATE @ TATE BRITAIN\n\n\n\n\n\n\nNov 29, 2023\n\n\nTom Savage, Sunil Manghani, Ed D‚ÄôSouza\n\n\n\n\n\n\n\n\nAICHE 2023 & POLAR GAUSSIAN PROCESSES\n\n\n\n\n\n\n\n\n\nNov 12, 2023\n\n\nTom Savage\n\n\n\n\n\n\n\n\nEXPERT-GUIDED BAYESIAN OPTIMISATION FOR HUMAN-IN-THE-LOOP EXPERIMENTAL DESIGN OF KNOWN SYSTEMS\n\n\nNeurIPS workshop on Modern Experimental Desisgn and Active Learning in the Real World\n\n\n\n\n\n\nOct 27, 2023\n\n\nTom Savage, Antonio del Rio Chanona\n\n\n\n\n\n\n\n\nCONVERGENCE PLOTS\n\n\n\n\n\n\n\n\n\nOct 24, 2023\n\n\nTom Savage\n\n\n\n\n\n\n\n\nBOOK REVIEW - GAZZA AGONISTES, IAN HAMILTON\n\n\nPublished in The Felix October 2023\n\n\n\n\n\n\nOct 22, 2023\n\n\nTom Savage\n\n\n\n\n\n\n\n\nBOOK REVIEW - KILLING THATCHER, RORY CARROLL\n\n\nPublished in The Felix October 2023\n\n\n\n\n\n\nOct 21, 2023\n\n\nTom Savage\n\n\n\n\n\n\nNo matching items"
  }
]