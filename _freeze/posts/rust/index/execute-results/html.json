{
  "hash": "6b2c499dd8108a347a3d7d8fdc1fe854",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Implementing Gradient Descent in Vanilla Rust\"\nsubtitle: \"Probably not reccomended\"\ndate: \"07/29/2024\"\ncomments:\n    utterances:\n        repo: trsav/trsav.github.io\nhtml: \n    self-contained: true\n    grid: \n    margin-width: 450px\n    css: styles.css\n---\n\n\n:::{.cleanbox}\nI don't think this is really reccomended. Python is a lot more established for scientific programming. \nAs a researcher you want to quickly prototype a method, or approach to solving a problem. Rust does not let you do this. \nThe only reason I've done the following is because:\n\n1. I have implemented similar algorithms in almost every language I have ever learned.\n2. It is a means to an end, to learn Rust's type and memory management system.\n\nAnd with that, we begin...\n:::\n\nI'll first start with the very basics, defining the function to be optimized, one of the best functions of all time...the Rosenbrock function.\n\nIn Python this would be written as follows:\n\n```python\ndef f(x):\n    return sum(100 * (x[i+1] - x[i]**2)\\\n        **2 + (1 - x[i])**2 for i in range(len(x) - 1))\n```\n\nIn Rust we need to define types for what goes in and out of the function to make sure the compiler can perform type checking and ensure memory safety at compile time.\n\nWe set the output value to be a mutable float, which we add to. In Rust we omit the semicolon on the last line, allowing sum to be implicitly returned as the function's output.\n\n```rust\nfn f(x: Vec<f64>) -> f64 {\n    let mut sum = 0.0;\n    for i in 0..x.len() - 1 {\n        sum += 100.0 * (x[i + 1] - x[i].powi(2)).powi(2) + (1.0 - x[i]).powi(2);\n    }\n    sum\n}\n```\n\nNow I'll define an initial solution and evaluate this, printing the output of the function. This is simple enough we just need to ensure that our initial solution has the right type. \n\nWe'll make `x` mutable because we only care about its current state during optimization, not its history. This allows us to modify it in-place, potentially saving memory and computational overhead.\n\n```rust\nfn main() {\n    let mut x: Vec<f64> = vec![1.0, 2.0, 3.0, 4.0, 5.0];\n    let f_x = f(x); // evaluate function\n    println!(\"f(x) = {}\",f_x);\n}\n```\n\n```zsh\n$ cargo run\n$ f(x) = 14814\n```\n\n`vec!` is a macro that creates a new vector with specified values without having to push each element individually.\n\nA key aspect that we'll need is a basic gradient calculation. For now I'll implement the forward-difference method. Though this could be the central difference, backward-difference, etc...\n\n```rust\nfn forward_difference(f:fn(Vec<f64>) -> f64,x: Vec<f64>) -> Vec<f64>{\n    let h = 1e-8; // pertubation\n\n    // initialise mutable gradient vector with zeros\n    let mut grad = vec![0.0; x.len()];\n\n    // evaluate function \n    let f_x = f(x.clone());\n\n    for i in 0..x.len() {\n        // define a new solution vector\n        let mut x_forward = x.clone();\n        // perturb \n        x_forward[i] += h;\n        // calculate gradient in this direction\n        grad[i] = (f(x_forward)-f_x)/h;\n    }\n    grad // return grad \n}\n```\n\nThis takes a vector of floats and returns another vector of floats (the jacobian). \nWe use `x.clone()` for the initial function evaluation because Rust's ownership rules would otherwise move `x` into the function `f`, making it unavailable for subsequent use in the loop.\n\nNow I am going to implement a backtracking linesearch to calculate the optimal step-size at each gradient step. We need to define a few constants.\n\n```rust\nfn line_search(f:fn(Vec<f64>) -> f64, x: Vec<f64>, grad: Vec<f64>) -> f64 {\n\n    // linesearch constants\n    const TAU: f64 = 0.5;\n    const C: f64 = 0.5;\n\n    // initialize maximum stepsize\n    let mut a_j: f64 = 0.001;\n    \n    // calculate gradient dot product\n    let m: f64 = grad.iter().map(|&grad_i| grad_i*grad_i).sum();\n\n    let t = - C * m;\n    let f_x = f(x.clone());\n\n    // until condition...\n    loop {\n\n        // create an incremented x along gradient\n        let x_inc: Vec<f64> = x.iter().zip(grad.iter())\n            .map(|(&x_i, &grad_i)| x_i - a_j * grad_i)\n            .collect();\n        \n        // if a sufficient descent...\n        if f(x_inc) <= f_x - a_j * t {\n            return a_j;\n        }\n    \n        // else make the stepsize smaller\n        a_j *= TAU;\n        if a_j < 1e-10 {  // Prevent infinite loop\n            return a_j;\n        }\n    }\n}\n```\nThere's a fair bit going on here. Firstly, constants are defined using capital letters alongside their type annotation. \n\nThe dot product is calculated by iterating over elements within `grad`. We first map over all elements and multiply them by themselves. Then we sum the resulting vector.\n\n```rust\ngrad.iter().map(|&grad_i| grad_i*grad_i).sum()\n```\n\nTo iterate over multiple vectors at the same time we can zip them together as follows:\n\n```rust\nx.iter().zip(grad.iter())...\n```\n\nFinally, when we increment `x` along the gradient direction, we use `.collect()`, this converts the iterator which is returned from `.map` back into a `Vec<f64>`.\n\nNow all that's left is to put it all together, we'll include a little timing function \n\n```rust\n\nfn main() {\n\n    // start timer \n    let now = Instant::now();\n    \n    // tolerance on gradient magnitude \n    const TOL: f64 = 1e-6;\n\n    // initial solution \n    let mut x: Vec<f64> = vec![0.0,0.0,0.5,0.0,1.0];\n    let mut grad: Vec<f64> = forward_difference(f,x.clone());\n\n    // gradient magnitude\n    let mut abs_sum: f64 = grad.iter().map(|&grad_i| grad_i.abs()).sum();\n\n    // while this is above the tolerance\n    while abs_sum >= TOL {\n\n        // perform line search  \n        let a = line_search(f,x.clone(), grad.clone());\n\n        // calculate new solution\n        x.iter_mut().zip(grad.iter()).for_each(|(x_i, &grad_i)| *x_i -= a * grad_i);\n\n        // calculate new gradient\n        grad = forward_difference(f,x.clone());\n\n        // calculate new gradient magnitude \n        abs_sum = grad.iter().map(|&x| x.abs()).sum();\n    }\n    println!(\"{:?}\",x); // final solution\n    let elapsed = now.elapsed(); // elapsed time\n    println!(\"Elapsed: {:.2?}\", elapsed); \n}\n\n```\n\nThis doesn't really use anything dissimilar to what I've previously mentioned, but it means we can evaluate how long it takes to run. \n\n```zsh\n$ cargo run\n\n[0.9999993984338057, 0.9999988039652051, 0.9999976181857124, 0.9999952424415648, 0.9999904773199286]\nElapsed: 93.28ms\n```\n\nWe get the correct answer in 93ms. For peace of mind, I used Claude Sonnet 3.5 to translate the Rust code into standard Python and performed the same benchmark.\n\n::: {#e309b0f1 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show Python Code\"}\nimport time\n\ndef f(x):\n    return sum(100 * (x[i+1] - x[i]**2)**2 + (1 - x[i])**2 for i in range(len(x) - 1))\n\ndef forward_difference(f, x):\n    h = 1e-8\n    grad = [0.0] * len(x)\n    f_x = f(x)\n    for i in range(len(x)):\n        x_forward = x.copy()\n        x_forward[i] += h\n        grad[i] = (f(x_forward) - f_x) / h\n    return grad\n\ndef line_search(f, x, grad):\n    TAU = 0.5\n    C = 0.5\n    a_j = 0.001\n    m = sum(grad_i**2 for grad_i in grad)\n    t = -C * m\n    f_x = f(x)\n    while True:\n        x_inc = [x_i - a_j * grad_i for x_i, grad_i in zip(x, grad)]\n        if f(x_inc) <= f_x - a_j * t:\n            return a_j\n        a_j *= TAU\n        if a_j < 1e-10:  # Prevent infinite loop\n            return a_j\n\ndef main():\n    start_time = time.time()\n    TOL = 1e-6\n    x = [0.0,0.0,0.5,0.0,1.0]\n    grad = forward_difference(f, x)\n    abs_sum = sum(abs(g) for g in grad)\n    iteration = 0\n    \n    while abs_sum >= TOL:\n        iteration += 1\n        a = line_search(f, x, grad)\n        x = [x_i - a * grad_i for x_i, grad_i in zip(x, grad)]\n        # print(f\"Iteration {iteration}: abs_sum = {abs_sum}: alpha = {a}\")\n        grad = forward_difference(f, x)\n        abs_sum = sum(abs(g) for g in grad)\n    \n    print(x)\n    elapsed_time = time.time() - start_time\n    elapsed_time_in_ms = elapsed_time * 1000\n    print(f\"Elapsed: {elapsed_time_in_ms:.2f} ms\")\n\nif __name__ == \"__main__\":\n    main()\n```\n:::\n\n\n```zsh\n$ python main.py\n\n[0.9999993984338057, 0.9999988039652051, 0.9999976181857124, 0.9999952424415648, 0.9999904773199286]\nElapsed: 219.83 ms\n```\n\nWe get exactly the same answer, which is an indication that the code translation has worked, and we get this in 220 ms, about 2.5 times slower than Rust. This is sort of a given, as Rust is a compiled langauge, but it's good to see. \n\n\n### Flamegraph\n\nTo benchmark code in Rust we can use Flamegraph to analyse where our gradient descent algorithm is spending most of its time. Flamegraph provides a visualization of the call stack over time, showing which functions are consuming the most CPU cycles.\n\n```zsh\n$ cargo flamegraph\n```\n\n![](flamegraph.svg)\n\nThese are interactive SVGs which are nice. In this case, given the simplicity and short runtime of our gradient descent implementation, the Flamegraph might not be particularly informative. Most of the time is likely spent in the core computational functions (`f`, `forward_difference`, and `line_search`), without much depth to the call stack.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}