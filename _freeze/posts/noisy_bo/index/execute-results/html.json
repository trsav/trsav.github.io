{
  "hash": "b3a18d6cd3d42943f76f5653bdbfd17b",
  "result": {
    "markdown": "---\ntitle: \"BAYESIAN OPTIMISATION WITH NOISY MEASUREMENTS\"\nsubtitle: \"A REVIEW\"\nauthor: \n - name: \"Tom Savage\"\ncategories: [optimisation, machine learning]\ndate: \"12/05/2023\"\ncallout-appearance: minimal\nformat:\n  html:\n    css: styles.css\nbibliography: ref.bib\n---\n\nI have reworked some notes from @garnett_bayesoptbook_2023 regarding noisy-expected improvement. Please refer to the original text for a more detailed explanation.\n\n:::{.cleanbox}\nThe assumptions that hold for the expected improvement utility function do not hold when measurements have noise. \n\nWe really want to find the point where the _signal_ is optimised [@Jones1998].\n\nHow do you determine if a measurement is signal or noise?\n:::\n\nWe begin by specifying an underlying objective function, which we consider unknown, shown in @fig-underlying. We have access to observations which we assume contain normally distributed noise. \n\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Definition of underlying function...\"}\ndef f(x):\n  return 2.4*np.sin(2.8*x) - ((x-3.5*np.pi) ** 2)/4 +  3.8*np.cos(1.7*x) - (x**2)/16 + np.random.normal(0,2)\n```\n:::\n\n\n::: {.cell fig-width='50%' execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![The underlying noisy funtion to be maximised alongside example observations.](index_files/figure-html/fig-underlying-output-1.png){#fig-underlying width=634 height=184 fig-align='center'}\n:::\n:::\n\n\nWe first begin by motivating the usual expected-improvement criteria. \nWe are looking to maximise the <span style=\"color:red;\">expected</span> increase between the of the <span style=\"color:green;\">maximum of the mean of the resulting Gaussian process after making an observation at $x$</span> and the <span style=\"color:blue;\">maximum of the current Gaussian process</span> <span style=\"color:red;\">over potential observations $y$</span> which are Gaussian distributed as a result of our $\\mathcal{GP}$ model.\n$$\\alpha_{EI}(x;\\mathcal{D}) = {\\color{red}\\int} \\left[{\\color{green}\\max \\mu_{\\mathcal{D}'}} - {\\color{blue}\\mu^*}\\right]{\\color{red}\\mathcal{N}(y;\\mu,s^2)\\text{d}y}$$ {#eq-ei_noisy}\n\nBy formulating the expression with respect to the improvement in the mean of the current and subsequent Gaussian process models, **we are expressing our interest in optimising the signal** and not the noise or values of specific observations.\n\n:::{.cleanbox}\n**Important:**\nGiven a hypothetical observation $y$, the value of the mean of the resulting Gaussian process $\\mu_{\\mathcal{D}'}$ at given set of potential locations $\\mathbf{x}'$ is \n\n$$ \\mu_{\\mathcal{D}'} =  \\mu_{\\mathcal{D}} + \\frac{K_\\mathcal{D}(\\mathbf{x}',x)}{s}\\frac{y-\\mu}{s},$$\n\nwhere $\\mu$ and $s^2$ are the mean and standard deviation of the distribution of potential values $y$ could take. \n:::\n\nWhen we express this distribution in terms of the standard normal distribution $z := \\mathcal{N}(0,1)$, we have $y = \\mu + sz$ and as a result\n\n$$ \\mu_{\\mathcal{D}'} =  \\mu_{\\mathcal{D}}(\\mathbf{x}') + \\frac{K_\\mathcal{D}(\\mathbf{x}',x)}{s}z.$$\n\nPutting this back into @eq-ei_noisy, we now only have to take the expectation over the standard normal distribution resulting in the following. \n\n$$\\alpha_{EI}(x,\\mathcal{D}) = \\int \\max_{\\mathbf{x}'} \\left(\\mu_{\\mathcal{D}}(\\mathbf{x}') + \\frac{K_\\mathcal{D}(\\mathbf{x}',x)}{s}z \\right)\\phi(z)\\;\\text{d}z - \\mu^*$${#eq-ei_noisy2}\n\nTo begin with we will sample some initial data:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# our initial dataset\nx_data = np.linspace(0,3*np.pi,24)\ny_data = np.array([f(x_i) for x_i in x_data])\n```\n:::\n\n\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"GP model definition and training...\"}\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n    \n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\ndef build_GP(x,y,its):\n  likelihood = gpytorch.likelihoods.GaussianLikelihood()\n  model = ExactGPModel(x, y, likelihood)\n\n  model.train()\n  likelihood.train()\n\n  optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n\n  mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\n  for i in range(its):\n      optimizer.zero_grad()\n      output = model(x)\n      loss = -mll(output, y)\n      loss.backward()\n      optimizer.step()\n  return model,likelihood\nmodel,likelihood = build_GP(x_data,y_data,2000)\nGP = {'model':model,'likelihood':likelihood}\n```\n:::\n\n\n@fig-initial shows this data with an initial Gaussian process (importantly assuming in-exact observations).\n\n::: {.cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![A Gaussian process fit to the initial dataset.](index_files/figure-html/fig-initial-output-1.png){#fig-initial width=666 height=206 fig-align='center'}\n:::\n:::\n\n\nNow we will naively construct @eq-ei_noisy2 using 40 samples from the standard normal distribution, and naively calculate an approximation to the inner maximisation by evaluating at 100 location throughout the space (which defines our vector $\\mathbf{x}' \\in\\mathbb{R}^{100}$) and returning the max value from these. \n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndef noisy_EI(x,GP):\n  model = GP['model']\n  likelihood = GP['likelihood']\n  N = torch.distributions.Normal(0,1)\n  predicted_output = likelihood(model(x))\n  mean = predicted_output.mean\n  var = predicted_output.variance\n  z_vals = torch.linspace(-2,2,40)\n  integral = 0 \n  for z in z_vals:\n    x_prime_vals = torch.linspace(0,3*np.pi,100).reshape(-1,1)    \n    new_mean_vals = model.mean_module(x_prime_vals)\n    new_cov_vals = ((model.covar_module(x_prime_vals,x)/var)*z.item())[:,0]\n    integral += torch.max(new_mean_vals + new_cov_vals)\n  integral /= 100\n  return -integral\n```\n:::\n\n\nNow if we plot this function alongside the time taken to evaluate the acquisition function at 100 locations.\n\n::: {.cell execution_count=9}\n\n::: {.cell-output .cell-output-stdout}\n```\nTIME TAKEN:  0.9433 s. AVERAGE TIME PER EVALUATION:  0.0094 s\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-2.png){width=634 height=206 fig-align='center'}\n:::\n:::\n\n\n<!-- \n:::{.cleanbox}\n**Gaining an intuition:** Let's create a number of example Gaussian process models and see what the acquisition function looks like for each. \n\n1. Constant GP with a constant noise.\n\n::: {.cell execution_count=10}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=626 height=318 fig-align='center'}\n:::\n:::\n\n\nThe acquisition function is maximised at the center of the domain, with an approximately linear decrease either side. \nIf we assume that $\\mu_{\\mathcal{D}}(x) = 0$ and $\\sigma_{\\mathcal{D}}(x) = 1$, the acquisition function reduces to \n\n$$\\alpha_{EI}(x,\\mathcal{D}) = \\int \\max_{\\mathbf{x}'} \\left(K_{\\mathcal{D}}(x,\\mathbf{x}') z \\right)\\phi(z)\\;\\text{d}z,$$\n\n\nWhich is effectively the average distance from the rest of the domain...???? EXPLAIN THIS BETTER...WHY?\n\n::: {.line}\n:::\n&#8203;\n\n::: -->\n\n\n\n\n:::{.cleanbox}\n**Important:** For a fixed set of 'improvement locations' $\\mathbf{x}'$, the resulting posterior mean at each location can be interpreted as a 1D line as a function of $z$:\n\n$$\\mu_{\\mathcal{D}'}(z|x') = \\mu_{\\mathcal{D}}(x') + \\frac{K_{\\mathcal{D}}(x',x)}{s}z \\quad \\forall x'\\in \\mathbf{x}'$$\n\nTherefore finding the inner maximum new posterior mean as a function of $z$ can be translated to calculating the upper envelope of a set of lines. This upper envelope represents for a given $z$, the maximum posterior mean across all of the locations in $\\mathbf{{x}'}. The main idea is to remove the inner $\\max$ operator and replace it with something more tractable.\n\n$$\\int \\max \\left[\\text{lines} (z)\\right] \\phi(z) \\;\\text{d}z \\rightarrow \\int \\text{upper envelope} (z) \\phi(z) \\;\\text{d}z$$\n\nWhich is tractable for a piecewise linear upper envelope.\n:::\n\nFor the sake of completeness we will set up and run a basic Bayesian optimisation loop, but I'll just use a naive implementation for now.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Gradient-based multi-start optimisation of utility function...\"}\ndef aq_and_grad(x,GP):\n  x = np.array(x,dtype=np.float32)\n  x = torch.from_numpy(x.reshape(-1,1))\n  x = torch.tensor(x, requires_grad=True)\n  aq = noisy_EI(x,GP)\n  aq.backward()\n  f_val = aq.detach().numpy()\n  f_grad = x.grad.numpy()[0]\n  return (-f_val,-f_grad)\n\ndef optimise_aq(GP):\n  x0_list = np.linspace(0,3*np.pi,16)\n  f_best = 1E10\n  x_best = None\n  for x0 in x0_list:\n    res = minimize(aq_and_grad, x0, method='L-BFGS-B', jac=True,\n                  bounds=((0,3*np.pi),),args=(GP), options={'maxiter': 100})\n    if res.fun < f_best:\n      f_best = res.fun\n      x_best = res.x\n\n  return x_best\n```\n:::\n\n\nPlotting every 5 iterations, for a total of 50 iterations. \n\n::: {.cell layout-ncol='2' execution_count=12}\n``` {.python .cell-code}\nfor iteration in range(50):\n  # train GP\n  model,likelihood = build_GP(x_data,y_data,2000)\n  GP = {'model':model,'likelihood':likelihood}\n  GP['model'].eval(); GP['likelihood'].eval()\n\n  # optimise acquisition function\n  x_best = optimise_aq(GP)\n  y_best = f(x_best)\n\n  # add to dataset\n  x_data = torch.cat((x_data,torch.tensor(x_best)))\n  y_data = torch.cat((y_data,torch.tensor(y_best)))\n\n  if (iteration) % 5 == 0:\n    fig,ax = plt.subplots(2,1,figsize=figsize,sharex=True)\n    ax[0] = plot_model(ax[0],GP,x_data,y_data,in_loop=iteration+1)\n    ax[1] = plot_aq(ax[1],GP)\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){width=666 height=225 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-2.png){width=666 height=225 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-3.png){width=666 height=225 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-4.png){width=666 height=225 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-5.png){width=666 height=225 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-6.png){width=666 height=225 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-7.png){width=666 height=225 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-8.png){width=666 height=225 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-9.png){width=666 height=225 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-10.png){width=666 height=225 fig-align='center'}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}