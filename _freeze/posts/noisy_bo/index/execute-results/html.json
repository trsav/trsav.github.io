{
  "hash": "099742db721ec62611038496000ac2f8",
  "result": {
    "markdown": "---\ntitle: \"BAYESIAN OPTIMISATION WITH NOISY MEASUREMENTS\"\nsubtitle: \"A REVIEW\"\nauthor: \n - name: \"Tom Savage\"\ncategories: [optimisation, machine learning]\ndate: \"12/05/2023\"\ncallout-appearance: minimal\nformat:\n  html:\n    css: styles.css\nbibliography: ref.bib\n---\n\nI have reworked some notes from [@garnett_bayesoptbook_2023] regarding noisy-expected improvement. Please refer to the original text for a more detailed explanation.\n\n:::{.cleanbox}\nThe assumptions that hold for the expected improvement utility function do not hold when measurements have noise. \n\nWe really want to find the point where the _signal_ is optimised [@Jones1998].\n\nHow do you determine if a measurement is signal or noise?\n:::\n\nWe begin by specifying an underlying objective function, which we consider unknown, shown in @fig-underlying. We have access to observations which we assume contain normally distributed noise. \n\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Definition of underlying function...\"}\ndef f(x):\n  return 2.4*np.sin(2.8*x) - ((x-3.5*np.pi) ** 2)/4 +  3.8*np.cos(1.7*x) - (x**2)/16 + np.random.normal(0,2)\n```\n:::\n\n\n::: {.cell fig-width='50%' execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![The underlying noisy funtion to be maximised alongside example observations.](index_files/figure-html/fig-underlying-output-1.png){#fig-underlying width=634 height=184 fig-align='center'}\n:::\n:::\n\n\nWe first begin by motivating the usual expected-improvement criteria. \nWe are looking to maximise the <span style=\"color:red;\">expected</span> increase between the of the <span style=\"color:green;\">maximum of the mean of the resulting Gaussian process after making an observation at $x$</span> and the <span style=\"color:blue;\">maximum of the current Gaussian process</span> <span style=\"color:red;\">over potential observations $y$</span> which are Gaussian distributed as a result of our $\\mathcal{GP}$ model.\n$$\\alpha_{EI}(x;\\mathcal{D}) = {\\color{red}\\int} \\left[{\\color{green}\\max \\mu_{\\mathcal{D}'}} - {\\color{blue}\\mu^*}\\right]{\\color{red}\\mathcal{N}(y;\\mu,s^2)\\text{d}y}$$ {#eq-ei_noisy}\n\nBy formulating the expression with respect to the improvement in the mean of the current and subsequent Gaussian process models, **we are expressing our interest in optimising the signal** and not the noise or values of specific observations.\n\n:::{.cleanbox}\n**Important:**\nGiven a hypothetical observation $y$, the value of the mean of the resulting Gaussian process $\\mu_{\\mathcal{D}'}$ at given set of potential locations $\\mathbf{x}'$ is \n\n$$ \\mu_{\\mathcal{D}'} =  \\mu_{\\mathcal{D}} + \\frac{K_\\mathcal{D}(\\mathbf{x}',x)}{s}\\frac{y-\\mu}{s},$$\n\nwhere $\\mu$ and $s^2$ are the mean and standard deviation of the distribution of potential values $y$ could take. \n:::\n\nWhen we express this distribution in terms of the standard normal distribution $z := \\mathcal{N}(0,1)$, we have $y = \\mu + sz$ and as a result\n\n$$ \\mu_{\\mathcal{D}'} =  \\mu_{\\mathcal{D}}(\\mathbf{x}') + \\frac{K_\\mathcal{D}(\\mathbf{x}',x)}{s}z.$$\n\nPutting this back into @eq-ei_noisy, we now only have to take the expectation over the standard normal distribution resulting in the following. \n\n$$\\alpha_{EI}(x,\\mathcal{D}) = \\int \\max_{\\mathbf{x}'} \\left(\\mu_{\\mathcal{D}}(\\mathbf{x}') + \\frac{K_\\mathcal{D}(\\mathbf{x}',x)}{s}z \\right)\\phi(z)\\;\\text{d}z - \\mu^*$${#eq-ei_noisy2}\n\nTo begin with we will sample some initial data:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# our initial dataset\nx_data = np.random.uniform(0,3*np.pi,8)\ny_data = np.array([f(x_i) for x_i in x_data])\n```\n:::\n\n\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"GP model definition and training...\"}\nclass ExactGPModel(gpytorch.models.ExactGP):\n    def __init__(self, train_x, train_y, likelihood):\n        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n    \n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\ndef build_GP(x,y,its):\n  likelihood = gpytorch.likelihoods.GaussianLikelihood()\n  model = ExactGPModel(x, y, likelihood)\n\n  model.train()\n  likelihood.train()\n\n  optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n\n  mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n\n  for i in range(its):\n      optimizer.zero_grad()\n      output = model(x)\n      loss = -mll(output, y)\n      loss.backward()\n      optimizer.step()\n  return model,likelihood\nmodel,likelihood = build_GP(x_data,y_data,2000)\nGP = {'model':model,'likelihood':likelihood}\n```\n:::\n\n\n@fig-initial shows this data with an initial Gaussian process (importantly assuming in-exact observations).\n\n::: {.cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![A Gaussian process fit to the initial dataset.](index_files/figure-html/fig-initial-output-1.png){#fig-initial width=666 height=206 fig-align='center'}\n:::\n:::\n\n\nNow we will naively construct @eq-ei_noisy2 using 40 samples from the standard normal distribution, and naively calculate an approximation to the inner maximisation by evaluating at 100 location throughout the space (which defines our vector $\\mathbf{x}' \\in\\mathbb{R}^{100}$) and returning the max value from these. \n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndef noisy_EI(x,GP):\n  model = GP['model']\n  likelihood = GP['likelihood']\n  N = torch.distributions.Normal(0,1)\n  predicted_output = likelihood(model(x))\n  mean = predicted_output.mean\n  var = predicted_output.variance\n  z_vals = torch.linspace(-2,2,40)\n  integral = 0 \n  for z in z_vals:\n    x_prime_vals = torch.linspace(0,3*np.pi,100).reshape(-1,1)    \n    new_mean_vals = model.mean_module(x_prime_vals)\n    new_cov_vals = ((model.covar_module(x_prime_vals,x)/var)*z.item())[:,0]\n    integral += torch.max(new_mean_vals + new_cov_vals)\n  integral /= 100\n  return -integral\n```\n:::\n\n\nNow if we plot this function alongside the time taken to evaluate the acquisition function at 100 locations.\n\n::: {.cell execution_count=9}\n\n::: {.cell-output .cell-output-stdout}\n```\nNAIVE TIME TAKEN:  0.877 s. AVERAGE TIME PER EVALUATION:  0.0088 s\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-2.png){width=634 height=206 fig-align='center'}\n:::\n:::\n\n\n<!-- \n:::{.cleanbox}\n**Gaining an intuition:** Let's create a number of example Gaussian process models and see what the acquisition function looks like for each. \n\n1. Constant GP with a constant noise.\n\n::: {.cell execution_count=10}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=626 height=318 fig-align='center'}\n:::\n:::\n\n\nThe acquisition function is maximised at the center of the domain, with an approximately linear decrease either side. \nIf we assume that $\\mu_{\\mathcal{D}}(x) = 0$ and $\\sigma_{\\mathcal{D}}(x) = 1$, the acquisition function reduces to \n\n$$\\alpha_{EI}(x,\\mathcal{D}) = \\int \\max_{\\mathbf{x}'} \\left(K_{\\mathcal{D}}(x,\\mathbf{x}') z \\right)\\phi(z)\\;\\text{d}z,$$\n\n\nWhich is effectively the average distance from the rest of the domain...???? EXPLAIN THIS BETTER...WHY?\n\n::: {.line}\n:::\n&#8203;\n\n::: -->\n\n\n\n\n:::{.cleanbox}\n**Important:** For a fixed set of 'improvement locations' $\\mathbf{x}'$, the resulting posterior mean at each location can be interpreted as a 1D line as a function of $z$:\n\n$$\\mu_{\\mathcal{D}'}(z|x') = \\mu_{\\mathcal{D}}(x') + \\frac{K_{\\mathcal{D}}(x',x)}{s}z \\quad \\forall x'\\in \\mathbf{x}'$${#eq-posterior_mean}\n\nTherefore finding the inner maximum new posterior mean as a function of $z$ can be translated to calculating the upper envelope of a set of lines. This upper envelope represents for a given $z$, the maximum posterior mean across all of the locations in $\\mathbf{{x}'}$. The main idea is to remove the inner $\\max$ operator and replace it with something more tractable enabling analytical integration.\n\n$$\\int \\max \\left[\\text{lines} (z)\\right] \\phi(z) \\;\\text{d}z \\rightarrow \\int \\text{upper envelope} (z) \\phi(z) \\;\\text{d}z \\rightarrow \\text{analytical solution}$$\n\nWhich has an analytical solution for a piecewise linear upper envelope. To do so:\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nn = 100\na = np.random.uniform(0,0.2,n)\nb = np.random.uniform(-10,10,n)\n```\n:::\n\n\n::: {.cell execution_count=12}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){width=667 height=206}\n:::\n:::\n\n\nWe will first get rid of any lines that are definitely dominated between -5 and 5 (the range of $z$ values we are interested in). I'm not going to explain this too much but it's quite easy to derive this condition.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nl_store = []; u_store = []\nzl = -5; zu = 5\nfor i in range(len(a)):\n    l = a[i]*zl + b[i]\n    u = a[i]*zu + b[i]\n    l_store.append(l)\n    u_store.append(u)\n\nL_i = np.argmax(l_store)\nU_i = np.argmax(u_store)\n\ndel_i = []\nfor i in range(len(a)):\n    if l_store[i] < l_store[U_i] and u_store[i] < u_store[L_i]:\n        del_i.append(i)\n\na = np.delete(a,del_i)\nb = np.delete(b,del_i)\n```\n:::\n\n\nNow we will sort the lines by gradient \n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nsorted_indices = np.argsort(a)\na = a[sorted_indices]; \nb = b[sorted_indices]; \n```\n:::\n\n\n\n\n::: {.cell execution_count=16}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-1.png){width=670 height=206}\n:::\n:::\n\n\nThen initialise the set of dominated lines (a and b values respectively), respective intervals, and the current largest $z$ on the envelope.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\ndom_a = []\ndom_b = []\ninterval_store = []\nenvelope_z = -5\n```\n:::\n\n\nFor each line in order of increasing gradient (here we start with the first line indexed at $j=0$)\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nj = 0 \n```\n:::\n\n\nAdd the line to the set of dominating lines \n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\ndom_a.append(a[j])\ndom_b.append(b[j])\n```\n:::\n\n\nCalculate the $z$ intercept of the line with all lines of a larger gradient. As we only calculate this intersection with these remaining lines, the overall algorithm has $\\mathcal{O}(n\\log n)$ complexity. \n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nz_intercept_store = []\nfor i in range(j+1,n):\n    z_intercept = (dom_b[-1] - b[i])/(a[i] - dom_a[-1])\n    z_intercept_store.append(z_intercept)\n```\n:::\n\n\n::: {.cell execution_count=21}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-22-output-1.png){width=668 height=206 fig-align='center'}\n:::\n:::\n\n\nThe intercept with the lowest value of $z$ will be a potential vertex for the envelope, but we must check that there are no lines above it. \n\nTo do so we calculate the $\\mu$ value of each line at $min({\\mathbf{z})}$.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nmu_vals = []\nz_intercept = np.min(z_intercept_store)\nmu_intercept = dom_a[-1]*z_intercept + dom_b[-1]\n\nfor i in range(j+1,n):\n    mu_vals.append(a[i]*z_intercept + b[i])\n```\n:::\n\n\n::: {.cell execution_count=23}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-24-output-1.png){width=668 height=206 fig-align='center'}\n:::\n:::\n\n\nIf the maximum value of $\\mu$ is on the dominating line, and this value is larger than the current largest $z$ value in the envelope...then we know this must lie on the upper envelope. _Otherwise_ forget about this dominating line by setting it's definition to ```None```.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nif abs(mu_intercept-np.max(mu_vals)) < 1e-5 and z_intercept > envelope_z:\n  interval_store.append([envelope_z,z_intercept])\n  envelope_z = z_intercept\nelse:\n  dom_a[-1] = None\n  dom_b[-1] = None\n  interval_store.append([None,None])\n```\n:::\n\n\nNow we can plot the interval we have just calculated.\n\n::: {.cell execution_count=25}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-26-output-1.png){width=668 height=206 fig-align='center'}\n:::\n:::\n\n\nIf we now repeat this procedure starting with the line with the next largest gradient we can search for next vertex on the upper envelope. \n\n::: {.cell execution_count=26}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Full algorithm for calculating upper envelope of a set of lines...\"}\ndef upper_env(a,b):\n\n  l_store = []; u_store = []\n  zl = -5; zu = 5\n  for i in range(len(a)):\n      l = a[i]*zl + b[i]\n      u = a[i]*zu + b[i]\n      l_store.append(l)\n      u_store.append(u)\n\n  L_i = np.argmax(l_store)\n  U_i = np.argmax(u_store)\n\n  del_i = []\n  for i in range(len(a)):\n      if l_store[i] < l_store[U_i] and u_store[i] < u_store[L_i]:\n          del_i.append(i)\n\n  a = np.delete(a,del_i)\n  b = np.delete(b,del_i)\n  n = len(a)\n\n  sorted_indices = np.argsort(a)\n  a = a[sorted_indices]\n  b = b[sorted_indices]\n\n  dom_a = []\n  dom_b = []\n  interval_store = []\n  envelope_z = -5\n\n  for j in range(n):\n    dom_a.append(a[j])\n    dom_b.append(b[j])\n\n    z_intercept_store = []\n    for i in range(j+1,n):\n        z_intercept = (dom_b[-1] - b[i])/(a[i] - dom_a[-1])\n        z_intercept_store.append(z_intercept)\n\n    mu_vals = []\n    try:\n      z_intercept = np.min(z_intercept_store)\n    except:\n      interval_store.append([envelope_z,5])\n      break \n    mu_intercept = dom_a[-1]*z_intercept + dom_b[-1]\n    for i in range(j+1,n):\n        mu_vals.append(a[i]*z_intercept + b[i])\n\n    if abs(mu_intercept-np.max(mu_vals)) < 1e-9 and z_intercept > envelope_z:\n      interval_store.append([envelope_z,z_intercept])\n      envelope_z = z_intercept\n    else:\n      dom_a[-1] = None\n      dom_b[-1] = None\n      interval_store.append([None,None])\n\n  del_store = []\n  for i in range(len(dom_a)):\n      if dom_a[i] == None:\n          del_store.append(i)\n\n  dom_a = np.delete(dom_a,del_store)\n  dom_b = np.delete(dom_b,del_store)\n  interval_store = np.delete(interval_store,del_store,axis=0)\n\n  zl = -5\n  zu = 5\n\n  interval_store[interval_store > zu] = zu\n  interval_store[interval_store < zl] = zl\n\n  return dom_a,dom_b,interval_store\n```\n:::\n\n\n::: {.cell execution_count=27}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-28-output-1.png){width=668 height=206 fig-align='center'}\n:::\n:::\n\n\nThe expected value over $z$ (integrating over potential experimental outputs) of this upper envelope (which we use inplace of the inner $\\max$ operator), where $z \\sim \\mathcal{N}(0,1)$ is given by:\n\n$$\n\\sum_i \\int^{c_{i+1}}_{c_{i}}(a_iz+b_i)\\phi(z) \\; \\text{d}z \\rightarrow \\sum_i b_i [\\Phi (c_{i+1})-\\Phi(c_i)] + a_i[\\phi(c_i) - \\phi(c_{i+1})]$$\n\nwhere $\\Phi$ is the standard cumulative normal probability function, $\\phi$ is the standard normal probability function, $c$ are the $z$ values of the upper envelope intervals, and $a$ and $b$ are the gradient and intercept of upper envelope intervals corresponding to @eq-posterior_mean.\n\n_Intuitively_, we are solving the expected improvement integral analytically for each line segement within the upper envelope of lines (which we use as it is equivalent to the inner $\\max$ operator). \n\n:::\n\nFor the sake of completeness we will wrap this up into an acquisition function, and run a basic Bayesian optimisation loop\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\ndef noisy_EI_lines(x,GP):\n  with torch.no_grad(), gpytorch.settings.fast_pred_var():\n    model = GP['model']\n    likelihood = GP['likelihood']\n    model.eval()\n    likelihood.eval()\n    x_prime_vals = torch.linspace(0,3*np.pi,200)\n    predicted_output = likelihood(model(x))\n    mean = predicted_output.mean.item()\n    var = predicted_output.variance.item()\n    a = []\n    b = likelihood(model(x_prime_vals)).mean.numpy()\n    for x_prime in x_prime_vals.reshape(-1,1):\n      conc_x = torch.cat((x,x_prime))\n      covar = likelihood(model(conc_x)).covariance_matrix\n      a.append((covar[0,1]/var).item())\n    dom_a,dom_b,interval_store = upper_env(np.array(a),np.array(b))\n    N = torch.distributions.Normal(0,1)\n    sum = 0\n    for i in range(len(interval_store)):\n      c_i = torch.tensor(interval_store[i,0])\n      c_i1 = torch.tensor(interval_store[i,1])\n      sum += dom_b[i]*(N.cdf(c_i1) - N.cdf(c_i)) + dom_a[i]*(np.exp(N.log_prob(c_i)) - np.exp(N.log_prob(c_i1)))\n  return sum.item()\n```\n:::\n\n\n::: {.cell execution_count=29}\n\n::: {.cell-output .cell-output-stdout}\n```\nNAIVE TIME TAKEN:  0.8452 s. AVERAGE TIME PER EVALUATION:  0.0085 s\nUPPER ENVELOPE TIME TAKEN:  4.6748 s. AVERAGE TIME PER EVALUATION:  0.0467 s\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-30-output-2.png){width=757 height=374 fig-align='center'}\n:::\n:::\n\n\n::: {.cell execution_count=30}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Gradient-based multi-start optimisation of utility function...\"}\ndef aq_and_grad(x,GP):\n  x = torch.tensor(x)\n  f_val = noisy_EI_lines(x,GP)\n  return -f_val\n\ndef optimise_aq(GP):\n  x0_list = np.linspace(0,3*np.pi,16)\n  f_best = 1E10\n  x_best = None\n  for x0 in x0_list:\n    res = minimize(aq_and_grad, x0, bounds=((0,3*np.pi),),args=(GP), options={'maxiter': 100})\n    if res.fun < f_best:\n      f_best = res.fun\n      x_best = res.x\n\n  return x_best\n```\n:::\n\n\nPlotting the first 8 iterations.\n\n::: {.cell layout-ncol='2' execution_count=31}\n``` {.python .cell-code}\nfor iteration in range(8):\n  # train GP\n  model,likelihood = build_GP(x_data,y_data,2000)\n  GP = {'model':model,'likelihood':likelihood}\n  GP['model'].eval(); GP['likelihood'].eval()\n\n  # optimise acquisition function\n  x_best = optimise_aq(GP)\n  y_best = f(x_best)\n\n  # add to dataset\n  x_data = torch.cat((x_data,torch.tensor(x_best)))\n  y_data = torch.cat((y_data,torch.tensor(y_best)))\n\n  fig,ax = plt.subplots(2,1,figsize=figsize,sharex=True)\n  ax[0] = plot_model(ax[0],GP,x_data,y_data,in_loop=iteration)\n  ax[1] = plot_aq_lines(ax[1],GP)\n  plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-32-output-1.png){width=666 height=206 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-32-output-2.png){width=666 height=225 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-32-output-3.png){width=666 height=225 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-32-output-4.png){width=666 height=225 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-32-output-5.png){width=666 height=225 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-32-output-6.png){width=666 height=225 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-32-output-7.png){width=666 height=225 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-32-output-8.png){width=666 height=225 fig-align='center'}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}