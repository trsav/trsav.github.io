{
  "hash": "4a3cc6f73df11c862083a9aa1bd110cf",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"The Dead Blog Theory\"\nsubtitle: \"An exercise in website collapse.\"\ndate: \"08/23/2024\"\ncomments:\n    utterances:\n        repo: trsav/trsav.github.io\nhtml: \n    self-contained: true\n    grid: \n    margin-width: 450px\nreference-location: margin\ncitation-location: margin\nfootnote-location: margin\n---\n\n\n\n\n:::{.column-margin}\n![](subway.jpeg)\n:::\n\n:::{.cleanbox}\nThis post is rotting, and will soon become AI slop. What follows is a love letter to an internet that will never exist again, it is a  self-fulfilling obituary. Ashes to ashes, dust to dust, slop... to slop. \n:::\n\n:::{.changeable_text}\nThe dead internet theory states that sooner rather than later, genuine activity on the internet will be in the minority, and the majority of traffic will come from bots. Content will be produced by bots, and it will be engaged with by other bots. Every day, it becomes less of a theory, with 'dead' content being generally characterised as **slop**.\n:::\n\n:::{.changeable_text}\nIn the beginning was the Word, and then the next Word and the next Word, and then after enough words, someone claimed that a large-language model could think, and the Word was God^[John 1:1, creatively embellished, some would say blasphemously.\\x00]. \n:::\n\n:::{.changeable_text}\nContent traditionally produced by marketers, copywrighters, and journalists is slowly being replaced with generative content from LLMs. But what happens when the next-generation of LLMs are being trained on this pseudo-data from the internet?\n:::\n\n:::{.changeable_text}\nThe equivalent of the dead internet theory for LLMs, _model-collapse_ illustrates a scenario where datasets become so poisoned that it becomes impossible to ever train a new LLM effectively^[Why [AI companies enter into multi-million pound contracts with news organisations](https://www.reuters.com/technology/sam-altmans-openai-signs-content-agreement-with-news-corp-2024-05-22/) with a wealth of verifiably human, proofread and well written content.\\x00].\n:::\n\n:::{.changeable_text}\nLike pre-trinity test low background radiation steel^[[Often stolen from WW2 shipwrecks for particle detectors.](https://en.wikipedia.org/wiki/Low-background_steel)\\x00], pre-large-language model content will become sought after. Humanity will see a return to the handwritten word, literature originating before chatGPT will be considered sacred, evidence of genuine human achievement. \n:::\n\nAnd here, we, go...^[![](joker.jpg)]\n\n:::{.changeable_text}\nIn London there is a Raspberry Pi running a cronjob. It has access to the source code to this very post and is also loaded with a small local BERT-based model^[Specifically, [google-bert/bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased)\\x00].\n:::\n\n![Proof of life](pi.jpeg){width=80%}\n\n:::{.changeable_text}\nTwice a day, a random sentence from this post will be selected, and a random word will be omitted. The small, local language model will be them prompted to infer the missing word. This word will be replaced, my site re-rendered, the changes committed to the Git repository, and reflected here^[Stunning way to increase my Github contributions.\\x00]. Additionally I will prompt the model to add a word a day to the bottom of this post.\n:::\n\n::: {#772bc079 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\" code-summary=\"Show the code\"}\nfrom transformers import pipeline\nimport numpy as np\nimport time\n\ndef process_file(filename):\n    # Read file content\n    with open(filename, 'r') as file:\n        data = file.readlines()\n    \n    # Find lines with 'changeable_text'\n    changeable_lines = [i+1 for i, line in enumerate(data) if 'changeable_text' in line]\n    random_line = np.random.choice(changeable_lines)\n    text = data[random_line]\n    \n    # Process footnote if present\n    if '^[' in text:\n        footnote_index = text.index('^[')\n        end_footnote = text.index('\\\\x00]')\n        pre_footnote, post_footnote = text.split('^[')[0], text.split(\"\\\\x00]\")[1]\n        footnote = text[footnote_index:end_footnote+5]\n        footnote_word_index = text[:footnote_index].count(' ')\n        print('FOOTNOTE WORD INDEX:', footnote_word_index)\n        data[random_line] = pre_footnote + post_footnote\n    \n    # Replace random word with [MASK]\n    line = data[random_line].split()\n    random_word_index = np.random.randint(0, len(line))\n    print('REMOVED WORD', line[random_word_index])\n    line[random_word_index] = '[MASK]'\n    line = ' '.join(line)\n    \n    # Use BERT to fill [MASK]\n    unmasker = pipeline('fill-mask', model='bert-base-uncased')\n    res = unmasker(line)\n    replacement_word = res[0]['token_str']\n    print('REPLACEMENT WORD:', replacement_word)\n    line = line.replace('[MASK]', replacement_word)\n    \n    # Reinsert footnote if it existed\n    if 'footnote' in locals():\n        line_words = line.split()\n        line_words.insert(footnote_word_index, footnote)\n        line = ' '.join(line_words) + '\\n'\n    \n    line = line.replace(':::', '\\n:::\\n')\n    data[random_line] = line\n    \n    # Process 'extra' lines\n    extra_lines = [i+1 for i, line in enumerate(data) if 'extra' in line]\n    if extra_lines:\n        generated = data[extra_lines[0]].strip()\n        hypothesised = generated + ' [MASK]'\n        res = unmasker(hypothesised)\n        print('REPLACEMENT WORD:', res[0]['token_str'])\n        data[extra_lines[0]] = f\"{generated} {res[0]['token_str']}\\n\"\n    \n    # Update timestamp\n    utc_str = time.asctime(time.gmtime())\n    print('UTC:', utc_str)\n    data[-1] = f'Updated: {utc_str}\\n'\n    \n    # Write updated content back to file\n    with open(filename, 'w') as file:\n        file.writelines(data)\n\n# Execute the script\nfor i in range(100):\n    process_file('test_post.qmd')\n```\n:::\n\n\n:::{.changeable_text}\nToken to token, slop to slop, all good things must come to an end. \n:::\n\n:::{.extra}\nI \n:::\n\nUpdated: 2024-08-23 09:24:24\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}