{
  "hash": "8f017867ae2be9c111a240791e644092",
  "result": {
    "markdown": "---\ntitle: Optimisation Trajectory Plots\nauthor: Tom Savage\ncategories:\n  - Visualisation\nimage: figure.svg\ndate: 01/31/1624\ncallout-appearance: minimal\n---\n\n\n\nHere I will outline my personal preferences for best plotting practices for optimisation trajectory plots using [Matplotlib](https://matplotlib.org). I will be initialising the plot as demonstrated in my previous posts [here](https://sav.phd/posts/convergence_plots/) and [here](https://sav.phd/posts/plots/).\n\nI won't outline a single solution here, but rather a number of options and best practices. \n\nI'll start by plotting the following objective function:\n\n$$\\min_{x_1,x_2} \\quad 100(x_2-x_1^2)^2 + (x_1-1)^2 $$\n\n...and in Python, using MLX.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport mlx.core as mx\n\ndef f(x1,x2):\n    return 100*(x2-(x1**2))**2 + (x1-1)**2 +1\n```\n:::\n\n\nFor now we will use the bounds -3 and 3 for both $x_1$ and $x_2$. We can apply MLX's `vmap` functionality to really quickly evaluate an entire grid of points in parallel. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nf_grid = mx.vmap(mx.vmap(f,in_axes=(None,0)),in_axes=(0,None))\nn = 100 \nlb = -3; ub = 3\nx1 = mx.array([2.0])\nx1 = mx.linspace(start=lb,stop=ub,num=n)\nx2 = mx.linspace(start=lb,stop=ub,num=n)\n\ny = f_grid(x1,x2).T\n```\n:::\n\n\nJust for peace-of-mind, lets compare the time it takes to evaluate the function on a 100 X 100 grid the standard way:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport timeit \n\ndef normal_mat(x1,x2):\n    y = np.zeros((n,n))\n    for i in range(n):\n        for j in range(n):\n            y[i,j] = f(x1[i],x2[j])\n    return y.T\n\nnumpy_time = timeit.timeit(lambda: normal_mat(np.array(x1),np.array(x2)),number=10)\nmlx_time = timeit.timeit(lambda: f_grid(x1,x2),number=10)\n\nprint('Numpy (s): ',numpy_time,' ','MLX (s): ',mlx_time,'\\n')\nprint(np.round(numpy_time/mlx_time,4),' times faster.',);\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumpy (s):  0.20080224995035678   MLX (s):  0.0006692500319331884 \n\n300.0407  times faster.\n```\n:::\n:::\n\n\nWe begin by plotting these evaluations the default way...\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nX1,X2 = np.meshgrid(x1,x2);\nplt.contourf(X1,X2,y,levels=100);\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=570 height=415 fig-align='center'}\n:::\n:::\n\n\nAs with [2D scatter plots](https://sav.phd/posts/plots/), we will set up the plot using Matplotlib's object orientated syntax, set the aspect ratio to be equal using `ax.set_aspect('equal')`, and add labels of an appropriate size.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfig,ax = plt.subplots(1,1,figsize=(6,6))#| \nax.set_xlabel('$x_1$',fontsize=16)\nax.set_ylabel('$x_2$',fontsize=16)\nax.set_aspect('equal')\n```\n:::\n\n\n::: {.cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=443 height=439 fig-align='center'}\n:::\n:::\n\n\nImportantly, we need a colorbar in order to interpret values of the function. By doing it in the following way we have more control over its position and options.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nfun = ax.contourf(X1,X2,y,levels=100)\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes('right', size='5%', pad=0.1)\nfig.colorbar(fun, cax=cax, orientation='vertical').set_label(label='Objective Function Value',size=16)\n```\n:::\n\n\n::: {.cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=508 height=414 fig-align='center'}\n:::\n:::\n\n\nSince we will be plotting an optimisation trajectory on top of this, we should use a softer, more subtle colourmap. My personal favourite is `Spectral` (but I'm going to reverse it by appending `_r`). \n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfun = ax.contourf(X1,X2,y,levels=100,cmap='Spectral_r')\n```\n:::\n\n\n::: {.cell execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=508 height=414 fig-align='center'}\n:::\n:::\n\n\nThis function has very steep sides, with a shallow u-shaped valley. It would be better to plot the logarithm of the objective function. We can do this by defining where we want the levels to be using `geomspace` which creates logarithmically spaced values between the minimum and maximum values of the objective function. \n\nI also change the ticks of the colourbar to match the levels of the contour plot.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nlevs = np.geomspace(np.min(np.array(y)), np.max(np.array(y)), num=10)\nfun = ax.contourf(X1,X2,y,levs,cmap='Spectral_r',locator=ticker.LogLocator())\n\nticks = np.round(levs,2)\ncbar = fig.colorbar(fun, cax=cax, orientation='vertical')\ncbar.set_label(label='Objective Function Value',size=16)\ncbar.set_ticks(ticks)\ncbar.set_ticklabels(ticks)\n```\n:::\n\n\n::: {.cell execution_count=13}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){width=519 height=414 fig-align='center'}\n:::\n:::\n\n\nConsidering we will be plotting a trajectory on top of this background it makes sense to make it a little more subtle. We can do this by reducing the alpha value of the colourmap. \n\nTo maintain some distinction between level-sets, we can plot some faded, thin contour lines in addition to the filled contour. \n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nfun = ax.contourf(X1,X2,y,levs,cmap='Spectral_r',locator=ticker.LogLocator(),alpha=0.4)\nax.contour(X1,X2,y,levs,colors='k',linewidths=0.5,alpha=0.5)\n```\n:::\n\n\n::: {.cell execution_count=15}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-1.png){width=519 height=414 fig-align='center'}\n:::\n:::\n\n\nOK now to solve an optimisation problem, I will write probably the shortest first-order optimisation algorithm possible in MLX.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nx = mx.array([2.0,-2.0])\ngrad = mx.grad(f,argnums=(0,1))\nx_store = np.array(x)\nfor i in range(500):\n    x = x - 0.0002*mx.array(grad(x[0],x[1]))\n    x_store = np.vstack((x_store,np.array(x)));\n```\n:::\n\n\nNow I will plot the trajectory on top of the objective function, alongside highlighting the initial solution, and the final solution. \n\nI'll leave these black just to avoid any clashes with the background colourmap.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nax.plot(x_store[:,0],x_store[:,1],lw=2,c='k')\nax.scatter(x_store[0,0],x_store[0,1],s=100,c='k',marker='.',zorder=10,lw=2)\nax.scatter(x_store[-1,0],x_store[-1,1],s=100,c='k',marker='+',zorder=10,lw=2)\n```\n:::\n\n\n::: {.cell execution_count=18}\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-1.png){width=519 height=438 fig-align='center'}\n:::\n:::\n\n\nI think there's many more things you could do here, such as highlighting step sizes etc... but I'll leave it there for now.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}