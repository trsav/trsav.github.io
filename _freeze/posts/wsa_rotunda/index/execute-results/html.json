{
  "hash": "c0a666801d63d4804b94529c96d18c7a",
  "result": {
    "markdown": "---\ntitle: \"Context-Aware Generative Lab Identities for Winchester School of Art\"\nsubtitle: \"_The art of life lies in a constant readjustment to our surroundings._ - Kakuzō Okakura\"\nauthor: \"Tom Savage, Studio 3015\"\ndate: \"03/14/2024\"\nformat:\n    html: \n        css: styles.css\n        theme: custom.scss\nfig-cap-location: margin\nreference-location: margin\ncitation-location: margin\nexecute:\n  eval: false\n---\n\n:::{.cleanbox}\nThis post details a consulting/research project for the [Winchester School of Art (WSA)](https://wsarotunda.soton.ac.uk) digital Rotunda platform, alongside [Studio 3015](https://www.studio3015.co.uk/projects/). If you have a similar project or concept, feel free to contact me at tom.savage(at)hotmail.com\n:::\n\n### Task\n\nThe Rotunda platform at the Winchester School of Art is largely digital, therefore contraints around static logo design can be relaxed.\nThe goal of this project was to enable existing research lab identities to change in response to specific media such as text and images. \nThrough the use of generative AI, the visual identity for each lab becomes dynamic, responding and changing based on contextual information around specific projects.\n\n:::{#fig-existing layout-ncol=4}\n\n![](1-logo.png)\n\n![](2-logo.png)\n\n![](3-logo.png)\n\n![](4-logo.png)\n\nExisting designs for the E-Textile, Global Smart, Data Image, and Social Practice labs.\n\n:::\n\n### From Manual to Digital \n\nI began by recreating the existing logos for each lab in an automated manner.\nTo achieve this I used the [svgpathtools](https://github.com/mathandy/svgpathtools/tree/master) library, enabling me to change the forms via Python^[By doing so I not only have the ability to programmatically change the form of a logo, but I also obtain the SVG for 'free' which can be used within WSA graphics.]. \nTherefore, each logo is subsequently defined by a series of coordinates stored within a dictionary, enabling easy addition of new labs or modification of existing designs.\n\nFor example, here is the complete specification of the E-textile lab design.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nlogos[\"e-textiles\"] = {}\n\nlogos[\"e-textiles\"][\"small_circles\"] = [\n    (0, 0),(0, 2),(0, 4),(2, 2),(2, 6),\n    (4, 0),(4, 4),(6, 0),(6, 2),(6, 6),\n    (4, 6),(0, 6),(6, 6),\n]\nlogos[\"e-textiles\"][\"big_circles\"] = [(2, 0), (2, 4), (6, 4), (4, 2)]\nlogos[\"e-textiles\"][\"connected\"] = [\n    [(0, 0), (0, 2)],[(0, 2), (2, 2)],[(6, 0), (6, 2)],\n    [(2, 6), (4, 6)],[(0, 6), (2, 6)],[(0, 6), (0, 4)],\n    [(4, 4), (4, 6)],[(4, 6), (6, 6)],\n]\nlogos[\"e-textiles\"][\"white_connected\"] = [\n    [(-1, 3), (2, 3)], \n    [(5, -1), (5, 2)]\n    ]\n```\n:::\n\n\nTo construct a given logo I use this information alongside svgpath tools to build up the representation in Python.\n\n:::{#fig-build layout-ncol=5}\n\n![](base_logo_1.png)\n\n![](base_logo_2.png)\n\n![](base_logo_3.png)\n\n![](base_logo_4.png)\n\n![](base_logo.png)\n\nThe process of creating a logo computationally from the dictionary that defines it.\n:::\n\n\n\nGiven this level of control and automation over the space of general designs, I turned to thinking about how the logo should change in response to context. \nAfter some experimenting we settled on the following two ways:\n\n1. Individual 'shapes' are allowed to grow and shrink, enabling either a more solid unified form, or a more distributed representation.\n2. Individual 'shapes' are switched from circles to squares, enabling a more digital and less connected form. \n\n:::{#fig-existing layout-ncol=1}\n![](logo_grid.png)\n\nDifferent combinations of parameters and how they affect the base E-textiles lab design.\n:::\n\n\nIt was decided that these two parameters should provide a representation of how **physically distributed a project** is, as well  how **interdisciplinary** a project is respectively.\n\nBy parameterising the existing designs in this way, each lab's identity is allowed to respond to contextual information around projects, but in a controlled and deliberate way, maintaining the overall form and identity.\n\nIn addition, depending on the use the colour of each design may be changed according to a specified pallette.\n\n\n### Inferring Parameters\n\nTo infer the parameters representing the physical distribution, and interdisciplinary nature of a project I prompt an LLM with both a description and image. \n\nThe information flow is as follows:\n\n![](information.png)\n\nSome prompt engineering is performed to ensure that JSON is consistently returned^[In tests successful 97-99% of the time, failing when the parameter explanation overflows.], and reliable explanations for why the parameters have been chosen. As part of this I include examples of lab descriptions and respective parameters within the prompt as context and to provide examples of outputs. When applying image data, the image is first described, and then in a separate call, the description applied to inform the logo.\n\nEach sentiment and subsequent parameter is prompted separately due to recent advances in low-cost, fast LLM inference^[See below.], to ensure that forgetting or context-shift doesn't occur.\n\n### LLM Inference\n\nLLM inference is performed on [Anthropic's Claude 3 Haiku](https://www.anthropic.com/news/claude-3-haiku). I find it is extremely low cost, fast, and it's performance is enough to infer the sentiment of the project descriptions. Despite it's speed and cost, it is surprisingly effective at accurately describing images^[Example follows...].\n\n:::{.cleanbox}\n**UTC**: 2024-03-14T11:50:31Z\n\n**Model Name**: claude-3-haiku-20240307\n\n**Prompt**: Describe this image.\n\n**Image**:\n\n![](lab-image.png){width=50% fig-align=\"center\"}\n\n\n**Output**: _This image shows various electronic and technical equipment. There is a device that appears to be a test instrument or measurement tool, with knobs and displays. Next to it is a device that looks like a portable audio player or recorder. On the table in front of these devices is a document or diagram labeled \"Code in Line / Rehabilitation Hand-Night\", which seems to be some kind of technical or engineering schematic. There is also a red tool or instrument that appears to be used for testing or working with the equipment._\n\n\n\n**Input Tokens**: 795 ($0.25/1000000 tokens)\n\n**Output Tokens**: 108 ($1.25/1000000 tokens)\n\n**Total Cost**: $0.00033375 (0.03¢)\n\n**Time**: 3.12 (s)\n\n:::\n\n### Creating an Interface\n\nThe goal of the interface for the generative lab identity inference is to make it as simple as possible for those with a non-technical background to use, on any device^[Including Winchester School of Art teaching and administrative staff, and students]. \n\nTo do so, I packaged the Python implementation into a Flask app with a basic password-protected front-end. The application is then hosted on Heroku, a cloud-based platform.\n\nThis exposes the application on the web for anyone to use, protects against the LLM API credits being used by the public^[Assuming the password remains a secret to those with access], and removes the need to install or interact with code.\n\n:::{.center}\n![](app.png)\n:::\n\nThe user is prompted to provide a project title, description, project visual, and the lab itself. \nThis information is then used as previously described to produce an independent lab logo, with explanation. \n\n### Creating a Loading Image\n\nIn order to design a visually interesting graphic for the front end, I decided to fine-tine a diffusion model. To begin with I created a new 3D model of _The Rotunda_ in AutoCAD.\n\n:::{.center}\n![](above.png){width=25%}\n![](side.png){width=25%}\n\n:::\n\n Using this 3D model I created a number of synthetic images of the building, combining these with an existing dataset of real photographs.\n\n::: {layout-ncol=8}\n\n![](datasets/rotunda_data/below sketchy wireframe.png) \n\n![](datasets/rotunda_data/black and white photograph.jpeg)\n\n![](datasets/rotunda_data/close outside image.jpeg)\n\n![](datasets/rotunda_data/close up render.png)\n\n![](datasets/rotunda_data/dark line wireframe.png)\n\n![](datasets/rotunda_data/dark orthographic render.png)\n\n![](datasets/rotunda_data/mirrored render.png)\n\n![](datasets/rotunda_data/outside image.jpeg)\n\n![](datasets/rotunda_data/photograph.jpeg)\n\n![](datasets/rotunda_data/render.png)\n\n![](datasets/rotunda_data/sketchy wireframe.png)\n\n![](datasets/rotunda_data/upside down photograph.jpeg)\n\n![](datasets/rotunda_data/white render.png)\n\n![](datasets/rotunda_data/wide outside image.jpeg)\n\n![](datasets/rotunda_data/wireframe.png)\n\n![](datasets/rotunda_data/metallic render.png)\n\n:::\n\n\nThis dataset of approximately 40 images was then used to fine-tuned a Stable Diffusion model^[Locally on my M2 Pro Macbook over the course of 3 days.] using _textual inversion_ to learn the concept of _The Rotunda_.\n\n\n\n::: columns\n::: {.column width=\"40%\"}\n\n![](diffusion_output.gif){width=90%}\n:::\n\n::: {.column width=\"60%\"}\n\nThis custom token is then used to perform inference (or generation in the context of generative AI) to generate novel representations and views of the building.<br>\n\nThese representations (in the form of a GIF) provide a sense of the technology involved within the lab logo generation tool, this GIF is also displayed whilst inference is being performed in the background.\n:::\n:::\n\nThe images, whilst individually imperfect, collectively result in a coherent representation.^[Likely the most unnecessarily complex login/loading GIF ever produced.]\n\n:::{.cleanbox}\nThe full application is as follows:\n\n:::{.center}\n![](full_example.gif){width=100%}\n:::\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}